# Linear Regression

## Linear regression - theory
Linear regression model is a line y=ax+b, where sum of distances between all y=axi+b and given yi (sum of squares) is minimal.  

Assume that there is approximately a linear relationship between X and Y:  
$$ Y \approx \beta_0 + \beta_1X$$
where \(\beta\)~0~ is an __intercept__ and \(\beta\)~1~ is a __slope__

Parameters of the line could be calculated using __least squares methods__:

$$\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}{(x_i - \bar{x})^2}} $$
$$\beta_0 = \bar{y} - \beta_1\bar{x} $$

## Generate random data set for the linear model
Suppose we want to simulate from the following linear model:
y = \(\beta\)~0~ + \(\beta\)~1~x + \(\epsilon\),  
where \(\epsilon\) ~ N(0,2^2^). Assume x ~ N(0,1^2^), \(\beta\)~0~ = 0.5, \(\beta\)~1~ = 2.  

```{r}
set.seed(20)
x <-rnorm(100)
e <- rnorm(100, 0, 2)
y <- 0.5 + 2*x + e
summary(y)
plot(x,y)
```

## Practical example
Practical example from [Wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))  
Set of data: (1,6), (2,5), (3, 7), (4,10)

```{r}
x <- c(1,2,3,4)
y <- c(6,5,7,10)
plot(y~x, xlim=c(0,5), ylim=c(4,10))
abline(3.5, 1.4)
r <- lm(y~x)
segments(x, y, x, r$fitted.values, col="green")
```

We have to find the line corresponding to the minimal sum of errors (distances from the each point to this line):
1. For all points:  
$$\beta_1 + 1\beta_2 = 6$$

$$\beta_1 + 2\beta_2 = 5$$
$$\beta_1 + 3\beta_2 = 7$$
$$\beta_1 + 4\beta_2 = 10$$
the least squares S:  
$$S(\beta_1, \beta_2) = [6 - (\beta_1 + 1\beta_2)]^2 + [5 - (\beta_1 + 2\beta_2)]^2 + [7 - (\beta_1 + 3\beta_2)]^2 + [10 - (\beta_1 + 4\beta_2)]^2 = 4\beta_1^2 + 30\beta_2^2 + 20\beta_1\beta_2 - 56\beta_1 - 154\beta_2 + 210$$
The minimum is:
$$\frac{\partial{S}}{\partial{\beta_1}} = 0 = 8 \beta_1 + 60\beta_2 - 154$$
$$\frac{\partial{S}}{\partial{\beta_2}} = 0 = 20 \beta_1 + 20\beta_2 - 56$$
Result in a system of two equations in two unkowns gives:
$$\beta_1 = 3.5$$
$$\beta_2 = 1.4$$
The line of best fit:  
$y = 3.5 + 1.4x$  
All possible regression lines goes through the intersection point $(\bar{x}, \bar{y})$

## Mean squared error (MSE)

**Standard error of train data**  
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2$$

**Standard error of learn data**  
$$MSE = \frac{1}{n_o}\sum_{i=1}^{n_o}(y_i^o - \hat{f}(x_i^o))^2$$

## Linear model in R
```{r}
# Height and weight vectors for 19 children
height <- c(69.1,56.4,65.3,62.8,63,57.3,59.8,62.5,62.5,59.0,51.3,64,56.4,66.5,72.2,65.0,67.0,57.6,66.6)
weight <- c(113,84,99,103,102,83,85,113,84,99,51,90,77,112,150,128,133,85,112)

plot(height,weight)
# Fit linear model
model <- lm(weight ~ height) # weight = slope*weight + intercept
abline(model)   # Regression line

# correlation between variables
cor(height,weight)

# Get data from the model
#get the intercept(b0) and the slope(b1) values
model

# detailed information about the model
summary(model)

# check all attributes calculated by lm
attributes(model)

# getting only the intercept
model$coefficients[1] #or model$coefficients[[1]]

# getting only the slope
model$coefficients[2] #or model$coefficients[[2]]

# checking the residuals
residuals(model)

# predict the weight for a given height, say 60 inches
model$coefficients[[2]]*60 + model$coefficients[[1]]

# Mean squared error (MSE)
predicted.weights <- predict(model, newdata = as.data.frame(weight))
mse <- mean(( weight - predicted.weights)^2, na.rm = TRUE)
mse
```

## Linear regression model for multiple parameters

## Choosing explanatory variables for the model
In general, less MSE then better prediction. We can arrange variables by their importance 
to predict and choose the number of significant variables.  

1. run model with different number of variables.  
2. Arrange MSE for each variable.  
3. Compair MSE for different number of variables using t-test.  
If difference is significant => use more variables.  

In the same way we can compair different models.  

```{r}
library(mosaicData)
head(CPS85)

# relation wage ~ education
# wage - response variable
# educ, exper, age are explanatory variables

# linear model
model1 <- lm(wage ~ educ, data = CPS85)
model2 <- lm(wage ~ educ + age, data = CPS85)
model3 <- lm(wage ~ educ + age + exper, data = CPS85)

pred1 <- predict(model1, newdata = CPS85)
pred2 <- predict(model2, newdata = CPS85)
pred3 <- predict(model3, newdata = CPS85)

# Compair MSE
mse1 <- mean(( CPS85$wage - pred1)^2, na.rm = TRUE)
mse2 <- mean(( CPS85$wage - pred2)^2, na.rm = TRUE)
mse3 <- mean(( CPS85$wage - pred3)^2, na.rm = TRUE)

mse <- data.frame(model_1 = mse1,
                  model_2 = mse2,
                  model_3 = mse3)
mse

# Using both educ and age variables reduese MSE => improve model, where
# adding exper does not improve model
```

## Assessment of model performance for categorical data.
Errors for categorical data can be calculated as number of errors prediction model makes.  
Test whether predicted values match actual values.  
Likelihood: extract the probability that the model assigned to the observed outcome.  

## Confidence intervals for linear model
```{r}
# 0. Build linear model 
data("cars", package = "datasets")
model <- lm(dist ~ speed, data = cars)
# 1. Add predictions 
pred.int <- predict(model, interval = "prediction")
mydata <- cbind(cars, pred.int)
# 2. Regression line + confidence intervals
library("ggplot2")
p <- ggplot(mydata, aes(speed, dist)) +
  geom_point() +
  stat_smooth(method = lm)
# 3. Add prediction intervals
p + geom_line(aes(y = lwr), color = "red", linetype = "dashed")+
    geom_line(aes(y = upr), color = "red", linetype = "dashed")
```