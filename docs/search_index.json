[["index.html", "R statistics Chapter 1 Introduction", " R statistics Mark Goldberg 2021-05-12 Chapter 1 Introduction This is the collection of statistical methods in R. "],["statistics-r-functions-reference.html", "Chapter 2 Statistics R functions reference 2.1 Get data 2.2 Data inspection 2.3 Plots 2.4 Analysis of the distribution 2.5 Distributions 2.6 t-Test 2.7 ANOVA 2.8 Machine Learning Functions Reference", " Chapter 2 Statistics R functions reference 2.1 Get data 2.2 Data inspection head(df) typeof(df) # data type dim(df) # dimention nrow(df) # number of rows ncol(df) # number of columns str(df) # data structure summary(df) # data summary names(df) # names of columns colnames(df) # also column names table(df) # frequency of categorical data 2.3 Plots plot(x ~ y) barplot(df) boxplot(v) hist(x) pie(groupsize, labels, col, ...) 2.4 Analysis of the distribution # mode getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } getmode(v) # mode mean(v) # mean mean(v, trim=0.1) # trimmed mean median(v) # median min(v) max(v) range(v) # c(mun(v), max(v)) max(v)-min(v) # range sort(v) rank(v) sample(v) # subsample sample(v, size) var(v) # variance sd(v) # standard deviation cor(v) # correlation cov(v) # covariation scale(v) # z-scores quantile(v) IQR(v) # interquantile range: IQR = Q3 ‚Äì Q1 qqnorm(v) # normal probability plot qqline(v) # adds a line to a normal probability plot passing through 1Q and 3Q 2.5 Distributions d - probability density p - cumulative distribution q - quantile function (inverse cumulative distribution) r - random variables from distribution # Normal distribution dnorm(x, mean, sd) # Probability Density Function (PDF) pnorm(q, mean, sd) # Cumulative Distribution Function (CDF) qnorm(p, mean, sd) # quantile function - inverse of pnorm rnorm(n, mean, sd) # random numbers from normal distribution # Binomial dbinom(x) # Probability Density Function (PDF) pbinom(q) # Cumulative Distribution Function (CDF) qbinom(p) # quantile function - inverse of pnorm rbinom(n) # random numbers from normal distribution # Poisson dpois() ppois() qpois() rpois() # Exponential dexp() pexp() qexp() rexp() # Chi-squared distribution dchisq(v, df) # density of the interval pchisq() # distribution of the interval (AUC) qchisq() # quantiles rchisq() # random deviates 2.6 t-Test t.test(x, mu = 0, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95) t.test(v, mu) # one-sample t-test, mu - null hypothesized value t.test(v1, v2) # two-sample t-test t.test(v1, v2, var.equal=T) t.test(var1, var2, paired=T) wilcox.test(v1, v2, paired=T) 2.7 ANOVA # One way ANOVA oneway.test(x ~ f) aov(x ~ f) anova(m1, m2) # compair two models 2.8 Machine Learning Functions Reference 2.8.1 Linear Regression lm_model &lt;- lm(y ‚àº x1 + x2, data=as.data.frame(cbind(y,x1,x2))) summary(lm_model) lm(y ~ x1 + x2 + x3) # multiple linear regression lm(log(y) ~ x) # log transformed lm(sqrt(y) ~ x) # sqrt transformed lm( y ~ log(x)) # fields transformed llm(log(y) ~ log(x)) # everything is transformed lm(y ~ .) # use all fields for regression model lm(y ~ x + 0) # forced zero intercept lm(y ~ x*k) # interaction of two variables lm(y ~ x + k + x:k) # product of xkl but without interaction lm(y ~ (x + k + ... + l)^2) # all first order interactions lm(y ~ I(x1 + x2)) # sum of variables lm(y ~ I(x1^2)) # product of variables (not interation) lm(y ~ x + I(x^2) + I(x^3)) # polynomial regression lm(y ~ poly(x,3)) # same as previous # Forward/backward stepwise regression # improve model fit &lt;- lm(y ~ x1 + x2) bwd.fit &lt;- step(fit, direction = &#39;backward&#39;) fwd.fit &lt;- step(fit, direction = &#39;forward&#39;, scope( ~ x1 + x2)) # Test linear model plot(m) # plot residuals car::outlier.test(m) dwtest(m) # Durbin-Watson Test of the model residuals # Prediction predicted_values &lt;- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test))) # Apriori dataset &lt;- read.csv(&quot;C:\\\\Datasets\\\\mushroom.csv&quot;, header = TRUE) mushroom_rules &lt;- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9)) summary(mushroom_rules) inspect(mushroom_rules) # Logistic Regression glm() glm_mod &lt;-glm(y ‚àº x1+x2, family=binomial(link=&quot;logit&quot;), data=as.data.frame(cbind(y,x1,x2))) # K-Means Clustering kmeans_model &lt;- kmeans(x=X, centers=m) # k-Nearest Neighbor Classification knn_model &lt;- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K) # Naƒ±ve Bayes library(e1071) nB_model &lt;- naiveBayes(y ‚àº x1 + x2, data=as.data.frame(cbind(y,x1,x2))) # Decision Trees (CART) library(rpart) # Tree-based models rpart(formula, data, method,control) formula outcome~predictor1+predictor2+predictor3+etc. data data frame method &#39;class&#39; for a classification tree &#39;anova&#39; for a regression tree control optional parameters for controlling tree growth. minsplit = 50 - minimal number of observation in a node cp=0.001 - cost coplexity factor cart_model &lt;- rpart(y ‚àº x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=&quot;class&quot;) printcp(fit) # display cp table plotcp(fit) # plot cross-validation results rsq.rpart(fit) # plot approximate R-squared and relative error for different splits (2 plots). labels are only appropriate for the &quot;anova&quot; method. print(fit) # print results summary(fit) # detailed results including surrogate splits plot(fit) # plot decision tree text(fit) # label the decision tree plot post(fit, file=) # create postscript plot of decision tree plot.rpart(cart_model) text.rpart(cart_model) # AdaBoost # boosting functions - uses decision trees as base classifiers library(rpart) library(ada) # Let X be the matrix of features, and labels be a vector of 0-1 class labels. boost_model &lt;- ada(x=X, y=labels) # Support Vector Machines (SVM) library(e1071) # Let X be the matrix of features, and labels be a vector of 0-1 class labels. Let the regularization parameter be C. Use the following commands to generate the SVM model and view details: svm_model &lt;- svm(x=X, y=as.factor(labels), kernel =&quot;radial&quot;, cost=C) summary(svm_model) "],["combinatorics.html", "Chapter 3 Combinatorics", " Chapter 3 Combinatorics Permutation with repetition (–ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏) \\(P_n = n!\\) k-permutations of n (—Ä–∞–∑–º–µ—â–µ–Ω–∏—è) \\(A_n^k = \\frac{n!}{(n - k)!}\\) Combination (—Å–æ—á–µ—Ç–∞–Ω–∏—è) \\(C_n^k = \\frac{ n! }{ (n-k)! k! }\\) Binom \\((a + b)^n = \\sum_{k=0}^{n} C_n^k a^{n - k}b^k\\) \\((a+b)^2 = a^2 + 2ab + b^2\\) \\((a+b)^3 = a^3 + 3a^2b + 3ab^2 + b^3\\) "],["probability.html", "Chapter 4 Probability", " Chapter 4 Probability "],["basic-statistics.html", "Chapter 5 Basic Statistics 5.1 Definitions 5.2 Probability", " Chapter 5 Basic Statistics 5.1 Definitions population - all existing samples sample - subset of statistical population simple random sample - random subset stratified sample - fist clustering, than random sample from cluster sample - random choosing from several existing clusters variables - discret, continuous, ordinal (—Ä–∞–Ω–≥–æ–≤–∞—è) 5.2 Probability A standard French-suited deck of playing cards contains 52 cards; 13 each of hearts (‚ô•), spades (‚ô†), clubs (‚ô¶), and diamonds (‚ô£). Assuming that you have a well-shuffled deck in front of you, the probability of drawing any given card is 1/52 ‚âà 1.92%. Calculate the probability of drawing any of the four aces! That is, calculate the probability of drawing üÇ° or üÇ± or üÉÅ or üÉë using the sum rule and assign it to prob_to_draw_ace. # Calculate the probability of drawing any of the four aces prob_to_draw_ace &lt;- 1/52 + 1/52 + 1/52 + 1/52 Cards and the product rule Again, assuming that you have a well-shuffled deck in front of you, the probability of drawing any given card is 1/52 ‚âà 1.92% . The probability of drawing any of the four aces is 1/52 + 1/52 + 1/52 + 1/52 = 4/52. Once an ace has been drawn, the probability of picking any of the remaining three is 3/51. If another ace is drawn the probability of picking any of the remaining two is 2/50, and so on. Use the product rule to calculate the probability of picking the four aces in a row from the top of a well-shuffled deck and assign it to prob_to_draw_four_aces. # Calculate the probability of picking four aces in a row prob_to_draw_four_aces &lt;- 4/52 * 3/51 * 2/50 * 1/49 "],["statistical-distributions.html", "Chapter 6 Statistical distributions 6.1 Normal Distribution 6.2 Bernoulli Distribution 6.3 Binomial Distribution 6.4 Beta distribution 6.5 Geometric Distribution 6.6 Uniform Distributions 6.7 Poisson Distribution 6.8 Exponential Distribution 6.9 Chi-squared Distribution", " Chapter 6 Statistical distributions 6.1 Normal Distribution x &lt;- seq(-6, 6, length=100) y1 &lt;- dnorm(x, sd=1) y2 &lt;- dnorm(x, sd=0.5) y3 &lt;- dnorm(x, sd=2) plot(x, y1, xlim=c(-6,6), ylim=c(0,0.8), type=&quot;l&quot;, lwd=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Normal distribution&quot;) lines(x, y2, col=&quot;red&quot;) lines(x, y3, col=&quot;green&quot;) legend(&quot;topright&quot;, legend=c(&quot;sd = 1&quot;, &quot;sd = 0.5&quot;, &quot;sd = 2&quot;), col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;), lty=c(1,1,1), lwd=2) # Area under the curve # For normal distribution auc = 1 (probability for all) library(DescTools) AUC(x, y1) ## [1] 1 AUC(x,y2) ## [1] 1 AUC(x,y3) # to broad distribution, some samples out of the area ## [1] 0.9972921 6.2 Bernoulli Distribution Bernoulli distribution is the discrete probability distribution of a random variable which takes the value 1 with probability p and the value 0 with probability q=1-p wiki. Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes‚Äìno question. The Bernoulli distribution is a special case of the binomial distribution with n=1. Bernoulli process, a random process consisting of a sequence of independent Bernoulli trials. 6.3 Binomial Distribution It describes the outcome of n independent trials in an experiment. Each trial is assumed to have only two outcomes, either success or failure. If the probability of a successful trial is p, then the probability of having x successful outcomes in an experiment of n independent trials is as follows. \\[f(x) = p^x(1-p)^{(n-x)}\\] where x = 0,1,2,3,4,‚Ä¶. The binomial distribution is the basis for the popular binomial test of statistical significance. Rules: 1. Must be a fixed number of trials. 2. Trials must be independent (the outcome of one rial does not affect the others). 3. Each trial has only two outcomes: success or failure. 4. The probability of success remains the same in all trials. \\(n\\) - number of trials \\(p\\) - probability of successful outcome for each trial \\(q\\) - probability of failure outcome for each trial \\(x\\) - number of successes \\(P(x)\\) - probability of the number of successes \\(q = 1 - p\\) \\(p = 1 - q\\) Example Suppose there are twelve multiple choice questions in an English class quiz. Each question has five possible answers, and only one of them is correct. Find the probability of having four or less correct answers if a student attempts to answer every question at random. Solution Since only one out of five possible answers is correct, the probability of answering a question correctly by random is 1/5=0.2. We can find the probability of having exactly 4 correct answers by random attempts as follows. dbinom(4, size=12, prob=0.2) ## [1] 0.1328756 To find the probability of having four or less correct answers by random attempts, we apply the function dbinom with x = 0,‚Ä¶,4. dbinom(0, size=12, prob=0.2) + dbinom(1, size=12, prob=0.2) + dbinom(2, size=12, prob=0.2) + dbinom(3, size=12, prob=0.2) + dbinom(4, size=12, prob=0.2) ## [1] 0.9274445 Alternatively, we can use the cumulative probability function for binomial distribution pbinom. pbinom(4, size=12, prob=0.2) ## [1] 0.9274445 Tasks 60% of people who purchase sports cars are men. If 10 sports car owners are randomly selected, find the probability that exactly 7 are men (A: 0.215). Sources Binomial Distribution Statistics Lecture 5.3: A study of Binomial Probability Distributions Youtube ‚ÄòBinomial distributions - Probabilities of probabilities‚Äô at 3Blue1Brown channel part 1part 2 6.4 Beta distribution The beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by \\(\\alpha\\) and \\(\\beta\\). wiki. 6.5 Geometric Distribution In probability theory and statistics, the geometric distribution is either one of two discrete probability distributions: The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ‚Ä¶ } The probability distribution of the number Y = X ‚àí 1 of failures before the first success, supported on the set { 0, 1, 2, 3, ‚Ä¶ } wiki \\[(1 - p)^{k-1} for k trials where k \\in {1,2,3,...}\\] \\[(1 - p)^kp for k failures where k \\in {0,1,2,...}\\] 6.6 Uniform Distributions Continuous uniform distribution is a family of symmetric probability distributions. The distribution describes an experiment where there is an arbitrary outcome that lies between certain bounds. The bounds are defined by the parameters, \\(a\\) and \\(b\\), which are the minimum and maximum values. The interval can either be closed \\([a, b]\\) or open \\((a, b)\\). Therefore, the distribution is often abbreviated \\(U(a, b)\\), where \\(U\\) stands for uniform distribution wiki. Probability density function: \\[ \\begin{cases} \\frac{1}{b - a} &amp; \\text{for} \\, x \\in [a,b] \\\\ 1 &amp; \\text{for} \\, x &lt; a \\, \\text{or} \\, x\\, &gt; b \\end{cases}\\] 6.7 Poisson Distribution Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. wiki Also knows as Poisson Process. \\[P(k) = e^{\\lambda}\\frac{\\lambda^k}{k!}\\] The Poisson distribution is also the limit of a binomial distribution, for which the probability of success for each trial equals Œª divided by the number of trials, as the number of trials approaches infinity. According to the Central Limit Theorem for big \\(\\lambda\\) the Normal distribution will be the approximation for the Poisson distribution. Criteria of the Poisson process: 1. Events are independent of each other. The occurrence of one event does not affect the probability another event will occur. 2. The average rate (events per time period) is constant. 3. Two events cannot occur at the same time. Models: - Number of calls per minute in a call center. - Number of decay events that occur from a radioactive source in a given observation period. - The number of meteorites greater than 1 meter diameter that strike Earth in a year. - The number of patients arriving in an emergency room between 10 and 11 pm. - The number of laser photons hitting a detector in a particular time interval. Example: 12 cars cross the bridge in 1 min in average. What is the probability that 17 or more cars will cross the bridge? ppois(16, lambda=12, lower=FALSE) ## [1] 0.101291 # simulate Poisson x &lt;- rpois(n = 10000, lambda = 3) hist(x) # Calculate the probability of break-even mean(x &gt;= 15) ## [1] 0 6.8 Exponential Distribution Exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. wiki \\[P(x) = \\lambda e ^{ - \\lambda x }\\] It is a particular case of the gamma distribution. Suppose the mean checkout time of a supermarket cashier is 3 minutes. Find the probability of a customer checkout being completed by the cashier in less than 2 minutes. pexp(2, rate=1/3) ## [1] 0.4865829 6.9 Chi-squared Distribution In probability theory and statistics, the \\(\\{chi}^2\\) distribution with k degrees of freedom is the distribution of a sum of the squares of \\(k\\) independent standard normal random variables. wiki If \\(X_1,X_2,‚Ä¶,X_m\\) are \\(m\\) independent random variables having the standard normal distribution, then the following quantity follows a Chi-Squared distribution with \\(m\\) degrees of freedom. Its mean is \\(m\\), and its variance is \\(2m\\). \\[V = X_1^2 + X_2^2 + ... + X_m^{2} \\tilde{} X_{(m)}^2\\] Summ of squares of standard normal, independen random variables are distributed according to the chi-square distribution with k degrees of freedom, where k is the number of random variables being summed. Chi-squared distribution applications: * Chi-square test of independence in contingency tables * Chi-square test of goodness of fit of observed data to hypothetical distributions * Likelihood-ratio test for nested models * Log-rank test in survival analysis * Cochran‚ÄìMantel‚ÄìHaenszel test for stratified contingency tables x = seq(0,25, length=100) v = dchisq(x, df=7) plot(x,v, type=&#39;l&#39;) # quantiles of chi-squared q = c(.25, .5, .75, .999) qchi = qchisq(q, df = 7, lower.tail = TRUE) abline(v=qchi, lty=2, col=&#39;red&#39;) qchi ## [1] 4.254852 6.345811 9.037148 24.321886 Sources Seven Must-Know Statistical Distributions and Their Simulations for Data Science Probability distributions Hartmann, K., Krois, J., Waske, B. (2018): E-Learning Project SOGA: Statistics and Geospatial Data Analysis. Department of Earth Sciences, Freie Universitaet Berlin. "],["primary-data-analysis.html", "Chapter 7 Primary data analysis 7.1 Analysis of sample distribution 7.2 Handling missing data 7.3 Dealing with outliers", " Chapter 7 Primary data analysis 7.1 Analysis of sample distribution 7.1.1 Histogram # sample of random integers v &lt;- round(rnorm(n=50, sd=5, mean=100)) par(mfrow=c(1,2)) stripchart(v, method = &quot;stack&quot;, pch=19, cex=2, offset=.5, at=.15, main = &quot;Dotplot of random value&quot;, xlab = &quot;Random value&quot;) hist(v) # add density x &lt;- density(v)$x y &lt;- (10/max(density(v)$y))*density(v)$y # scale y to plot with histogram lines(x, y, col=&quot;red&quot;, lwd=2) 7.2 Handling missing data Ignore: Discard samples with missing values. Impute: ‚ÄòFill in‚Äô the missing values with other values. Accept: Apply methods that are unaffected by the missing data. library(naniar) mammographic &lt;- read.csv(&#39;./DATA/mammographic.data&#39;) any_na(mammographic) # Replace ? with NAs: bands mammographic &lt;- replace_with_na_all(mammographic, ~.x == &#39;?&#39;) any_na(mammographic) miss_var_summary(mammographic) Vizualysing missing data library(ggpubr) a &lt;- vis_miss(mammographic) # comulative b &lt;- vis_miss(mammographic, cluster=TRUE) c &lt;- gg_miss_case(mammographic) ggarrange(a, b, c + rremove(&quot;x.text&quot;), labels = c(&quot;frame view&quot;, &quot;cumulative&quot;, &quot;missing&quot;), ncol = 3, nrow = 1) Missing data types - MCAR: Missing Completely At Random - MAR: Missing At Random - MNAR: Missing Not At Random Type Imputation Deletion Visual cues MCAR Recommended Will not lead to bias Random or noisy patterns in missingness clusters MAR Recommended May lead to bias Well-defined missingness clusters when arrangin for a particular variable(s) MNAR Will lead to bias Will lead to bias Neither visual pattern above holds It can be difficult to ascertain the missingness type using visual inspection! Internal evaluation Compair distributions with/without imputed values: Mean Variance Scale Exterlan evaluation Build ML models with/without imputated values and evaluate impact of imputation method on ML model performance: Classification Regression Clustering etc. Ideally imputation should not bring big differences. Mean and linear imputations library(naniar) library(simputation) # Impute with the mean imp_mean &lt;- bands %&gt;% bind_shadow(only_miss = TRUE) %&gt;% add_label_shadow() %&gt;% impute_mean_all() # Impute with lm imp_lm &lt;- bands %&gt;% bind_shadow(only_miss = TRUE) %&gt;% add_label_shadow() %&gt;% impute_lm(Blade_pressure ~ Ink_temperature) %&gt;% impute_lm(Roughness ~ Ink_temperature) %&gt;% impute_lm(Ink_pct ~ Ink_temperature) Combining multiple imputation models # Aggregate the imputation models imp_models &lt;- bind_rows(mean = imp_mean, lm = lmp_lm, .id = &quot;imp_model&quot;) head(imp_models) 7.3 Dealing with outliers Outliers are rare values that appear far away from the majority of the data. Outliers can bias the results and potentially lead to incorrect conclusions if not handled properly. It is possible to remove outliers from the data but removing data points can introduce other types of bias into the results, and potentially result in losing critical information. If outliers seem to have a lot of influence on the results, a nonparametric test such as the Wilcoxon Signed Rank Test may be appropriate to use instead. Outliers can be identified visually using a boxplot. The 3-sigma rule (for normally distributed data) The 1.5*IQR rule (more general) Outliers: any value lower that \\(Q1 - 1.5 x IQR\\) or any higher than \\(Q3 + 1.5 x IQR\\) Multivariate methods * Distance-based: K-nearest neighbors (kNN) distance * Density-based: Local outlier factor (LOF) **1.5*IQR rule** Outliers: any value lower that \\(Q1 - 1.5 x IQR\\) or any higher than \\(Q3 + 1.5 x IQR\\) Distance-based methods Average distance to the K-nearest neighbors Density-based methods Number of the neighboring points within a certain distance Assumption: outliers often lie far from their neighbors Local Outlier Factor (LOF) * Measures the local deviation of a data point with respect to its neighbors. * Outliers are observations with substantially lower density than their neighbors. get.knn() from FNN package Each observation \\(x\\) has an associated score LOF(\\(x\\)) LOF(\\(x\\)) \\(\\approx\\) 1 similar density to its neighbors LOF(\\(x\\)) &lt; 1 higher density than neighbors (inlier) LOF(\\(x\\)) &gt; 1 lower density than neighbors (outlier) lof() function from dbscan package What to do with outlier observations? 1. Retention: Keep them in your dataset and, if possible, use algorithms that are robust to outliers. - e.g.¬†K nearest-neighbors (kNN), tree-based methods (decision tree, random forest) 2. Imputation: Use an imputation method to replace their value with a less extreme observation. - e.g.¬†mode imputation, linear imputation, kNN imputation. 3. Capping: Replace them with the value of the 5-th percentile (lower limit) or 95-th percentile (upper limit). 4. Exclusion: Not recommended, especially in small datasets or those where a normal distribution cannot be assumed. cars &lt;- read.csv(&#39;./DATA/cars.csv&#39;) cars &lt;- cars[,1:3] head(cars) ## distance consume speed ## 1 28 5 26 ## 2 12 42 30 ## 3 112 55 38 ## 4 129 39 36 ## 5 185 45 46 ## 6 83 64 50 boxplot(cars$consume) consume_quartiles &lt;- quantile(cars$consume) consume_quartiles ## 0% 25% 50% 75% 100% ## 4 41 46 52 122 # Scale data and create scatterplot: cars_scaled require(dplyr) glimpse(cars) ## Rows: 388 ## Columns: 3 ## $ distance &lt;int&gt; 28, 12, 112, 129, 185, 83, 78, 123, 49, 119, 124, 118, 123, 247, 124, 173,‚Ä¶ ## $ consume &lt;int&gt; 5, 42, 55, 39, 45, 64, 44, 5, 64, 53, 56, 46, 59, 51, 47, 51, 56, 51, 49, ‚Ä¶ ## $ speed &lt;int&gt; 26, 30, 38, 36, 46, 50, 43, 40, 26, 30, 42, 38, 59, 58, 46, 24, 36, 32, 39‚Ä¶ cars_scaled &lt;- as.data.frame(scale(cars)) plot(distance ~ consume, data = cars_scaled, main = &#39;Fuel consumption vs. distance&#39;) # Calculate upper threshold: upper_th upper_th &lt;- consume_quartiles[4] + 1.5 * (consume_quartiles[4] - consume_quartiles[2]) upper_th ## 75% ## 68.5 # Print the sorted vector of distinct potential outliers sort(unique(cars$consume[cars$consume &gt; upper_th])) ## [1] 69 71 74 79 81 87 99 108 115 122 library(FNN) # Compute KNN score cars_knn &lt;- get.knn(data = cars_scaled, k = 7) cars1 &lt;- cars cars1$knn_score &lt;- rowMeans(cars_knn$nn.dist) # Print top 5 KNN scores and data point indices: top5_knn (top5_knn &lt;- order(cars1$knn_score, decreasing = TRUE)[1:5]) ## [1] 320 107 335 56 190 print(cars1$knn_score[top5_knn]) ## [1] 4.472813 2.385471 2.246905 2.122242 1.740575 # Plot variables using KNN score as size of points plot(distance ~ consume, data = cars1, cex = knn_score, pch = 20) library(dbscan) # Add lof_score column to cars1 cars1 &lt;- cars cars1$lof_score &lt;- lof(cars_scaled, minPts = 7) # Print top 5 LOF scores and data point indices: top5_lof (top5_lof &lt;- order(cars1$lof_score, decreasing = TRUE)[1:5]) ## [1] 165 186 161 228 52 print(cars1$lof_score[top5_lof]) ## [1] 2.606151 2.574906 2.328733 2.252610 2.027528 # Plot variables using LOF score as size of points plot(distance ~ consume, data = cars1, cex = lof_score, pch = 20) "],["data-normalization.html", "Chapter 8 Data normalization 8.1 Normality test 8.2 Finding Confidence intervals", " Chapter 8 Data normalization Data normalization (feature scaling) is not always needed for e.g.¬†decision-tree-based models. Data normalization is beneficial for: Support Vector Machines, K-narest neighbors, Logistic Regression Neural networks Clustering algorithms (K-means, K-medoids, DBSCAN, etc.) Feature extraction (Principal Component Analysis, Linear Discriminant Analysis, etc) Min-max scaling Maps a numerical value \\(x\\) to the [0,1] interval \\[x&#39; = \\frac{x - min}{max - min}\\] Ensures that all features will share the exact same scale. Does not cope well with outliers. Standardization (Z-score normalization) Maps a numerical value to \\(x\\) to a new distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\) \\[x&#39; = \\frac{x - \\mu}{\\sigma}\\] More robust to outliers then min-max normalization. Normalized data may be on different scales. Example library(dplyr) # generate data with different ranges df &lt;- data.frame( a = sample(seq(0, 2, length.out=20)), b = sample(seq(100, 500, length.out=20))) # view data glimpse(df) ## Rows: 20 ## Columns: 2 ## $ a &lt;dbl&gt; 1.3684211, 1.1578947, 1.2631579, 2.0000000, 0.7368421, 1.5789474, 0.0000000, 1.05‚Ä¶ ## $ b &lt;dbl&gt; 331.5789, 457.8947, 394.7368, 184.2105, 247.3684, 310.5263, 121.0526, 100.0000, 1‚Ä¶ # data ranges to see if normalization is needed sapply(df, range) ## a b ## [1,] 0 100 ## [2,] 2 500 # ranges are different =&gt; normalize # Apply min-max and standardization nrm &lt;- df %&gt;% mutate(a_MinMax = (a - min(a)) / (max(a) - min(a)), b_MinMax = (b - min(b)) / (max(b) - min(b)), a_ZScore = (a - mean(a)) / sd(a), b_ZScore = (b - mean(b)) / sd(b) ) # ranges for normalized data sapply(nrm, range) ## a b a_MinMax b_MinMax a_ZScore b_ZScore ## [1,] 0 100 0 0 -1.605793 -1.605793 ## [2,] 2 500 1 1 1.605793 1.605793 # plots par(mfrow=c(1,3)) boxplot(nrm[, 1:2], main = &#39;Original&#39;) boxplot(nrm[, 3:4], main = &#39;Min-Max&#39;) boxplot(nrm[, 5:6], main = &#39;Z-Score&#39;) 8.1 Normality test It is possible to use histogram to estimate normality of the distribution. # QQ-plot - fit normal distibution qqnorm(v); qqline(v) var(v) # variance: sd = sqrt(var) ## [1] 23.76367 sd(v) # standard deviation ## [1] 4.8748 sd(v)/sqrt(length(v)) # standard error sd/sqrt(n) ## [1] 0.6894008 # Z-score (standartization) # transform distribution to mean=0, variance=1 # z = (x - mean(n))/sd vs &lt;- scale(v)[,1] vs ## [1] -1.75186680 0.29949948 0.50463610 -2.16214006 0.91490936 -0.11077378 -0.72618366 ## [8] 0.70977273 1.53031924 0.09436285 0.50463610 0.29949948 -0.93132029 0.29949948 ## [15] -0.72618366 1.12004599 -0.11077378 -0.72618366 0.29949948 -1.13645692 -0.72618366 ## [22] -0.11077378 -1.13645692 -1.34159355 0.70977273 0.09436285 0.09436285 0.29949948 ## [29] 1.53031924 -0.31591041 1.32518262 -0.52104703 -1.95700343 1.32518262 0.70977273 ## [36] -0.11077378 0.91490936 0.91490936 0.91490936 1.32518262 0.70977273 0.09436285 ## [43] -2.36727668 1.53031924 -0.31591041 -0.72618366 -1.54673017 0.91490936 0.09436285 ## [50] -0.52104703 par(mfrow=c(1,2)) hist(v, breaks=10) hist(vs, breaks=10) 8.2 Finding Confidence intervals # sample of random integers x &lt;- round(rnorm(n=50, sd=5, mean=100)) # Confidence interval for normal distribution with p=0.95 m &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(0.95)*s/sqrt(n) confidence &lt;- c(m-error, m+error) confidence ## [1] 99.16802 101.19198 # Confidence interval for t-distribution with p=0.95 a &lt;- 5 s &lt;- 2 n &lt;- 20 error &lt;- qt(0.975,df=n-1)*s/sqrt(n) # confidence interval c(left=a-error, right=a+error) ## left right ## 4.063971 5.936029 Sources Practicing Machine Learning Interview Questions in R on DataCamp "],["primary-data-analysis-case-studies.html", "Chapter 9 Primary data analysis - Case studies", " Chapter 9 Primary data analysis - Case studies Abbakumov, 2016, lectures # GET DATA df &lt;- read.table(&quot;./DATA/Swiss_Bank_Notes.csv&quot;, header=T, sep=&quot; &quot;, dec=&quot;,&quot;) head(df) # Data explanation: parameters of Swiss Banknotes # Size of data: 200 (100 are real, 100 are false) # Length - length # H_l - height left # H_r - height right # dist_l - border left # dist_up - border up # Diag - diagonal # ? Find false banknotes # 1. Let&#39;s add a column with 100 filled 0 and 100 filled with 1. origin &lt;- 0 df &lt;- data.frame(df, origin) df$origin[1:100] &lt;-1 # Set origin as factor - binary data (0,1) df$origin &lt;- as.factor(df$origin) is.factor(df$origin) # 2. HISTORGRAM par(mfrow=c(length(colnames(df))/2,2)) for (i in 1:length(colnames(df))) { hist(df[,i], main = paste(colnames(df)[i])) } # Histogram for Diagonals par(mfrow=c(1,1)) hist(df$Diag, breaks=18, probability=TRUE) # Barplot barplot(VADeaths, beside=TRUE, legend=TRUE, ylim=c(0, 100), ylab=&quot;Deaths per 1000&quot;, main=&quot;Death rates in Virginia&quot;) # Pieplot groupsize &lt;- c(18, 30, 32, 10, 10) labels &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;F&quot;) pie(groupsize, labels, col=c(&quot;red&quot;, &quot;white&quot;, &quot;grey&quot;, &quot;black&quot;, &quot;blue&quot;)) # All pairs of data scatter plot plot(df) # Length ~ Dial plot(df$Length, df$Diag) # true notes points(df$Length[df$origin==1], df$Length[df$origin==0], pch=3, col=&quot;green&quot;) # false notes points(df$Length[df$origin==1], df$Length[df$origin==0], pch=1, col=&quot;red&quot;) # If factors are given, plot makes boxplot plot(df$origin, df$Diag) title(&quot;Swiss Bank Notes&quot;) # GET DATA - TOWNS town &lt;- read.table(&quot;DATA/town_1959_2.csv&quot;, header=T, sep=&quot;\\t&quot;, dec=&quot;.&quot;) town summary(town) # Median is more stable to outliers summary(town[,3]) # lets remove 2 first towns from the data summary(town[3:1004,3]) hist(town[,3]) # log scale allows us to see outliers better hist(log(town[,3]), breaks=50) # Truncated mean is better than mean mean(town[,3], trim=0.05) # GET DATA - BASKETBALL bb &lt;- read.table(&quot;DATA/basketball.csv&quot;, header=F, sep=&quot;;&quot;, dec=&quot;.&quot;) bb # NBA Player characteristics: # percent of positives vs: # SF - light forvard # PF - heavy forvard # C - center # G - defender summary(bb[,1]) par(mfrow=c(1,1)) plot(bb[,1]~bb[,2]) par(mfrow=c(2,2)) for (i in 1:4) { hist(bb[,1], breaks=5*i, main=paste(&quot;Breaks&quot;, 5*i), ylab=&quot;&quot;) } for (i in unique(bb[,2])) { hist(bb[bb[,2]==i ,1], breaks=6, xlim=c(min(bb[,1])-5, max(bb[,1]+5)), col=&quot;white&quot;, main=i, ylab=&quot;&quot;) } # Conclusion: for several groups of data boxplots may be more informative than histograms "],["hypothesis-testing.html", "Chapter 10 Hypothesis testing 10.1 Hypothesis testing theory 10.2 Hypothesis test (Practice)", " Chapter 10 Hypothesis testing 10.1 Hypothesis testing theory Null hypothesis (H0): 1. H0: m = Œº 2. H0: m \\(\\leq\\) Œº 3. H0: m \\(\\geq\\) Œº Alternative hypotheses (Ha): 1. Ha:m ‚â† Œº (different) 2. Ha:m &gt; Œº (greater) 3. Ha:m &lt; Œº (less) Note: Hypothesis 1. are called two-tailed tests and hypotheses 2. &amp; 3. are called one-tailed tests. The p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true. Note: p-value is not the probability that the null hypothesis is true. Note: Absence of evidence ‚ßß evidence of absence. Cutoffs for hypothesis testing *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected. not rejected (‚Äònegative‚Äô) rejected (‚Äòpositive‚Äô) H0 true True negative (specificity) False Positive (Type I error) H0 false False Negative (Type II error) True positive (sensitivity) Type II errors are usually more dangerous. Significance level \\(/alpha\\) is the probability of mistakenly rejecting the null hypothesis, if the null hypothesis is true. This is also called false positive and type I error. 10.2 Hypothesis test (Practice) Check distribution (is normal?): shapiro.test() Distribution uniformity (are both normal?) Dependence of variable (are x and y dependent?) Difference of distribution parameters (are means different?) ###. 1 Is distribution normal? town &lt;- read.table(&quot;~/DataAnalysis/DATA/town_1959_2.csv&quot;, header=T, sep=&quot;\\t&quot;, dec=&quot;.&quot;) town # histogram hist(town[,3]) # log scale hist(log(town[,3]), breaks=50) # ? Is log scaled number of sitizens per town is normally distributed? # test for normal distribution of dataset data &lt;- log(town[,3]) shapiro.test(data) # H0: data is normally distributed # W = 0.97467 - # p-value = 3.15e-12 -&gt; p &lt; alpha (0.01) - H0 is incorrect # Our distripution is not normal # Different tests for normality in package &quot;nortest&quot; install.packages(&quot;nortest&quot;) library(nortest) ad.test(data) # Anderson-Darling test lillie.test(data) # Lilliefors (Kolmogorov-Smirnov) test # Emperical rule: if n &lt; 2000 -&gt; shapiro.test, else -&gt; lillie.test # In theory it is possible to use method for normally distributed data, # even if they are not normal, but almost normal: # 1. No outlayers # 2. Symmetry of histogram # 3. Bell-shaped form of histogram # Method to make normal from not-normal distribution # 1. Remove outlayers # 2. Transform data to get symmetry: log, Boxcox # 3. Bimodality: split samples into groups # Boxcox transformation install.packages(&quot;AID&quot;) library(AID) bctr = boxcoxnc(data) bctr$results ### 2. Compair centers of two distributions # Emperical rule: if notmal -&gt; mean, else -&gt; median # if median -&gt; Mann‚ÄìWhitney-Wilcoxon (MWW) test # if mean -&gt; one of Student&#39;s tests # Student t-test # t.test(x,y, alternative=&quot;two.sided&quot;, paired=FALSE, var.equal=FALSE) # How to compair if dispersions are the same? # Fligner-Killeen test # Brown-Forsythe test #fligner.test(x~g, data=data.table) # Example x &lt;- read.table (&quot;~/DataAnalysis/R_data_analysis/DATA/Albuquerque Home Prices_data.txt&quot;, header=T) names(x) summary(x) # Change -9999 to NA x$AGE[x$AGE==-9999] &lt;- NA x$TAX[x$TAX==-9999] &lt;- NA fligner.test(PRICE~COR, data=x) # Dispersion is the same for both distribution # p &gt; alpha (0.01) t.test(x$PRICE[x$COR==1], x$PRICE[x$COR==0], alternative=&quot;less&quot;, paired=FALSE, var.equal=TRUE) # p-value = 0.1977 (H0 is true) # Mood&#39;s median test names(x) x1 &lt;- x[x$NE==1,1] x2 &lt;- x[x$NE==0,1] m &lt;- median(c(x1,x2)) f11 &lt;- sum(x1&gt;m) f12 &lt;- sum(x2&gt;m) f21 &lt;- sum(x1&lt;=m) f22 &lt;- sum(x2&lt;=m) table &lt;- matrix(c(f11,f12, f21, f22), nrow=2,ncol=2) chisq.test(table) # p=0.47 &gt; alpha=0.05: H0 is true # medians are the same ## Mann‚ÄìWhitney-Wilcoxon (MWW) test # Check if medians are the same # U1 = n1*n2+{n1*(n1+1)/2 - T1} # U1 = n1*n2+{n2*(n2+1)/2 - T2} # U=min(U1,U2) # Ti - —Å—É–º–º–∞ —Ä–∞–Ω–≥–æ–≤ –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏–∑ –≤—ã–±–æ—Ä–∫–∏ i # n1, n2 - —Ä–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ wilcox.test(x,y, alternative=&quot;two.sided&quot;, paired=FALSE, exact=TRUE, correct=FALSE) ###. 3. Dependency of variables # H0: x, y independent # Correlation analysis # Correlation (pearson) mesure linear dependency! # Correlation k depends on outlayers (must be removed) plot(x$SQFT, x$TAX) cor(x$SQFT, x$TAX, use = &quot;complete.obs&quot;) cor(x$SQFT, x$TAX, use = &quot;complete.obs&quot;, method = &quot;spearman&quot;) cor(x, use = &quot;complete.obs&quot;, method = &quot;spearman&quot;) cor(x, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) cor.test(x$SQFT, x$TAX, use = &quot;complete.obs&quot;) "],["t-procedures.html", "Chapter 11 t-Procedures 11.1 t-test and normal distribution 11.2 One-sample t-test 11.3 Practical example: t-test in R 11.4 Two samples t-test 11.5 Compare Student‚Äôs t and normal distributions 11.6 Non-parametric tests 11.7 Mann-Whitney U Rank Sum Test 11.8 Wilcoxon test", " Chapter 11 t-Procedures 11.1 t-test and normal distribution t-distribution assumes that the observations are independent and that they follow a normal distribution. If the data are dependent, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors? It is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed. Independence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement. FIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use Wilcoxon test or permutation tests. 11.2 One-sample t-test One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (Œº). t-statistics: \\(t = \\frac{m - \\mu}{s/\\sqrt{n}}\\), where m is the sample mean n is the sample size s is the sample standard deviation with n‚àí1 degrees of freedom Œº is the theoretical value Q: And what should I do with this t-statistics? Q: What is the difference between t-test and ANOVA? Q: What is the smallest sample size which can be tested by t-test? Q: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests. 11.3 Practical example: t-test in R We want to test if N is different from given mean Œº=0: N = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25) t.test(N, mu = 0, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.9931 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf 0.964019 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.01383 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.1552496 1.0487504 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.006916 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 0.239981 Inf ## sample estimates: ## mean of x ## 0.602 FIXME: why it accepts all alternatives at the same time (less and greater?) 11.4 Two samples t-test Do two different samples have the same mean? H0: 1. H0: m1 - m2 = 0 2. H0: m1 - m2 \\(\\leq\\) 0 3. H0: m1 - m2 \\(\\geq\\) 0 Ha: 1. Ha: m1 - m2 ‚â† 0 (different) 2. Ha: m1 - m2 &gt; 0 (greater) 3. Ha: m1 - m2 &lt; 0 (less) The paired sample t-test has four main assumptions: The dependent variable must be continuous (interval/ratio). The observations are independent of one another. The dependent variable should be approximately normally distributed. The dependent variable should not contain any outliers. Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with Likert-type scales. t-statistics: \\(t=\\frac{y - x}{SE}\\), where y and x are the samples means. SE is the standard error for the difference. If H0 is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample). To apply t-test samples must be tested if they have equal variance: equal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic). 11.5 Compare Student‚Äôs t and normal distributions x &lt;- seq(-4, 4, length=100) hx &lt;- dnorm(x) degf &lt;- c(1, 3, 8, 30) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;, &quot;gold&quot;, &quot;black&quot;) labels &lt;- c(&quot;df=1&quot;, &quot;df=3&quot;, &quot;df=8&quot;, &quot;df=30&quot;, &quot;normal&quot;) plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Comparison of t Distributions&quot;) for (i in 1:4){ lines(x, dt(x,degf[i]), lwd=2, col=colors[i]) } legend(&quot;topright&quot;, inset=.05, title=&quot;Distributions&quot;, labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors) To generate data with known mean and sd: rnorm2 &lt;- function(n,mean,sd) { mean+sd*scale(rnorm(n)) } r &lt;- rnorm2(100,4,1) ### t-test a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179) b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180) # test homogeneity of variances using Fisher‚Äôs F-test var.test(a,b) ## ## F test to compare two variances ## ## data: a and b ## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.5223017 8.4657950 ## sample estimates: ## ratio of variances ## 2.102784 # variance is homogene (can use var.equal=T in t.test) # t-test t.test(a,b, var.equal=TRUE, # variance is homogene (tested by var.test(a,b)) paired=FALSE) # samples are independent ## ## Two Sample t-test ## ## data: a and b ## t = -0.94737, df = 18, p-value = 0.356 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.93994 4.13994 ## sample estimates: ## mean of x mean of y ## 174.8 178.2 11.6 Non-parametric tests 11.7 Mann-Whitney U Rank Sum Test The dependent variable is ordinal or continuous. The data consist of a randomly selected sample of independent observations from two independent groups. The dependent variables for the two independent groups share a similar shape. 11.8 Wilcoxon test The Wilcoxon is a non-parametric test which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions. "],["tests-for-categorical-variables.html", "Chapter 12 Tests for categorical variables 12.1 Chi-squared tests", " Chapter 12 Tests for categorical variables Categorical variable can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. 12.1 Chi-squared tests The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher‚Äôs exact test can be used. data = rbind(c(83,35), c(92,43)) data ## [,1] [,2] ## [1,] 83 35 ## [2,] 92 43 chisq.test(data, correct=F) ## ## Pearson&#39;s Chi-squared test ## ## data: data ## X-squared = 0.14172, df = 1, p-value = 0.7066 chisq.test(testor,correct=F) ## Fisher‚Äôs Exact test R Example: Group TumourShrinkage-No TumourShrinkage-Yes Total 1 Treatment 8 3 11 2 Placebo 9 4 13 3 Total 17 7 24 The null hypothesis is that there is no association between treatment and tumour shrinkage. The alternative hypothesis is that there is some association between treatment group and tumour shrinkage. data = rbind(c(8,3), c(9,4)) data ## [,1] [,2] ## [1,] 8 3 ## [2,] 9 4 fisher.test(data) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: data ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.1456912 10.6433317 ## sample estimates: ## odds ratio ## 1.176844 The output Fisher‚Äôs exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is no evidence of an association between treatment group and tumour shrinkage. "],["multiple-testing.html", "Chapter 13 Multiple testing 13.1 The Bonferroni correction", " Chapter 13 Multiple testing When performing a large number of tests, the type I error is inflated: for Œ±=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x ‚Ä¶ (n-times) &lt;&lt;&lt; 0.095 The larger the number of tests performed, the higher the probability of a false rejection! Many data analysis approaches in genomics rely on itemby-item (i.e.¬†multiple) testing: Microarray or RNA-Seq expression profiles of ‚Äúnormal‚Äù vs ‚Äúperturbed‚Äù samples: gene-by-gene ChIP-chip: locus-by-locus RNAi and chemical compound screens Genome-wide association studies: marker-by-marker QTL analysis: marker-by-marker and trait-by-trait False positive rate (FPR) - the proportion of false positives among all resulst. False discovery rate (FDR) - the proportion of false positives among all significant results. Example: 20,000 genes, 100 hits, 10 of them wrong. FPR: 0.05% FDR: 10% 13.1 The Bonferroni correction The Bonferroni correction sets the significance cut-off at Œ±/n.¬† "],["sources.html", "Chapter 14 Sources 14.1 t-test", " Chapter 14 Sources One-Sample T-test in R 14.1 t-test The data set shows energy expend in two groups of women: stature library(ISwR) data(energy) attach(energy) ## The following objects are masked from energy (pos = 22): ## ## expend, stature head(energy) ## expend stature ## 1 9.21 obese ## 2 7.53 lean ## 3 7.48 lean ## 4 8.08 lean ## 5 8.09 lean ## 6 10.15 lean tapply(expend, stature, mean) ## lean obese ## 8.066154 10.297778 H0: there is no difference in averages between lean and obese. t.test(expend ~ stature) ## ## Welch Two Sample t-test ## ## data: expend by stature ## t = -3.8555, df = 15.919, p-value = 0.001411 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.459167 -1.004081 ## sample estimates: ## mean in group lean mean in group obese ## 8.066154 10.297778 Alternative hypothesis is true - means are different. Mean difference is in between -3.5 and 1.0 with a probability 95%. The risk of error is 0.15% 14.1.1 Two-tailed test Compair two sets of variables. data(intake) # from package ISwR attach(intake) ## The following objects are masked from intake (pos = 22): ## ## post, pre head(intake) ## pre post ## 1 5260 3910 ## 2 5470 4220 ## 3 5640 3885 ## 4 6180 5160 ## 5 6390 5645 ## 6 6515 4680 mean(post - pre) ## [1] -1320.455 Is difference of means significant? t.test(pre, post, paired=TRUE) ## ## Paired t-test ## ## data: pre and post ## t = 11.941, df = 10, p-value = 3.059e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1074.072 1566.838 ## sample estimates: ## mean of the differences ## 1320.455 The difference is significant with a probability 95%. The difference is in between 1074.1 and 1566.8 kJ/day "],["wilcoxon-signed-rank-test.html", "Chapter 15 Wilcoxon signed-rank test", " Chapter 15 Wilcoxon signed-rank test The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used to compare two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e.¬†it is a paired difference test). It can be used as an alternative to the paired Student‚Äôs t-test (also known as ‚Äút-test for matched pairs‚Äù or ‚Äút-test for dependent samples‚Äù) when the distribution of the difference between two samples‚Äô means cannot be assumed to be normally distributed. A Wilcoxon signed-rank test can be used to determine whether two dependent samples were selected from populations having the same distribution. wiki Two data samples are matched if they come from repeated observations of the same subject. Using the Wilcoxon Signed-Rank Test, we can decide whether the corresponding data population distributions are identical without assuming them to follow the normal distribution. Example In the built-in data set named immer, the barley yield in years 1931 and 1932 of the same field are recorded. The yield data are presented in the data frame columns Y1 and Y2. library(MASS) head(immer) ## Loc Var Y1 Y2 ## 1 UF M 81.0 80.7 ## 2 UF S 105.4 82.3 ## 3 UF V 119.7 80.4 ## 4 UF T 109.7 87.2 ## 5 UF P 98.3 84.2 ## 6 W M 146.6 100.4 Problem Without assuming the data to have normal distribution, test at .05 significance level if the barley yields of 1931 and 1932 in data set immer have identical data distributions. Solution The null hypothesis is that the barley yields of the two sample years are identical populations. To test the hypothesis, we apply the wilcox.test function to compare the matched samples. For the paired test, we set the ‚Äúpaired‚Äù argument as TRUE. As the p-value turns out to be 0.005318, and is less than the .05 significance level, we reject the null hypothesis. wilcox.test(immer$Y1, immer$Y2, paired=TRUE) ## Warning in wilcox.test.default(immer$Y1, immer$Y2, paired = TRUE): cannot compute exact p- ## value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: immer$Y1 and immer$Y2 ## V = 368.5, p-value = 0.005318 ## alternative hypothesis: true location shift is not equal to 0 Answer At .05 significance level, we conclude that the barley yields of 1931 and 1932 from the data set immer are nonidentical populations. Source Wilcoxon Signed-Rank Test "],["analysis-of-variance-anova.html", "Chapter 16 Analysis of Variance (ANOVA) 16.1 One-way ANOVA 16.2 Sources", " Chapter 16 Analysis of Variance (ANOVA) 16.1 One-way ANOVA variance = SS/df, where SS - sum of squares and df - degree of freedom \\(SS = \\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}\\), where \\(\\mu\\) is the sample mean n is the sample size \\(var(x) = \\frac{1}{n}{\\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}}\\) SST = SSE + SSC = W + B, where SST - Total Sum of Squares SSE - Error Sum of Squares - within (W) SSC - Sum of Squares Columns (treatmens) - between (B) C - columns (treatments) N - total number of observations Mean squared of columns - MSC = SSC/df_columns, where df_columns = C-1 Mean squared of error - MSE = SSE/df_error, where df_error = N-C Sum of squares (total) - SST, where df_total = N-1 F-statistics - F = MSC/MSE Let‚Äôs calculate degree of freedom for our example: df_columns = 3-1 = 2, MSC = SSC/2 df_error = 21-3 = 18, MSE = SSE/18 df_total = 21-1 = 20 # 3 groups of students with scores (1-100): a = c(82,93,61,74,69,70,53) b = c(71,62,85,94,78,66,71) c = c(64,73,87,91,56,78,87) sq = function(x) { sum((x - mean(x))^2) } sq(a) ## [1] 1039.429 sq(b) ## [1] 751.4286 sq(c) ## [1] 1021.714 Using R packages: # data # Number of calories consumed by month: may &lt;- c(2166, 1568, 2233, 1882, 2019) sep &lt;- c(2279, 2075, 2131, 2009, 1793) dec &lt;- c(2226, 2154, 2583, 2010, 2190) d &lt;- stack(list(may=may, sep=sep, dec=dec)) d ## values ind ## 1 2166 may ## 2 1568 may ## 3 2233 may ## 4 1882 may ## 5 2019 may ## 6 2279 sep ## 7 2075 sep ## 8 2131 sep ## 9 2009 sep ## 10 1793 sep ## 11 2226 dec ## 12 2154 dec ## 13 2583 dec ## 14 2010 dec ## 15 2190 dec names(d) ## [1] &quot;values&quot; &quot;ind&quot; oneway.test(values ~ ind, data=d, var.equal=TRUE) ## ## One-way analysis of means ## ## data: values and ind ## F = 1.7862, num df = 2, denom df = 12, p-value = 0.2094 # alternative using aov res &lt;- aov(values ~ ind, data = d) res ## Call: ## aov(formula = values ~ ind, data = d) ## ## Terms: ## ind Residuals ## Sum of Squares 174664.1 586719.6 ## Deg. of Freedom 2 12 ## ## Residual standard error: 221.1183 ## Estimated effects may be unbalanced summary(res) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ind 2 174664 87332 1.786 0.209 ## Residuals 12 586720 48893 16.2 Sources Example for one-way ANOVA: youtube by Brandon Foltz "],["t-test-anova-difference.html", "Chapter 17 t-test ANOVA difference", " Chapter 17 t-test ANOVA difference The t-test and ANOVA examine whether group means differ from one another. The t-test compares two groups, while ANOVA can do more than two groups. The t-test ANOVA have three assumptions: independence assumption (the elements of one sample are not related to those of the other sample), normality assumption (samples are randomly drawn from the normally distributed populstions with unknown population means; otherwise the means are no longer best measures of central tendency, thus test will not be valid), and equal variance assumption (the population variances of the two groups are equal) ANCOVA (analysis of covariance) includes covariates, interval independent variables, in the right-hand side to control their impacts. MANOVA (multivariate analysis of variance) has more than one left-hand side variable. t-test and ANOVA usage. "],["chi-squared-test.html", "Chapter 18 Chi-squared test 18.1 Multinomial Goodness of Fit", " Chapter 18 Chi-squared test 18.1 Multinomial Goodness of Fit A population is called multinomial if its data is categorical and belongs to a collection of discrete non-overlapping classes. The null hypothesis for goodness of fit test for multinomial distribution is that the observed frequency fi is equal to an expected count \\[e_i\\] in each category. It is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level Œ±. Example Survey response about the student‚Äôs smoking habit: ‚ÄúHeavy,‚Äù ‚ÄúRegul‚Äù (regularly), ‚ÄúOccas‚Äù (occasionally) and ‚ÄúNever.‚Äù The Smoke data is multinomial. library(MASS) levels(survey$Smoke) ## [1] &quot;Heavy&quot; &quot;Never&quot; &quot;Occas&quot; &quot;Regul&quot; smoke_freq = table(survey$Smoke) smoke_freq ## ## Heavy Never Occas Regul ## 11 189 19 17 # estimated probabilities smoke_prob = c(heavy = .045, never = .795, occas = .085, regul = .075) Determine whether the sample data in smoke_freq supports estimated probabilities in smoke_prob at .05 significance level. chisq.test(smoke_freq, p=smoke_prob) ## ## Chi-squared test for given probabilities ## ## data: smoke_freq ## X-squared = 0.10744, df = 3, p-value = 0.9909 As the p-value 0.991 is greater than the .05 significance level, we do not reject the null hypothesis that the sample data in survey supports the smoking statistics. Sources Multinomial Goodness of Fit "],["non-parametric-methods.html", "Chapter 19 Non-parametric Methods 19.1 Sign Test 19.2 Wilcoxon Signed-Rank Test 19.3 Mann-Whitney-Wilcoxon Test 19.4 Kruskal-Wallis Test", " Chapter 19 Non-parametric Methods A statistical method is called non-parametric if it makes no assumption on the population distribution or sample size. This is in contrast with most parametric methods in elementary statistics that assume the data is quantitative, the population has a normal distribution and the sample size is sufficiently large. In general, conclusions drawn from non-parametric methods are not as powerful as the parametric ones. However, as non-parametric methods make fewer assumptions, they are more flexible, more robust, and applicable to non-quantitative data. 19.1 Sign Test 19.2 Wilcoxon Signed-Rank Test 19.3 Mann-Whitney-Wilcoxon Test 19.4 Kruskal-Wallis Test "],["correlation.html", "Chapter 20 Correlation", " Chapter 20 Correlation df &lt;- mtcars # correlation miles per galon of petrol ~ horse power cor.test(x = df$mpg, y = df$hp) # get parameters of analysed correlation fit &lt;- cor.test(x = df$mpg, y = df$hp) str(fit) fit$p.value # the same correlation in &#39;formula&#39; form (see. ?cor.test) ?cor.test cor.test(~mpg+hp, df) # Plot using generic plot function of ggplot from ggplot2 package plot(x = df$mpg, y = df$hp) library(ggplot2) # add color by number of engine&#39;s cylinders ggplot(df, aes(x = mpg, y = hp, col = factor(cyl)))+geom_point(size=5) # Subset of necessary data from mtcars df.sub &lt;- df[,c(1,3:7)] # Scatterplots for all pairs of variables pairs(df.sub) # Correlations of all pairs of variables cor(df.sub) # Correlation using corr.test function from &#39;psych&#39; package # Correlation for all pairs of variables library(psych) fit.sub &lt;- corr.test(df.sub) fit.sub$r # correlations fit.sub$p # dependecies of variables "],["methods-and-algorithms-of-machine-learning.html", "Chapter 21 Methods and algorithms of machine learning", " Chapter 21 Methods and algorithms of machine learning Regression Analysis * Ordinary Least Squares Regression (OLSR) * Linear Regression * Logistic Regression * Stepwise Regression * Polynomial Regression * Locally Estimated Scatterplot Smoothing (LOESS) Distance-based algorithms * k-Nearest Neighbor (kNN) * Learning Vector Quantization (LVQ) * Self-organizing Map (SOM) Regularization Algorithms * Ridge Regression * Least Absolute Shrinkage and Selection Operator (LASSO) * Elastic Net * Least-Angle Regression (LARS) Decision Tree Algorithms * Classification and Regression Tree (CART) * Iterative Dichotomiser 3 (ID3) * C4.5 and C5.0 * Chi-squared Automatic Interation Detection (CHAID) * Random Forest * Conditional Decision Trees Bayesian Algorithms * Naive Bayes * Gaussian Naive Bayes * Multinomial Naive Bayes * Bayesian Belief Network (BBN) * Bayesian Network (BN) Clustering Algorithms * k-Means * k-Medians * Partitioning Around MEdoids (PAM) * Hierarchical Clustering Association Rule Mining Algorithms * Apriori algorithm * Eclat algorithm * FP-growth algorithm * Context Based Rule Mining Artifical Neural Network Algorithms * Perceptron * Back-Propagation * Hopfield Network * Radial Basis Function Network (RBFN) Deep Learining Algorithms * Deep Boltzmann Machine (DBM) * Deep Belief Networks (DBN) * Convolutional NEural Network (CNN) * Stacked Auto-Encoders Dimensionality Reduction Algorithms * Principal Component Analysis (PCA) * Principal Compnent Regression (PCR) * Partial LEast Squares Regression (PLSR) * Multidimensional Scaling (MDS) * Linear Discriminant Analysis (LDA) * Mixtrue Discriminant Analysis (MDA) * Quadratic Discriminant Analysis (QDA) 1. PCA (linear) 2. t-SNE (non-parametric/ nonlinear) 3. Sammon mapping (nonlinear) 4. Isomap (nonlinear) 5. LLE (nonlinear) 6. CCA (nonlinear) 7. SNE (nonlinear) 8. MVU (nonlinear) 9. Laplacian Eigenmaps (nonlinear) Ensemble Algorithms * Boosting * Bagging * AdaBoost * Stacked Generalization (blending) * Gradient Boosting Machines (GBM) Text Mining * Automatic summarization * Named entity recognition (NER) * Optical character recognition (OCR) * Part-of-speech tagging * Sentiment analysis * Speech recognition * Topic Modeling "],["split-data-into-train-and-test-subsets.html", "Chapter 22 Split data into train and test subsets", " Chapter 22 Split data into train and test subsets Here you can find several simple approaches to split data into train and test subset to fit and to test parameters of your model. We want to take 0.8 of our initial data to train our model. Data: datasets::iris. First approach is to create a vector containing randomly selected row ids and to apply this vector to split data. inTrain = sample(nrow(iris), nrow(iris)*0.8) # split data train = iris[inTrain, ] test = iris[-inTrain, ] The same idea to split data as before using caret package. The advantage is that createDataPartition function allows to split data many times and use these subsets to estimate parameters of our model. library(caret) trainIndex &lt;- createDataPartition(iris$Species, p=.8, list = FALSE, # if FALSE - create a vector/matrix, if TRUE - create a list times = 1) # how many subsets # split data train &lt;- iris[trainIndex, ] test &lt;- iris[-trainIndex, ] Another approch is to create a logical vecotor containing randomly distributed true/false and apply this vector to subset data. inTrain = sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.8,0.2)) # select data train = iris[inTrain, ] test = iris[!inTrain, ] Using caTools. library(caTools) inTrain = sample.split(iris, SplitRatio = .8) train = subset(iris, inTrain == TRUE) test = subset(iris, inTrain == FALSE) Using dplyr library(dplyr) iris$id &lt;- 1:nrow(iris) train &lt;- iris %&gt;% dplyr::sample_frac(.8) test &lt;- dplyr::anti_join(iris, train, by = &#39;id&#39;) "],["estimate-model-accuracy.html", "Chapter 23 Estimate model accuracy 23.1 Continuous variables 23.2 Discret variables", " Chapter 23 Estimate model accuracy 23.1 Continuous variables # Estimate model precision for one continious dependent variable (Y) # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home ### GENERATE DATA # Generate data for given function y.func &lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3} ### Parameters for data generation my.seed &lt;- 123 # set seed n.all &lt;- 100 # number of samples train.percent &lt;- 0.85 # percent of train data res.sd &lt;- 1 # standard deviation of noise x.min &lt;- 5 # minimal value x.max &lt;- 104 # maximal value # generate random x values set.seed(my.seed) x &lt;- runif(x.min, x.max, n = n.all) # set normally distributed noise for each value set.seed(my.seed) res &lt;- rnorm(mean = 0, sd = res.sd, n = n.all) # calculate y values using funcion and add noise y &lt;- y.func(x) + res # vector for subseting train data (contains numbers of selected elements in x) set.seed(my.seed) inTrain &lt;- sample(seq_along(x), size = train.percent*n.all) inTrain # Subset vectors with train and test data subsets # Train data x.train &lt;- x[inTrain] y.train &lt;- y[inTrain] # Test data x.test &lt;- x[-inTrain] y.test &lt;- y[-inTrain] # x and y to display given function &#39;y.func&#39; x.line &lt;- seq(x.min, x.max, length = n.all) y.line &lt;- y.func(x.line) x.line y.line ### PLOT PRIMARY DATA #------------------------------------------------------------------------------ # Plot parameters x.lim &lt;- c(x.min, x.max) y.lim &lt;- c(min(y), max(y)) # Plot generated train data plot(x.train, y.train, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = x.lim, ylim = y.lim, cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # Header mtext(&#39;Initial train data and real function&#39;, side = 3) # test values points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # true function lines(x.line, y.line, lwd = 2, lty = 2) # legend legend(&#39;bottomright&#39;, legend = c(&#39;train data&#39;, &#39;test data&#39;, &#39;f(X)=4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3&#39;), pch = c(16, 16, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;), lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2) #------------------------------------------------------------------------------ ### MODEL # For modeling spline with degree of freedoms from 2 (straight line) to 50 (1/2 of N) # To demonstrate spline with df = 6 mod &lt;- smooth.spline(x = x.train, y = y.train, df = 6) # Model data for error estimation y.model.train &lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test &lt;- predict(mod, data.frame(x = x.test))$y[, 1] # Calculate mean squared errors for test and model data MSE &lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) names(MSE) &lt;- c(&#39;train&#39;, &#39;test&#39;) round(MSE, 2) # Now let&#39;s make splines for all degrees of freedoms (2-50) # Max degrees of fredom for spline model max.df &lt;- 50 tbl &lt;- data.frame(df = 2:max.df) # Create result table for recording calculated MSE tbl$MSE.train &lt;- 0 # Column for train data tbl$MSE.test &lt;- 0 # Column for test data # For all degrees of freedoms for (i in 2:max.df) { # build model mod &lt;- smooth.spline(x = x.train, y = y.train, df = i) # model values from calculaton of errors y.model.train &lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test &lt;- predict(mod, data.frame(x = x.test))$y[, 1] # Calculate MSE for predicted and train data sets MSE &lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) # record errors into the table tbl[tbl$df == i, c(&#39;MSE.train&#39;, &#39;MSE.test&#39;)] &lt;- MSE } head(tbl) ### PLOT - Dependency of MSEs from model flexibility (degree of freedoms) #------------------------------------------------------------------------------ plot(x = tbl$df, y = tbl$MSE.test, type = &#39;l&#39;, col = &#39;red&#39;, lwd = 2, xlab = &#39;Degree of freedoms&#39;, ylab = &#39;MSE&#39;, ylim = c(min(tbl$MSE.train, tbl$MSE.test), max(tbl$MSE.train, tbl$MSE.test)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # Header mtext(&#39;MSE dependency from degree of freedoms&#39;, side = 3) points(x = tbl$df, y = tbl$MSE.test, pch = 21, col = &#39;red&#39;, bg = &#39;red&#39;) lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) # unrecoverable error abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) # legend legend(&#39;topright&#39;, legend = c(&#39;train&#39;, &#39;test&#39;), pch = c(NA, 16), col = c(grey(0.2), &#39;red&#39;), lty = c(1, 1), lwd = c(2, 2), cex = 1.2) #------------------------------------------------------------------------------ # Detect degree of freedom with minimal error for test data min.MSE.test &lt;- min(tbl$MSE.test) df.min.MSE.test &lt;- tbl[tbl$MSE.test == min.MSE.test, &#39;df&#39;] df.min.MSE.test # Compromize for flexibility and pricision of model df.my.MSE.test &lt;- df.min.MSE.test my.MSE.test &lt;- tbl[tbl$df == df.my.MSE.test, &#39;MSE.test&#39;] my.MSE.test # Display optimal df abline(v = df.my.MSE.test, lty = 2, lwd = 2) points(x = df.my.MSE.test, y = my.MSE.test, pch = 15, col = &#39;blue&#39;) mtext(df.my.MSE.test, side = 1, line = -1, at = df.my.MSE.test, col = &#39;blue&#39;, cex = 1.2) ### PLOT OPTIMAL MODEL mod.MSE.test &lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) # for smoothed curves of model x.model.plot &lt;- seq(x.min, x.max, length = 250) y.model.plot &lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] x.lim &lt;- c(x.min, x.max) y.lim &lt;- c(min(y), max(y)) # Plot train data with noise #------------------------------------------------------------------------------ plot(x.train, y.train, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = x.lim, ylim = y.lim, cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # Header mtext(&#39;Given data and optimal model&#39;, side = 3) # Test data points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # True function lines(x.line, y.line, lwd = 2, lty = 2) # Model lines(x.model.plot, y.model.plot, lwd = 2, col = &#39;blue&#39;) # legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;, &#39;model&#39;), pch = c(16, 16, NA, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;, &#39;blue&#39;), lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2) #------------------------------------------------------------------------------ 23.2 Discret variables # Estimate model precision for one descrete dependent variable (Y) # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home library(&#39;mlbench&#39;) library(&#39;class&#39;) library(&#39;car&#39;) library(&#39;class&#39;) library(&#39;e1071&#39;) library(&#39;MASS&#39;) # discrete function rules &lt;- function(x1, x2){ ifelse((x1 &gt; 20 &amp; x2 &lt; 50) | (x1 &lt; 18 &amp; x2 &gt; 52), 1, 0) } # parameters my.seed &lt;- 123 n &lt;- 100 train.percent &lt;- 0.85 # Generate data set set.seed(my.seed) x1 &lt;- rnorm(20, 3.7, n = n) set.seed(my.seed + 1) x2 &lt;- rnorm(50, 3.3, n = n) # Vector to subset train and test data set.seed(my.seed) inTrain &lt;- sample(seq_along(x1), train.percent*n) x1.train &lt;- x1[inTrain] x2.train &lt;- x2[inTrain] x1.test &lt;- x1[-inTrain] x2.test &lt;- x2[-inTrain] # Create descrete parameter according to given function y.train &lt;- rules(x1.train, x2.train) y.test &lt;- rules(x1.test, x2.test) # Table with train data df.train.1 &lt;- data.frame(x1 = x1.train, x2 = x2.train, y = y.train) df.train.1 # Table with test data df.test.1 &lt;- data.frame(x1 = x1.test, x2 = x2.test) df.test.1 ### PLOT TEST DATA x1.grid &lt;- rep(seq(floor(min(x1)), ceiling(max(x1)), by = 1), ceiling(max(x2)) - floor(min(x2)) + 1) x2.grid &lt;- rep(seq(floor(min(x2)), ceiling(max(x2)), by = 1), each = ceiling(max(x1)) - floor(min(x1)) + 1) # –∫–ª–∞—Å—Å—ã –¥–ª—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π —Å–µ—Ç–∫–∏ y.grid &lt;- rules(x1.grid, x2.grid) # —Ñ—Ä–µ–π–º –¥–ª—è —Å–µ—Ç–∫–∏ df.grid.1 &lt;- data.frame(x1 = x1.grid, x2 = x2.grid, y = y.grid) # —Ü–≤–µ—Ç–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ cls &lt;- c(&#39;blue&#39;, &#39;orange&#39;) cls.t &lt;- c(rgb(0, 0, 1, alpha = 0.5), rgb(1,0.5,0, alpha = 0.5)) # –≥—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ plot(df.grid.1$x1, df.grid.1$x2, pch = &#39;¬∑&#39;, col = cls[df.grid.1[, &#39;y&#39;] + 1], xlab = &#39;X1&#39;, ylab = &#39;Y1&#39;, main = &#39;–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞, —Ñ–∞–∫—Ç&#39;) # —Ç–æ—á–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π points(df.train.1$x1, df.train.1$x2, pch = 21, bg = cls.t[df.train.1[, &#39;y&#39;] + 1], col = cls.t[df.train.1[, &#39;y&#39;] + 1]) ### 1. Build a model using Naive Bayess classifier nb &lt;- naiveBayes(y ~ ., data = df.train.1) # –ø–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –∫–∞–∫ –∫–ª–∞—Å—Å—ã y.nb.train &lt;- ifelse(predict(nb, df.train.1[, -3], type = &quot;raw&quot;)[, 2] &gt; 0.5, 1, 0) # –≥—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ plot(df.grid.1$x1, df.grid.1$x2, pch = &#39;¬∑&#39;, col = cls[df.grid.1[, &#39;y&#39;] + 1], xlab = &#39;X1&#39;, ylab = &#39;Y1&#39;, main = &#39;–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞, –º–æ–¥–µ–ª—å naiveBayes&#39;) # —Ç–æ—á–∫–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ –º–æ–¥–µ–ª–∏ points(df.train.1$x1, df.train.1$x2, pch = 21, bg = cls.t[y.nb.train + 1], col = cls.t[y.nb.train + 1]) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ tbl &lt;- table(y.train, y.nb.train) tbl Acc &lt;- sum(diag(tbl)) / sum(tbl) Acc cat(&#39;–ö–∞–∫ –º–æ–∂–Ω–æ –≤–∏–¥–µ—Ç—å –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ, —Ç—É —á–∞—Å—Ç—å –∂—ë–ª—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∞ –≤ –ª–µ–≤–æ–π –≤–µ—Ä—Ö–Ω–µ–π –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–µ–≤–µ—Ä–Ω–æ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ä–µ—à–∞—é—â–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Ä–∞–∑—Ä—ã–≤ –∂—ë–ª—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ —Å–∏–Ω–∏–º. –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Ç–æ–º—É, —á—Ç–æ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –º–µ—Ç–æ–¥ –∏—Å—Ö–æ–¥–∏—Ç –∏–∑ –¥–æ–ø—É—â–µ–Ω–∏—è –æ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—ä—è—Å–Ω—è—é—â–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –Ω–∏—Ö. –û–¥–Ω–∞–∫–æ –≤ —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ —ç—Ç–æ –¥–æ–ø—É—â–µ–Ω–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è. –°–¥–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –∫–ª–∞—Å—Å–æ–≤ Y –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –∏ –æ—Ü–µ–Ω–∏–º —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ö–∞–∫ –º–æ–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∏–∂–µ, —á–µ–º –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ. –£—á–∏—Ç—ã–≤–∞—è, –∫–∞–∫ –≤–µ–¥—ë—Ç —Å–µ–±—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ, —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏ –¥–æ–≤–µ—Ä—è—Ç—å –Ω–µ —Å—Ç–æ–∏—Ç.&#39;) # –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É y.nb.test &lt;- ifelse(predict(nb, df.test.1, type = &quot;raw&quot;)[, 2] &gt; 0.5, 1, 0) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ tbl &lt;- table(y.test, y.nb.test) tbl # —Ç–æ—á–Ω–æ—Å—Ç—å, –∏–ª–∏ –≤–µ—Ä–Ω–æ—Å—Ç—å (Accuracy) Acc &lt;- sum(diag(tbl)) / sum(tbl) Acc ### 2. Build kNN model for k=3 y.knn.train &lt;- knn(train = scale(df.train.1[, -3]), test = scale(df.train.1[, -3]), cl = df.train.1$y, k = 3) # –≥—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ plot(df.grid.1$x1, df.grid.1$x2, pch = &#39;¬∑&#39;, col = cls[df.grid.1[, &#39;y&#39;] + 1], xlab = &#39;X1&#39;, ylab = &#39;Y1&#39;, main = &#39;–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞, –º–æ–¥–µ–ª—å kNN&#39;) # —Ç–æ—á–∫–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ –º–æ–¥–µ–ª–∏ points(df.train.1$x1, df.train.1$x2, pch = 21, bg = cls.t[as.numeric(y.knn.train)], col = cls.t[as.numeric(y.knn.train)]) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ tbl &lt;- table(y.train, y.knn.train) tbl # —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy) Acc &lt;- sum(diag(tbl)) / sum(tbl) Acc # –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É y.knn.test &lt;- knn(train = scale(df.train.1[, -3]), test = scale(df.test.1[, -3]), cl = df.train.1$y, k = 3) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ tbl &lt;- table(y.test, y.knn.test) tbl # —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy) Acc &lt;- sum(diag(tbl)) / sum(tbl) Acc # As we can see, kNN method is more accurate than Naive Bayes for this case. "],["model-evaluation.html", "Chapter 24 Model evaluation", " Chapter 24 Model evaluation bias and variance are two sources of error in Machine Learning. Bias: error from incorrect model assumptions. Err(Training) High bias means underfitting Variance: error from sensitivity to small fluctuations in the training set Err(Testing) - Err(Training) High variance means overfitting Bias-variance tradeoff - Finding an adequate balance between model learning and model generalization. To reduce model bias: 1. Increase the model size. 2. Modify input features using error analysis. 3. Reduce or eliminate regularization. 4. Modify model architecture. To reduce model variance: 1. Add more training data. 2. Add regularization (this reduce variance but increase bias). 3. Peform feature selection. 4. Decrease model size. Strategies to build ensemble model: - Bagging (Bootstrap AGGregatING) (Random Forests) - Boosting (AdaBoost, Gradient Boosted Trees) - Stacking (Linear regression, elastic net regression) library(caret) # Calculates performance metrics across all resamples resamples() # Correlation among the base learners&#39; predictions modelCor() "],["cross-validation-and-bootstrep.html", "Chapter 25 Cross-validation and Bootstrep", " Chapter 25 Cross-validation and Bootstrep # Cross-validation and bootstep # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home library(&#39;ISLR&#39;) # –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Auto library(&#39;GGally&#39;) # –º–∞—Ç—Ä–∏—á–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ library(&#39;boot&#39;) # —Ä–∞—Å—á—ë—Ç –æ—à–∏–±–∫–∏ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π my.seed &lt;- 1 # Get data and do primary visual inspection head(Auto) str(Auto) ggpairs(Auto[, -9]) # —Ç–æ–ª—å–∫–æ mpg ~ horsepower plot(Auto$horsepower, Auto$mpg, xlab = &#39;horsepower&#39;, ylab = &#39;mpg&#39;, pch = 21, col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4)) # –æ–±—â–µ–µ —á–∏—Å–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π n &lt;- nrow(Auto) # –¥–æ–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ train.percent &lt;- 0.5 # –≤—ã–±—Ä–∞—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É set.seed(my.seed) inTrain &lt;- sample(n, n * train.percent) plot(Auto$horsepower[inTrain], Auto$mpg[inTrain], xlab = &#39;horsepower&#39;, ylab = &#39;mpg&#39;, pch = 21, col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4)) points(Auto$horsepower[-inTrain], Auto$mpg[-inTrain], pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4)) legend(&#39;topright&#39;, pch = c(16, 16), col = c(&#39;blue&#39;, &#39;red&#39;), legend = c(&#39;test&#39;, &#39;train&#39;)) ### Build different types of models ## 1. Linear model mpg = a+b*horsepower # –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏: –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–±–ª—Ü–æ–≤ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞–ø—Ä—è–º—É—é attach(Auto) # –ø–æ–¥–≥–æ–Ω–∫–∞ –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ fit.lm.1 &lt;- lm(mpg ~ horsepower, subset = inTrain) # —Å—á–∏—Ç–∞–µ–º MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ mean((mpg[-inTrain] - predict(fit.lm.1, Auto[-inTrain, ]))^2) # –æ—Ç—Å–æ–µ–¥–∏–Ω–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏ detach(Auto) #------------------------------------------------------------------------------ ## 2. Square Model mpg = a + b*horsepower + c*horsepower^2 # –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏: –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–±–ª—Ü–æ–≤ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞–ø—Ä—è–º—É—é attach(Auto) # –ø–æ–¥–≥–æ–Ω–∫–∞ –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ fit.lm.2 &lt;- lm(mpg ~ poly(horsepower, 2), subset = inTrain) # —Å—á–∏—Ç–∞–µ–º MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ mean((mpg[-inTrain] - predict(fit.lm.2, Auto[-inTrain, ]))^2) detach(Auto) #------------------------------------------------------------------------------ ## 3. Qubiq Model mpg = a + b*horsepower + c*horsepower^2 + d*horsepower^3 # –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏: –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–±–ª—Ü–æ–≤ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞–ø—Ä—è–º—É—é attach(Auto) # –ø–æ–¥–≥–æ–Ω–∫–∞ –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ fit.lm.3 &lt;- lm(mpg ~ poly(horsepower, 3), subset = inTrain) # —Å—á–∏—Ç–∞–µ–º MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ mean((mpg[-inTrain] - predict(fit.lm.3, Auto[-inTrain, ]))^2) detach(Auto) #------------------------------------------------------------------------------ ## LOOCV - Cross validation # –ø–æ–¥–≥–æ–Ω–∫–∞ –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ fit.glm &lt;- glm(mpg ~ horsepower, data = Auto) # —Å—á–∏—Ç–∞–µ–º LOOCV-–æ—à–∏–±–∫—É cv.err &lt;- cv.glm(Auto, fit.glm) # —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ -- –ø–æ —Ñ–æ—Ä–º—É–ª–µ LOOCV-–æ—à–∏–±–∫–∏, # –≤—Ç–æ—Ä–æ–µ -- —Å –ø–æ–ø—Ä–∞–≤–∫–æ–π –Ω–∞ —Å–º–µ—â–µ–Ω–∏–µ cv.err$delta[1] # –≤–µ–∫—Ç–æ—Ä —Å LOOCV-–æ—à–∏–±–∫–∞–º–∏ cv.err.loocv &lt;- rep(0, 5) names(cv.err.loocv) &lt;- 1:5 # —Ü–∏–∫–ª –ø–æ —Å—Ç–µ–ø–µ–Ω—è–º –ø–æ–ª–∏–Ω–æ–º–æ–≤ for (i in 1:5){ fit.glm &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.err.loocv[i] &lt;- cv.glm(Auto, fit.glm)$delta[1] } # —Ä–µ–∑—É–ª—å—Ç–∞—Ç cv.err.loocv #------------------------------------------------------------------------------ ## k-something cross-validation k-–∫—Ä–∞—Ç–Ω–∞—è –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ # –æ—Ü–µ–Ω–∏–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –º–µ–Ω—è—è —Å—Ç–µ–ø–µ–Ω—å # –≤–µ–∫—Ç–æ—Ä —Å –æ—à–∏–±–∫–∞–º–∏ –ø–æ 10-–∫—Ä–∞—Ç–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ cv.err.k.fold &lt;- rep(0, 5) names(cv.err.k.fold) &lt;- 1:5 # —Ü–∏–∫–ª –ø–æ —Å—Ç–µ–ø–µ–Ω—è–º –ø–æ–ª–∏–Ω–æ–º–æ–≤ for (i in 1:5){ fit.glm &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.err.k.fold[i] &lt;- cv.glm(Auto, fit.glm, K = 10)$delta[1] } # result cv.err.k.fold # Compare with MSE err.test ##### BOOTSTREP head(Portfolio) str(Portfolio) # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏—Å–∫–æ–º–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ alpha.fn &lt;- function(data, index){ X = data$X[index] Y = data$Y[index] (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2*cov(X, Y)) } # —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å alpha –ø–æ –≤—Å–µ–º 100 –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º alpha.fn(Portfolio, 1:100) # —Å–æ–∑–¥–∞—Ç—å –±—É—Ç—Å—Ç—Ä–µ–ø-–≤—ã–±–æ—Ä–∫—É –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å alpha set.seed(my.seed) alpha.fn(Portfolio, sample(100, 100, replace = T)) # —Ç–µ–ø–µ—Ä—å -- –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ boot(Portfolio, alpha.fn, R = 1000) #–ë—É—Ç—Å—Ç—Ä–µ–ø –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —Ä–∞—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –º–Ω–æ–≥–æ —Ä–∞–∑, –¥–µ–ª–∞—è –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏ –∏–∑ –Ω–∞—à–∏—Ö 100 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π. –í –∏—Ç–æ–≥–µ —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º –º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ—à–∏–±–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞, –Ω–µ –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –¥–æ–ø—É—â–µ–Ω–∏—è –æ –∑–∞–∫–æ–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ Œ±=0.576 —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –æ—à–∏–±–∫–æ–π sŒ±^=0.089. #T–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ # –ü—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–æ–±–ª–µ–º—ã –≤ –æ—Å—Ç–∞—Ç–∫–∞—Ö –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–≤–µ—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –æ—à–∏–±–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –û–±–æ–π—Ç–∏ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –º–æ–∂–Ω–æ, –ø—Ä–∏–º–µ–Ω–∏–≤ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —ç—Ç–∏—Ö –æ—à–∏–±–æ–∫ –±—É—Ç—Å—Ç—Ä–µ–ø. # –û—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ---------------------------- # –æ—Ü–µ–Ω–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ # mpg = beta_0 + beta_1 * horsepower —Å –ø–æ–º–æ—â—å—é –±—É—Ç—Å—Ç—Ä–µ–ø–∞, # —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –æ—à–∏–±–æ–∫ –ø–æ –ú–ù–ö # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –ü–õ–† –ø–æ –≤—ã–±–æ—Ä–∫–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö boot.fn &lt;- function(data, index){ coef(lm(mpg ~ horsepower, data = data, subset = index)) } boot.fn(Auto, 1:n) # –ø—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –±—É—Ç—Å—Ç—Ä–µ–ø-–≤—ã–±–æ—Ä–∫–µ set.seed(my.seed) boot.fn(Auto, sample(n, n, replace = T)) # –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é boot –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ # (1000 –≤—ã–±–æ—Ä–æ–∫ —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏) boot(Auto, boot.fn, 1000) # —Å—Ä–∞–≤–Ω–∏–º —Å –ú–ù–ö attach(Auto) summary(lm(mpg ~ horsepower))$coef detach(Auto) # –æ—Ü–µ–Ω–∫–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –ú–ù–ö -- –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ —Å –¥–æ–ø—É—â–µ–Ω–∏—è–º–∏ # –≤—ã—á–∏—Å–ª–∏–º –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ boot.fn.2 &lt;- function(data, index){ coef(lm(mpg ~ poly(horsepower, 2), data = data, subset = index)) } # –ø—Ä–∏–º–µ–Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é –∫ 1000 –±—É—Ç—Å—Ä–µ–ø-–≤—ã–±–æ—Ä–∫–∞–º set.seed(my.seed) boot(Auto, boot.fn, 1000) "],["linear-regression-1.html", "Chapter 26 Linear Regression 26.1 Linear regression - theory 26.2 Generate random data set for the linear model 26.3 Practical example 26.4 Mean squared error (MSE) 26.5 Linear model in R 26.6 Linear regression model for multiple parameters 26.7 Choosing explanatory variables for the model 26.8 Assessment of model performance for categorical data. 26.9 Confidence intervals for linear model 26.10 Practical examples for linear model regression", " Chapter 26 Linear Regression 26.1 Linear regression - theory Linear regression model is a line y=ax+b, where sum of distances between all y=axi+b and given yi (sum of squares) is minimal. Assume that there is approximately a linear relationship between X and Y: \\[ Y \\approx \\beta_0 + \\beta_1X\\] where \\(\\beta\\)0 is an intercept and \\(\\beta\\)1 is a slope Parameters of the line could be calculated using least squares methods: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}} \\] \\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x} \\] 26.2 Generate random data set for the linear model Suppose we want to simulate from the following linear model: y = \\(\\beta\\)0 + \\(\\beta\\)1x + \\(\\epsilon\\), where \\(\\epsilon\\) ~ N(0,22). Assume x ~ N(0,12), \\(\\beta\\)0 = 0.5, \\(\\beta\\)1 = 2. set.seed(20) x &lt;-rnorm(100) e &lt;- rnorm(100, 0, 2) y &lt;- 0.5 + 2*x + e summary(y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -6.4084 -1.5402 0.6789 0.6893 2.9303 6.5052 plot(x,y) 26.3 Practical example Practical example from Wikipedia Set of data: (1,6), (2,5), (3, 7), (4,10) x &lt;- c(1,2,3,4) y &lt;- c(6,5,7,10) plot(y~x, xlim=c(0,5), ylim=c(4,10)) abline(3.5, 1.4) r &lt;- lm(y~x) segments(x, y, x, r$fitted.values, col=&quot;green&quot;) We have to find the line corresponding to the minimal sum of errors (distances from the each point to this line): 1. For all points: \\[\\beta_1 + 1\\beta_2 = 6\\] \\[\\beta_1 + 2\\beta_2 = 5\\] \\[\\beta_1 + 3\\beta_2 = 7\\] \\[\\beta_1 + 4\\beta_2 = 10\\] the least squares S: \\[S(\\beta_1, \\beta_2) = [6 - (\\beta_1 + 1\\beta_2)]^2 + [5 - (\\beta_1 + 2\\beta_2)]^2 + [7 - (\\beta_1 + 3\\beta_2)]^2 + [10 - (\\beta_1 + 4\\beta_2)]^2 = 4\\beta_1^2 + 30\\beta_2^2 + 20\\beta_1\\beta_2 - 56\\beta_1 - 154\\beta_2 + 210\\] The minimum is: \\[\\frac{\\partial{S}}{\\partial{\\beta_1}} = 0 = 8 \\beta_1 + 60\\beta_2 - 154\\] \\[\\frac{\\partial{S}}{\\partial{\\beta_2}} = 0 = 20 \\beta_1 + 20\\beta_2 - 56\\] Result in a system of two equations in two unkowns gives: \\[\\beta_1 = 3.5\\] \\[\\beta_2 = 1.4\\] The line of best fit: \\(y = 3.5 + 1.4x\\) All possible regression lines goes through the intersection point \\((\\bar{x}, \\bar{y})\\) 26.4 Mean squared error (MSE) Standard error of train data \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2\\] Standard error of learn data \\[MSE = \\frac{1}{n_o}\\sum_{i=1}^{n_o}(y_i^o - \\hat{f}(x_i^o))^2\\] 26.5 Linear model in R # Height and weight vectors for 19 children height &lt;- c(69.1,56.4,65.3,62.8,63,57.3,59.8,62.5,62.5,59.0,51.3,64,56.4,66.5,72.2,65.0,67.0,57.6,66.6) weight &lt;- c(113,84,99,103,102,83,85,113,84,99,51,90,77,112,150,128,133,85,112) plot(height,weight) # Fit linear model model &lt;- lm(weight ~ height) # weight = slope*weight + intercept abline(model) # Regression line # correlation between variables cor(height,weight) ## [1] 0.8848454 # Get data from the model #get the intercept(b0) and the slope(b1) values model ## ## Call: ## lm(formula = weight ~ height) ## ## Coefficients: ## (Intercept) height ## -143.227 3.905 # detailed information about the model summary(model) ## ## Call: ## lm(formula = weight ~ height) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.816 -5.678 0.003 9.156 17.423 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -143.2266 31.1802 -4.594 0.000259 *** ## height 3.9047 0.4986 7.831 4.88e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.89 on 17 degrees of freedom ## Multiple R-squared: 0.783, Adjusted R-squared: 0.7702 ## F-statistic: 61.32 on 1 and 17 DF, p-value: 4.876e-07 # check all attributes calculated by lm attributes(model) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; &quot;fitted.values&quot; ## [6] &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; &quot;xlevels&quot; &quot;call&quot; ## [11] &quot;terms&quot; &quot;model&quot; ## ## $class ## [1] &quot;lm&quot; # getting only the intercept model$coefficients[1] #or model$coefficients[[1]] ## (Intercept) ## -143.2266 # getting only the slope model$coefficients[2] #or model$coefficients[[2]] ## height ## 3.904675 # checking the residuals residuals(model) ## 1 2 3 4 5 6 ## -13.586376027 7.002990499 -12.748612814 1.013073510 -0.767861396 2.488783423 ## 7 8 9 10 11 12 ## -5.272902901 12.184475869 -16.815524131 11.850836722 -6.083169400 -16.672535926 ## 13 14 15 16 17 18 ## 0.002990499 -4.434222250 11.309132932 17.422789545 14.613440486 3.317381064 ## 19 ## -4.824689703 # predict the weight for a given height, say 60 inches model$coefficients[[2]]*60 + model$coefficients[[1]] ## [1] 91.05384 # Mean squared error (MSE) predicted.weights &lt;- predict(model, newdata = as.data.frame(weight)) mse &lt;- mean(( weight - predicted.weights)^2, na.rm = TRUE) mse ## [1] 106.177 26.6 Linear regression model for multiple parameters 26.7 Choosing explanatory variables for the model In general, less MSE then better prediction. We can arrange variables by their importance to predict and choose the number of significant variables. run model with different number of variables. Arrange MSE for each variable. Compair MSE for different number of variables using t-test. If difference is significant =&gt; use more variables. In the same way we can compair different models. library(mosaicData) head(CPS85) ## wage educ race sex hispanic south married exper union age sector ## 1 9.0 10 W M NH NS Married 27 Not 43 const ## 2 5.5 12 W M NH NS Married 20 Not 38 sales ## 3 3.8 12 W F NH NS Single 4 Not 22 sales ## 4 10.5 12 W F NH NS Married 29 Not 47 clerical ## 5 15.0 12 W M NH NS Married 40 Union 58 const ## 6 9.0 16 W F NH NS Married 27 Not 49 clerical # relation wage ~ education # wage - response variable # educ, exper, age are explanatory variables # linear model model1 &lt;- lm(wage ~ educ, data = CPS85) model2 &lt;- lm(wage ~ educ + age, data = CPS85) model3 &lt;- lm(wage ~ educ + age + exper, data = CPS85) pred1 &lt;- predict(model1, newdata = CPS85) pred2 &lt;- predict(model2, newdata = CPS85) pred3 &lt;- predict(model3, newdata = CPS85) # Compair MSE mse1 &lt;- mean(( CPS85$wage - pred1)^2, na.rm = TRUE) mse2 &lt;- mean(( CPS85$wage - pred2)^2, na.rm = TRUE) mse3 &lt;- mean(( CPS85$wage - pred3)^2, na.rm = TRUE) mse &lt;- data.frame(model_1 = mse1, model_2 = mse2, model_3 = mse3) mse ## model_1 model_2 model_3 ## 1 22.51575 21.03578 21.0353 # Using both educ and age variables reduese MSE =&gt; improve model, where # adding exper does not improve model 26.8 Assessment of model performance for categorical data. Errors for categorical data can be calculated as number of errors prediction model makes. Test whether predicted values match actual values. Likelihood: extract the probability that the model assigned to the observed outcome. 26.9 Confidence intervals for linear model # 0. Build linear model data(&quot;cars&quot;, package = &quot;datasets&quot;) model &lt;- lm(dist ~ speed, data = cars) # 1. Add predictions pred.int &lt;- predict(model, interval = &quot;prediction&quot;) ## Warning in predict.lm(model, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses mydata &lt;- cbind(cars, pred.int) # 2. Regression line + confidence intervals library(&quot;ggplot2&quot;) p &lt;- ggplot(mydata, aes(speed, dist)) + geom_point() + stat_smooth(method = lm) # 3. Add prediction intervals p + geom_line(aes(y = lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ geom_line(aes(y = upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning in grid.Call.graphics(C_polygon, x$x, x$y, index): semi-transparency is not supported ## on this device: reported only once per page 26.10 Practical examples for linear model regression In this simple example we have 6 persons (3 males and 3 femails) and their score from 0 to 10. We want to build a model to see the dependence of score on gender: score ~ gender + \\(\\epsilon\\), where \\(\\epsilon\\) is an error # create data frame for the dataset df = data.frame(gender=c(rep(0,3), rep(1,3)), score=c(10,8,7, 1,3,2)) df ## gender score ## 1 0 10 ## 2 0 8 ## 3 0 7 ## 4 1 1 ## 5 1 3 ## 6 1 2 # build linear model x = lm(score ~ gender, df) summary(x) ## ## Call: ## lm(formula = score ~ gender, data = df) ## ## Residuals: ## 1 2 3 4 5 6 ## 1.667e+00 -3.333e-01 -1.333e+00 -1.000e+00 1.000e+00 1.110e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.3333 0.7454 11.180 0.000364 *** ## gender -6.3333 1.0541 -6.008 0.003863 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.291 on 4 degrees of freedom ## Multiple R-squared: 0.9002, Adjusted R-squared: 0.8753 ## F-statistic: 36.1 on 1 and 4 DF, p-value: 0.003863 "],["linear-regression-complex-cases.html", "Chapter 27 Linear regression complex cases 27.1 Cars 27.2 Linear regression modeling, compair with kNN 27.3 More complex example 27.4 NEXT part 27.5 NEXT Part", " Chapter 27 Linear regression complex cases 27.1 Cars df &lt;- mtcars # Subset of necessary data from mtcars df.sub &lt;- df[,c(1,3:7)] # Linear regression model fit &lt;- lm(mpg ~ hp, df) fit summary(fit) # Plot regression models using ggplot # parameter geom_smooth builds regression model library(ggplot2) # auto model ggplot(df, aes(hp, mpg))+geom_point(size=2)+geom_smooth() # for linear model: method = &#39;lm&#39; ggplot(df, aes(hp, mpg))+geom_point(size=2)+geom_smooth(method = &quot;lm&quot;) # Split data into two groups by am - Transmission (0 = automatic, 1 = manual) ggplot(df, aes(hp, mpg, col=factor(am)))+ geom_point(size=2)+ geom_smooth(method = &quot;lm&quot;) # Prediction of values using linear regression model fitted_values_mpg &lt;- data.frame(mpg=df$mpg, fitted=fit$fitted.values) fitted_values_mpg View(fitted_values_mpg) # Lets predict galons of petrol for given horse powers new_hp &lt;- data.frame(hp=c(100, 150, 129, 300)) predict(fit, new_hp) new_hp$mpg &lt;- predict(fit, new_hp) new_hp # Lets make regression model for cylinders as numeric (not factor) fit &lt;- lm(mpg ~ cyl, df) fit # Regression line for cyclinders ggplot(df, aes(cyl, mpg))+ geom_point()+ geom_smooth(method=&quot;lm&quot;)+ theme(axis.text=element_text(size=25), axis.title=element_text(size=25, face=&quot;bold&quot;)) 27.2 Linear regression modeling, compair with kNN library(&#39;GGally&#39;) library(&#39;lmtest&#39;) library(&#39;FNN&#39;) # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã my.seed &lt;- 12345 train.percent &lt;- 0.85 # –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö fileURL &lt;- &#39;https://sites.google.com/a/kiber-guu.ru/msep/mag-econ/salary_data.csv?attredirects=0&amp;d=1&#39; # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —Ñ–∞–∫—Ç–æ—Ä—ã wages.ru &lt;- read.csv(fileURL, row.names = 1, sep = &#39;;&#39;, as.is = T) wages.ru$male &lt;- as.factor(wages.ru$male) wages.ru$educ &lt;- as.factor(wages.ru$educ) wages.ru$forlang &lt;- as.factor(wages.ru$forlang) # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ set.seed(my.seed) inTrain &lt;- sample(seq_along(wages.ru$salary), nrow(wages.ru) * train.percent) df.train &lt;- wages.ru[inTrain, c(colnames(wages.ru)[-1], colnames(wages.ru)[1])] df.test &lt;- wages.ru[-inTrain, -1] # Variable description # salary ‚Äì —Å—Ä–µ–¥–Ω–µ–º–µ—Å—è—á–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ—Å–ª–µ –≤—ã—á–µ—Ç–∞ –Ω–∞–ª–æ–≥–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 12 –º–µ—Å—è—Ü–µ–≤ (—Ä—É–±–ª–µ–π); # male ‚Äì –ø–æ–ª: 1 ‚Äì –º—É–∂—á–∏–Ω–∞, 0 ‚Äì –∂–µ–Ω—â–∏–Ω–∞; # educ ‚Äì —É—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: # 1 ‚Äì 0-6 –∫–ª–∞—Å—Å–æ–≤, # 2 ‚Äì –Ω–µ–∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ (7-8 –∫–ª–∞—Å—Å–æ–≤), # 3 - –Ω–µ–∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–ª—é—Å —á—Ç–æ-—Ç–æ –µ—â—ë, # 4 ‚Äì –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ, # 5 ‚Äì –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ, 6 ‚Äì –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ –≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –≤—ã—à–µ; # forlang - –∏–Ω–æ—Å—Ç. —è–∑—ã–∫: 1 ‚Äì –≤–ª–∞–¥–µ–µ—Ç, 0 ‚Äì –Ω–µ—Ç; # exper ‚Äì –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞–∂ c 1.01.2002 (–ª–µ—Ç). summary(df.train) ggp &lt;- ggpairs(df.train) print(ggp, progress = F) # —Ü–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É male ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;male&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = male)) print(ggp, progress = F) # —Ü–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É educ ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;educ&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = educ)) print(ggp, progress = F) # —Ü–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É forlang ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;forlang&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = forlang)) print(ggp, progress = F) # Linear regression model model.1 &lt;- lm(salary ~ . + exper:educ + exper:forlang + exper:male, data = df.train) summary(model.1) ## Exclude uninfluencial parameters # Exclude eper:educ as paramaeters are not important model.2 &lt;- lm(salary ~ . + exper:forlang + exper:male, data = df.train) summary(model.2) # Exclude male1:exper model.3 &lt;- lm(salary ~ . + exper:forlang, data = df.train) summary(model.3) # forlang1 is less important, has no sence model.4 &lt;- lm(salary ~ male + educ + exper, data = df.train) summary(model.4) df.train$educ &lt;- as.numeric(df.train$educ) df.test$educ &lt;- as.numeric(df.test$educ) model.6 &lt;- lm(salary ~ ., data = df.train) summary(model.6) # Model 6 is week, let&#39;s add exper:male interactions df.train$educ &lt;- as.numeric(df.train$educ) model.7 &lt;- lm(salary ~ . + exper:male, data = df.train) summary(model.7) # Obviously the best decision is not to use interactions for modeling # Test remainers # —Ç–µ—Å—Ç –ë—Ä–æ–π—à–∞-–ü–∞–≥–∞–Ω–∞ bptest(model.6) # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –î–∞—Ä–±–∏–Ω–∞-–£–æ—Ç—Å–æ–Ω–∞ dwtest(model.6) # –≥—Ä–∞—Ñ–∏–∫–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤ par(mar = c(4.5, 4.5, 2, 1)) par(mfrow = c(1, 3)) plot(model.7, 1) plot(model.7, 4) plot(model.7, 5) ### Comparison with kNN-method par(mfrow = c(1, 1)) # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è y –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ y.fact &lt;- wages.ru[-inTrain, 1] y.model.lm &lt;- predict(model.6, df.test) MSE.lm &lt;- sum((y.model.lm - y.fact)^2) / length(y.model.lm) # kNN —Ç—Ä–µ–±—É–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ df.train.num &lt;- as.data.frame(apply(df.train, 2, as.numeric)) df.test.num &lt;- as.data.frame(apply(df.test, 2, as.numeric)) for (i in 2:50){ model.knn &lt;- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% &#39;salary&#39;)], y = df.train.num[, &#39;salary&#39;], test = df.test.num, k = i) y.model.knn &lt;- model.knn$pred if (i == 2){ MSE.knn &lt;- sum((y.model.knn - y.fact)^2) / length(y.model.knn) } else { MSE.knn &lt;- c(MSE.knn, sum((y.model.knn - y.fact)^2) / length(y.model.knn)) } } # –≥—Ä–∞—Ñ–∏–∫ par(mar = c(4.5, 4.5, 1, 1)) plot(2:50, MSE.knn, type = &#39;b&#39;, col = &#39;darkgreen&#39;, xlab = &#39;–∑–Ω–∞—á–µ–Ω–∏–µ k&#39;, ylab = &#39;MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ&#39;) lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2) legend(&#39;bottomright&#39;, lty = c(1, 2), pch = c(1, NA), col = c(&#39;darkgreen&#39;, grey(0.2)), legend = c(&#39;k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–∞&#39;, &#39;—Ä–µ–≥—Ä–µ—Å—Å–∏—è (–≤—Å–µ —Ñ–∞–∫—Ç–æ—Ä—ã)&#39;), lwd = rep(2, 2)) Source Course ‚ÄòMath modeling‚Äô practical work, State University of Management, Moscow 27.3 More complex example # Linear regression modeling, compair with kNN # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home library(&#39;GGally&#39;) library(&#39;lmtest&#39;) library(&#39;FNN&#39;) # variable my.seed &lt;- 12345 train.percent &lt;- 0.85 wages.ru &lt;- read.csv(&quot;~/Projects/data_analysis/DATA/salary.csv&quot;, row.names = 1, sep = &#39;\\t&#39;, as.is = T) # transform data into factors when possible wages.ru$male &lt;- as.factor(wages.ru$male) wages.ru$educ &lt;- as.factor(wages.ru$educ) wages.ru$forlang &lt;- as.factor(wages.ru$forlang) # test data set.seed(my.seed) inTrain &lt;- sample(seq_along(wages.ru$salary), nrow(wages.ru) * train.percent) df.train &lt;- wages.ru[inTrain, c(colnames(wages.ru)[-1], colnames(wages.ru)[1])] df.test &lt;- wages.ru[-inTrain, -1] summary(df.train) ggp &lt;- ggpairs(df.train) print(ggp, progress = F) # —Ü–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É male ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;male&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = male)) print(ggp, progress = F) # —Ü–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É educ ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;educ&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = educ)) print(ggp, progress = F) # —Ü–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É forlang ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;forlang&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = forlang)) print(ggp, progress = F) # Linear regression model model.1 &lt;- lm(salary ~ . + exper:educ + exper:forlang + exper:male, data = df.train) summary(model.1) ## Exclude uninfluencial parameters # Exclude eper:educ as paramaeters are not important model.2 &lt;- lm(salary ~ . + exper:forlang + exper:male, data = df.train) summary(model.2) # Exclude male1:exper model.3 &lt;- lm(salary ~ . + exper:forlang, data = df.train) summary(model.3) # forlang1 is less important, has no sence model.4 &lt;- lm(salary ~ male + educ + exper, data = df.train) summary(model.4) df.train$educ &lt;- as.numeric(df.train$educ) df.test$educ &lt;- as.numeric(df.test$educ) model.6 &lt;- lm(salary ~ ., data = df.train) summary(model.6) # Model 6 is week, let&#39;s add exper:male interactions df.train$educ &lt;- as.numeric(df.train$educ) model.7 &lt;- lm(salary ~ . + exper:male, data = df.train) summary(model.7) # Obviously the best decision is not to use interactions for modeling # Test remainers # —Ç–µ—Å—Ç –ë—Ä–æ–π—à–∞-–ü–∞–≥–∞–Ω–∞ bptest(model.6) # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –î–∞—Ä–±–∏–Ω–∞-–£–æ—Ç—Å–æ–Ω–∞ dwtest(model.6) # –≥—Ä–∞—Ñ–∏–∫–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤ par(mar = c(4.5, 4.5, 2, 1)) par(mfrow = c(1, 3)) plot(model.7, 1) plot(model.7, 4) plot(model.7, 5) ### Comparison with kNN-method par(mfrow = c(1, 1)) # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è y –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ y.fact &lt;- wages.ru[-inTrain, 1] y.model.lm &lt;- predict(model.6, df.test) MSE.lm &lt;- sum((y.model.lm - y.fact)^2) / length(y.model.lm) # kNN —Ç—Ä–µ–±—É–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ df.train.num &lt;- as.data.frame(apply(df.train, 2, as.numeric)) df.test.num &lt;- as.data.frame(apply(df.test, 2, as.numeric)) for (i in 2:50){ model.knn &lt;- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% &#39;salary&#39;)], y = df.train.num[, &#39;salary&#39;], test = df.test.num, k = i) y.model.knn &lt;- model.knn$pred if (i == 2){ MSE.knn &lt;- sum((y.model.knn - y.fact)^2) / length(y.model.knn) } else { MSE.knn &lt;- c(MSE.knn, sum((y.model.knn - y.fact)^2) / length(y.model.knn)) } } # –≥—Ä–∞—Ñ–∏–∫ par(mar = c(4.5, 4.5, 1, 1)) plot(2:50, MSE.knn, type = &#39;b&#39;, col = &#39;darkgreen&#39;, xlab = &#39;–∑–Ω–∞—á–µ–Ω–∏–µ k&#39;, ylab = &#39;MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ&#39;) lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2) legend(&#39;bottomright&#39;, lty = c(1, 2), pch = c(1, NA), col = c(&#39;darkgreen&#39;, grey(0.2)), legend = c(&#39;k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–∞&#39;, &#39;—Ä–µ–≥—Ä–µ—Å—Å–∏—è (–≤—Å–µ —Ñ–∞–∫—Ç–æ—Ä—ã)&#39;), lwd = rep(2, 2)) 27.4 NEXT part # Linear regression (part 1) # Linear Regression (part 2) x &lt;- read.table(&quot;~/DataAnalysis/R_data_analysis/DATA/Diamond.dat&quot;, header=T) # Check 3 models # 1. ax+b # 2. ax^2+b # 3. ax^2+bx+c res.1 &lt;- lm(x[,2]~x[,1]) summary(res.1) ves2 &lt;- x[,1]*x[,1] res.2 &lt;- lm(x[,2]~ves2) summary(res.2) res.3 &lt;- lm(x[,2]~x[,1]+ves2) summary(res.3) # weight of diamands ~ weight^2 plot(x[,1]~ves2) # Conclusion: from 3 models, the 2d is optimal ### –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã ser.g.01 &lt;- read.table(&quot;~/DataAnalysis/R_data_analysis/DATA/series_g.csv&quot;, header=T, sep=&quot;;&quot;) # visual inspection ser.g.01 dim(ser.g.01) names(ser.g.01) # plot vaiable plot(ser.g.01$series_g, type=&quot;l&quot;) log.ser.g &lt;- log(ser.g.01$series_g) plot(log.ser.g, type=&quot;l&quot;) time. &lt;- 1:(144+12) month.01 &lt;- rep(c(1,0,0,0,0,0,0,0,0,0,0,0), 12+1) month.02 &lt;- rep(c(0,1,0,0,0,0,0,0,0,0,0,0), 12+1) month.03 &lt;- rep(c(0,0,1,0,0,0,0,0,0,0,0,0), 12+1) month.04 &lt;- rep(c(0,0,0,1,0,0,0,0,0,0,0,0), 12+1) month.05 &lt;- rep(c(0,0,0,0,1,0,0,0,0,0,0,0), 12+1) month.06 &lt;- rep(c(0,0,0,0,0,1,0,0,0,0,0,0), 12+1) month.07 &lt;- rep(c(0,0,0,0,0,0,1,0,0,0,0,0), 12+1) month.08 &lt;- rep(c(0,0,0,0,0,0,0,1,0,0,0,0), 12+1) month.09 &lt;- rep(c(0,0,0,0,0,0,0,0,1,0,0,0), 12+1) month.10 &lt;- rep(c(0,0,0,0,0,0,0,0,0,1,0,0), 12+1) month.11 &lt;- rep(c(0,0,0,0,0,0,0,0,0,0,1,0), 12+1) month.12 &lt;- rep(c(0,0,0,0,0,0,0,0,0,0,0,1), 12+1) log.ser.g[145:(144+12)] &lt;-NA ser.g.02 &lt;- data.frame(log.ser.g, time., month.01, month.02, month.03, month.04, month.05, month.06, month.07, month.08, month.09, month.10, month.11, month.12) ser.g.02 res.01 &lt;- lm(log.ser.g ~ time. + month.02 + month.03 + month.04 + month.05 + month.06 + month.07 + month.08 + month.09 + month.10 + month.11 + month.12, ser.g.02) summary(res.01) res.01$fitted.values plot(ser.g.02$log.ser.g, type=&quot;l&quot;) lines(res.01$fitted.values) plot(ser.g.02$log.ser.g, type=&quot;l&quot;, col=&quot;green&quot;) lines(res.01$fitted.values, col=&quot;red&quot;) x.lg = predict.lm(res.01, ser.g.02) x.lg plot(x.lg, type=&quot;l&quot;, col=&quot;red&quot;) lines(ser.g.02$log.ser.g, col=&quot;green&quot;) y &lt;- exp(x.lg) plot(y, type=&quot;l&quot;, col=&quot;red&quot;) lines(ser.g.01$series_g, col=&quot;green&quot;) 27.5 NEXT Part # –ø—Ä–æ–≤–µ—Å—Ç–∏ –æ—Ç–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö; # –æ—Ç–æ–±—Ä–∞—Ç—å –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è; # –∫–∞–∫ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–∏–¥–∂- –∏ –ª–∞—Å—Å–æ-—Ä–µ–≥—Ä–µ—Å—Å–∏—é; # –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: PCR –∏ PLS; # –∫–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π. library(&#39;ISLR&#39;) # –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Hitters library(&#39;leaps&#39;) # —Ñ—É–Ω–∫—Ü–∏—è regsubset() -- –æ—Ç–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ # –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö library(&#39;glmnet&#39;) # —Ñ—É–Ω–∫—Ü–∏—è glmnet() -- –ª–∞—Å—Å–æ library(&#39;pls&#39;) # —Ä–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞ –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã -- pcr() # –∏ —á–∞—Å—Ç–Ω—ã–π –ú–ù–ö -- plsr() my.seed &lt;- 1 ?Hitters fix(Hitters) names(Hitters) dim(Hitters) sum(is.na(Hitters$Salary)) Hitters &lt;- na.omit(Hitters) dim(Hitters) sum(is.na(Hitters$Salary)) ## –û—Ç–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª–∏ —Å —Å–æ—á–µ—Ç–∞–Ω–∏—è–º–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ –¥–æ 8 –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ regfit.full &lt;- regsubsets(Salary ~ ., Hitters) summary(regfit.full) # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª–∏ —Å —Å–æ—á–µ—Ç–∞–Ω–∏—è–º–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ –¥–æ 19 (–º–∞–∫—Å–∏–º—É–º –≤ –¥–∞–Ω–Ω—ã—Ö) regfit.full &lt;- regsubsets(Salary ~ ., Hitters, nvmax = 19) reg.summary &lt;- summary(regfit.full) reg.summary # —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç—á—ë—Ç–∞ –ø–æ –º–æ–¥–µ–ª–∏ (–∏—â–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞) names(reg.summary) # R^2 –∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π R^2 round(reg.summary$rsq, 3) # –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ plot(1:19, reg.summary$rsq, type = &#39;b&#39;, xlab = &#39;–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤&#39;, ylab = &#39;R-–∫–≤–∞–¥—Ä–∞—Ç&#39;) # —Å–æ–¥–∞ –∂–µ –¥–æ–±–∞–≤–∏–º —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π R-–∫–≤–∞–¥—Ä–∞—Ç points(1:19, reg.summary$adjr2, col = &#39;red&#39;) # –º–æ–¥–µ–ª—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º R-–∫–≤–∞–¥—Ä–∞—Ç–æ–º which.max(reg.summary$adjr2) ### 11 points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = &#39;red&#39;, cex = 2, pch = 20) legend(&#39;bottomright&#39;, legend = c(&#39;R^2&#39;, &#39;R^2_adg&#39;), col = c(&#39;black&#39;, &#39;red&#39;), lty = c(1, NA), pch = c(1, 1)) # C_p reg.summary$cp # —á–∏—Å–ª–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ —É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è which.min(reg.summary$cp) ### 10 # –≥—Ä–∞—Ñ–∏–∫ plot(reg.summary$cp, xlab = &#39;–ß–∏—Å–ª–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤&#39;, ylab = &#39;C_p&#39;, type = &#39;b&#39;) points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = &#39;red&#39;, cex = 2, pch = 20) # BIC reg.summary$bic # —á–∏—Å–ª–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ —É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è which.min(reg.summary$bic) ### 6 # –≥—Ä–∞—Ñ–∏–∫ plot(reg.summary$bic, xlab = &#39;–ß–∏—Å–ª–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤&#39;, ylab = &#39;BIC&#39;, type = &#39;b&#39;) points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = &#39;red&#39;, cex = 2, pch = 20) # –º–µ—Ç–æ–¥ plot –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ?plot.regsubsets plot(regfit.full, scale = &#39;r2&#39;) plot(regfit.full, scale = &#39;adjr2&#39;) plot(regfit.full, scale = &#39;Cp&#39;) plot(regfit.full, scale = &#39;bic&#39;) # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º BIC round(coef(regfit.full, 6), 3) ## –û—Ç–±–æ—Ä –ø—É—Ç—ë–º –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö # –ü–æ—à–∞–≥–æ–≤–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ regfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = &#39;forward&#39;) summary(regfit.fwd) regfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = &#39;backward&#39;) summary(regfit.bwd) round(coef(regfit.full, 7), 3) round(coef(regfit.fwd, 7), 3) round(coef(regfit.bwd, 7), 3) ### –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ set.seed(my.seed) train &lt;- sample(c(T, F), nrow(Hitters), rep = T) test &lt;- !train # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ regfit.best &lt;- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19) # –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä—è—Å–Ω—è—é—â–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ test.mat &lt;- model.matrix(Salary ~ ., data = Hitters[test, ]) # –≤–µ–∫—Ç–æ—Ä –æ—à–∏–±–æ–∫ val.errors &lt;- rep(NA, 19) # —Ü–∏–∫–ª –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ for (i in 1:19){ coefi &lt;- coef(regfit.best, id = i) pred &lt;- test.mat[, names(coefi)] %*% coefi # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤ –≤–µ–∫—Ç–æ—Ä val.errors[i] &lt;- mean((Hitters$Salary[test] - pred)^2) } round(val.errors, 0) # –Ω–∞—Ö–æ–¥–∏–º —á–∏—Å–ª–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ —É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ which.min(val.errors) ### 10 # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ round(coef(regfit.best, 10), 3) # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ regsubset() predict.regsubsets &lt;- function(object, newdata, id, ...){ form &lt;- as.formula(object$call[[2]]) mat &lt;- model.matrix(form, newdata) coefi &lt;- coef(object, id = id) xvars &lt;- names(coefi) mat[, xvars] %*% coefi } # –Ω–∞–±–æ—Ä —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö regfit.best &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19) round(coef(regfit.best, 10), 3) # k-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è # –æ—Ç–±–∏—Ä–∞–µ–º 10 –±–ª–æ–∫–æ–≤ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π k &lt;- 10 set.seed(my.seed) folds &lt;- sample(1:k, nrow(Hitters), replace = T) # –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–¥ –º–∞—Ç—Ä–∏—Ü—É —Å –æ—à–∏–±–∫–∞–º–∏ cv.errors &lt;- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19))) # –∑–∞–ø–æ–ª–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤ —Ü–∏–∫–ª–µ –ø–æ –±–ª–æ–∫–∞–º –¥–∞–Ω–Ω—ã—Ö for (j in 1:k){ best.fit &lt;- regsubsets(Salary ~ ., data = Hitters[folds != j, ], nvmax = 19) # —Ç–µ–ø–µ—Ä—å —Ü–∏–∫–ª –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä—è—Å–Ω—è—é—â–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö for (i in 1:19){ # –º–æ–¥–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è Salary pred &lt;- predict(best.fit, Hitters[folds == j, ], id = i) # –≤–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤ –º–∞—Ç—Ä–∏—Ü—É cv.errors[j, i] &lt;- mean((Hitters$Salary[folds == j] - pred)^2) } } # —É—Å—Ä–µ–¥–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–æ –∫–∞–∂–¥–æ–º—É —Å—Ç–æ–ª–±—Ü—É (—Ç.–µ. –ø–æ –±–ª–æ–∫–∞–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–π), # —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ—Ü–µ–Ω–∫—É MSE –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—ä—è—Å–Ω—è—é—â–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö mean.cv.errors &lt;- apply(cv.errors, 2, mean) round(mean.cv.errors, 0) # –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ plot(mean.cv.errors, type = &#39;b&#39;) points(which.min(mean.cv.errors), mean.cv.errors[which.min(mean.cv.errors)], col = &#39;red&#39;, pch = 20, cex = 2) # –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —Å 11 –æ–±—ä—è—Å–Ω—è—é—â–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –Ω–∞ –≤—Å—ë–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö reg.best &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19) round(coef(reg.best, 11), 3) # –∏–∑-–∑–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ glmnet() —Ñ–æ—Ä–º–∏—Ä—É–µ–º —è–≤–Ω–æ –º–∞—Ç—Ä–∏—Ü—É –æ–±—ä—è—Å–Ω—è—é—â–∏—Ö... x &lt;- model.matrix(Salary ~ ., Hitters)[, -1] # –∏ –≤–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π y &lt;- Hitters$Salary ### –ì—Ä–µ–±–Ω–µ–≤–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è # –≤–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –ª—è–º–±–¥–∞ grid &lt;- 10^seq(10, -2, length = 100) # –ø–æ–¥–≥–æ–Ω—è–µ–º —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π —Ä–∏–¥–∂-—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ ridge.mod &lt;- glmnet(x, y, alpha = 0, lambda = grid) # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π dim(coef(ridge.mod)) ## [1] 20 100 # –∑–Ω–∞—á–µ–Ω–∏–µ –ª—è–º–±–¥–∞ –ø–æ–¥ –Ω–æ–º–µ—Ä–æ–º 50 round(ridge.mod$lambda[50], 0) ## [1] 11498 # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ round(coef(ridge.mod)[, 50], 3) # –Ω–æ—Ä–º–∞ —ç–ª—å-–¥–≤–∞ round(sqrt(sum(coef(ridge.mod)[-1, 50]^2)), 2) # –≤—Å—ë —Ç–æ –∂–µ –¥–ª—è –ª—è–º–±–¥—ã –ø–æ–¥ –Ω–æ–º–µ—Ä–æ–º 60 # –∑–Ω–∞—á–µ–Ω–∏–µ –ª—è–º–±–¥–∞ –ø–æ–¥ –Ω–æ–º–µ—Ä–æ–º 50 round(ridge.mod$lambda[60], 0) # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ round(coef(ridge.mod)[, 60], 3) # –Ω–æ—Ä–º–∞ —ç–ª—å-–¥–≤–∞ round(sqrt(sum(coef(ridge.mod)[-1, 60]^2)), 1) # –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–æ–≤–æ–π –ª—è–º–±–¥—ã round(predict(ridge.mod, s = 50, type = &#39;coefficients&#39;)[1:20, ], 3) ## –ú–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ set.seed(my.seed) train &lt;- sample(1:nrow(x), nrow(x)/2) test &lt;- -train y.test &lt;- y[test] # –ø–æ–¥–≥–æ–Ω—è–µ–º —Ä–∏–¥–∂-–º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é (thresh –Ω–∏–∂–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) ridge.mod &lt;- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12) plot(ridge.mod) # –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –ª—è–º–±–¥–∞ = 4 ridge.pred &lt;- predict(ridge.mod, s = 4, newx = x[test, ]) round(mean((ridge.pred - y.test)^2), 0) # —Å—Ä–∞–≤–Ω–∏–º —Å MSE –¥–ª—è –Ω—É–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–≥–Ω–æ–∑ = —Å—Ä–µ–¥–Ω–µ–µ) round(mean((mean(y[train]) - y.test)^2), 0) # –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å —Å –ª—è–º–±–¥–∞ = 4 –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω–æ–π –ü–õ–† ridge.pred &lt;- predict(ridge.mod, s = 0, newx = x[test, ], exact = T, x = x[train, ], y = y[train]) round(mean((ridge.pred - y.test)^2), 0) # predict —Å –ª—è–º–±–¥–æ–π (s) = 0 –¥–∞—ë—Ç –º–æ–¥–µ–ª—å –ü–õ–† lm(y ~ x, subset = train) round(predict(ridge.mod, s = 0, exact = T, type = &#39;coefficients&#39;, x = x[train, ], y = y[train])[1:20, ], 3) ## –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ª—è–º–±–¥–∞ —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ # k-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è set.seed(my.seed) # –æ—Ü–µ–Ω–∫–∞ –æ—à–∏–±–∫–∏ cv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 0) plot(cv.out) # –∑–Ω–∞—á–µ–Ω–∏–µ –ª—è–º–±–¥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–µ–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –æ—à–∏–±–∫—É –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ bestlam &lt;- cv.out$lambda.min round(bestlam, 0) ## [1] 212 # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –¥–ª—è —ç—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ª—è–º–±–¥—ã ridge.pred &lt;- predict(ridge.mod, s = bestlam, newx = x[test, ]) round(mean((ridge.pred - y.test)^2), 0) ## [1] 96016 # –Ω–∞–∫–æ–Ω–µ—Ü, –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ª—è–º–±–¥—ã, # –Ω–∞–π–¥–µ–Ω–Ω–æ–π –ø–æ –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ out &lt;- glmnet(x, y, alpha = 0) round(predict(out, type = &#39;coefficients&#39;, s = bestlam)[1:20, ], 3) ## –õ–∞—Å—Å–æ lasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = grid) plot(lasso.mod) # –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ª—è–º–±–¥–∞ —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ set.seed(my.seed) cv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1) plot(cv.out) bestlam &lt;- cv.out$lambda.min lasso.pred &lt;- predict(lasso.mod, s = bestlam, newx = x[test, ]) round(mean((lasso.pred - y.test)^2), 0) # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ out &lt;- glmnet(x, y, alpha = 1, lambda = grid) lasso.coef &lt;- predict(out, type = &#39;coefficients&#39;, s = bestlam)[1:20, ] round(lasso.coef, 3) round(lasso.coef[lasso.coef != 0], 3) ### –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3: —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ –º–µ—Ç–æ–¥–æ–≤ PCR –∏ PLS ### 6.7.1 –†–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞ –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã # –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è set.seed(2) # –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ –ø–æ—á–µ–º—É –æ–Ω–∏ —Å–º–µ–Ω–∏–ª–∏ –∑–µ—Ä–Ω–æ; –ø–æ—Ö–æ–∂–µ, –æ–ø–µ—á–∞—Ç–∫–∞ pcr.fit &lt;- pcr(Salary ~ ., data = Hitters, scale = T, validation = &#39;CV&#39;) summary(pcr.fit) # –≥—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–æ–∫ validationplot(pcr.fit, val.type = &#39;MSEP&#39;) # –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–∞–ª—å–Ω–æ–≥–æ M: –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ set.seed(my.seed) pcr.fit &lt;- pcr(Salary ~ ., data = Hitters, subset = train, scale = T, validation = &#39;CV&#39;) validationplot(pcr.fit, val.type = &#39;MSEP&#39;) # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ pcr.pred &lt;- predict(pcr.fit, x[test, ], ncomp = 7) round(mean((pcr.pred - y.test)^2), 0) # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è M = 7 # (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –ø–æ –º–µ—Ç–æ–¥—É –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏) pcr.fit &lt;- pcr(y ~ x, scale = T, ncomp = 7) summary(pcr.fit) # –†–µ–≥—Ä–µ—Å—Å–∏—è –ø–æ –º–µ—Ç–æ–¥—É —á–∞—Å—Ç–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ set.seed(my.seed) pls.fit &lt;- plsr(Salary ~ ., data = Hitters, subset = train, scale = T, validation = &#39;CV&#39;) summary(pls.fit) # —Ç–µ–ø–µ—Ä—å –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ M = 2 # –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ–º MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π pls.pred &lt;- predict(pls.fit, x[test, ], ncomp = 2) round(mean((pls.pred - y.test)^2), 0) ## [1] 101417 # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ pls.fit &lt;- plsr(Salary ~ ., data = Hitters, scale = T, ncomp = 2) summary(pls.fit) "],["nonlinear-regression.html", "Chapter 28 Nonlinear regression", " Chapter 28 Nonlinear regression Nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. Some nonlinear data sets can be transformed to a linear model. Sone can not be transformed. For such modeling methods of Numerical analysis should be applied such as Newton‚Äôs method, Gauss-Newton method and Levenberg‚ÄìMarquardt method. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ü—Ä–∞–∫—Ç–∏–∫–∞ 7 –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –í –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –Ω–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ –∫–∞–∫: –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é; –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏; —Å—Ç—Ä–æ–∏—Ç—å —Å–ø–ª–∞–π–Ω—ã; —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π; —Å—Ç—Ä–æ–∏—Ç—å –æ–±–æ–±—â—ë–Ω–Ω—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ (GAM). –ú–æ–¥–µ–ª–∏: –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, —Å—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è –º–æ–¥–µ–ª—å, –æ–±–æ–±—â—ë–Ω–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å. –î–∞–Ω–Ω—ã–µ: Wage {ISLR} –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∫–æ–¥—É –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö —Å–º. –≤ [1], –≥–ª–∞–≤–∞ 7. library(&#39;ISLR&#39;) # –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Auto library(&#39;splines&#39;) # —Å–ø–ª–∞–π–Ω—ã library(&#39;gam&#39;) # –æ–±–æ–±—â—ë–Ω–Ω—ã–µ –∞–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ ## Warning: package &#39;gam&#39; was built under R version 3.3.3 ## Loading required package: foreach ## Warning: package &#39;foreach&#39; was built under R version 3.3.3 ## Loaded gam 1.14 library(&#39;akima&#39;) # –≥—Ä–∞—Ñ–∏–∫ –¥–≤—É–º–µ—Ä–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏ ## Warning: package &#39;akima&#39; was built under R version 3.3.3 library(&#39;ggplot2&#39;) # –∫—Ä–∞—Å–∏–≤—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ ## Warning: package &#39;ggplot2&#39; was built under R version 3.3.3 my.seed &lt;- 1 –†–∞–±–æ—Ç–∞–µ–º —Å –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º 3000 —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤-–º—É–∂—á–∏–Ω —Å—Ä–µ–¥–Ω–µ–∞—Ç–ª–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ Wage. –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –µ–≥–æ –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É –∏–º—ë–Ω —Ñ—É–Ω–∫—Ü–∏–µ–π attach(), –∏ –¥–∞–ª—å—à–µ –æ–±—Ä–∞—â–∞–µ–º—Å—è –Ω–∞–ø—Ä—è–º—É—é –∫ —Å—Ç–æ–ª–±—Ü–∞–º —Ç–∞–±–ª–∏—Ü—ã. attach(Wage) –†–∞–±–æ—Ç–∞–µ–º —Å–æ —Å—Ç–æ–ª–±—Ü–∞–º–∏: * wage ‚Äì –∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞ –¥–æ —É–ø–ª–∞—Ç—ã –Ω–∞–ª–æ–≥–æ–≤; * age ‚Äì –≤–æ–∑—Ä–∞—Å—Ç —Ä–∞–±–æ—Ç–Ω–∏–∫–∞ –≤ –≥–æ–¥–∞—Ö. –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∑–∞—Ä–ø–ª–∞—Ç—ã –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞ –°—É–¥—è –ø–æ –≥—Ä–∞—Ñ–∏–∫—É –Ω–∏–∂–µ, –∑–∑–∞–∏–º–æ—Å–≤—è–∑—å –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –∏ –≤–æ–∑—Ä–∞—Å—Ç–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–∞. –ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Ç–∞–∫–∂–µ –≥—Ä—É–ø–ø–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π —Å –≤—ã—Å–æ–∫–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º wage, –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ 250. gp &lt;- ggplot(data = Wage, aes(x = age, y = wage)) gp &lt;- gp + geom_point() + geom_abline(slope = 0, intercept = 250, col = &#39;red&#39;) gp –ü–æ–¥–≥–æ–Ω—è–µ–º –ø–æ–ª–∏–Ω–æ–º —á–µ—Ç–≤—ë—Ä—Ç–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –¥–ª—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞. fit &lt;- lm(wage ~ poly(age, 4), data = Wage) round(coef(summary(fit)), 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.70 0.73 153.28 0.00 ## poly(age, 4)1 447.07 39.91 11.20 0.00 ## poly(age, 4)2 -478.32 39.91 -11.98 0.00 ## poly(age, 4)3 125.52 39.91 3.14 0.00 ## poly(age, 4)4 -77.91 39.91 -1.95 0.05 –§—É–Ω–∫—Ü–∏—è poly(age, 4) —Å–æ–∑–¥–∞—ë—Ç —Ç–∞–±–ª–∏—Ü—É —Å –±–∞–∑–∏—Å–æ–º –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤: –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π age –≤ —Å—Ç–µ–ø–µ–Ω—è—Ö –æ—Ç 1 –¥–æ 4. round(head(poly(age, 4)), 3) ## 1 2 3 4 ## [1,] -0.039 0.056 -0.072 0.087 ## [2,] -0.029 0.026 -0.015 -0.003 ## [3,] 0.004 -0.015 0.000 0.014 ## [4,] 0.001 -0.015 0.005 0.013 ## [5,] 0.012 -0.010 -0.011 0.010 ## [6,] 0.018 -0.002 -0.017 -0.001 # –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Å–∞–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è age –≤ –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç–µ–ø–µ–Ω—è—Ö round(head(poly(age, 4, raw = T)), 3) ## 1 2 3 4 ## [1,] 18 324 5832 104976 ## [2,] 24 576 13824 331776 ## [3,] 45 2025 91125 4100625 ## [4,] 43 1849 79507 3418801 ## [5,] 50 2500 125000 6250000 ## [6,] 54 2916 157464 8503056 # –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç, –Ω–æ –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑–º–µ–Ω—è—é—Ç—Å—è fit.2 &lt;- lm(wage ~ poly(age, 4, raw = T), data = Wage) round(coef(summary(fit.2)), 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -184.15 60.04 -3.07 0.00 ## poly(age, 4, raw = T)1 21.25 5.89 3.61 0.00 ## poly(age, 4, raw = T)2 -0.56 0.21 -2.74 0.01 ## poly(age, 4, raw = T)3 0.01 0.00 2.22 0.03 ## poly(age, 4, raw = T)4 0.00 0.00 -1.95 0.05 # –≥—Ä–∞–Ω–∏—Ü—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π age agelims &lt;- range(age) # –∑–Ω–∞—á–µ–Ω–∏—è age, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –¥–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ (–æ—Ç min –¥–æ max —Å —à–∞–≥–æ–º 1) age.grid &lt;- seq(from = agelims[1], to = agelims[2]) # —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏ preds &lt;- predict(fit, newdata = list(age = age.grid), se = T) # –≥—Ä–∞–Ω–∏—Ü—ã –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–ª—è –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã se.bands &lt;- cbind(lower.bound = preds$fit - 2*preds$se.fit, upper.bound = preds$fit + 2*preds$se.fit) # —Å–º–æ—Ç—Ä–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç round(head(se.bands), 2) ## lower.bound upper.bound ## 1 41.33 62.53 ## 2 49.76 67.24 ## 3 57.39 71.76 ## 4 64.27 76.09 ## 5 70.44 80.27 ## 6 75.94 84.28 –†–∏—Å—É–µ–º –ª–µ–≤—É—é –ø–∞–Ω–µ–ª—å –≥—Ä–∞—Ñ–∏–∫–∞ —Å–æ —Å–ª–∞–π–¥–∞ 4 –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ (—Ä–∏—Å. 7.1 –∫–Ω–∏–≥–∏). –§—É–Ω–∫—Ü–∏—è matlines() —Ä–∏—Å—É–µ—Ç –≥—Ä—Ñ–∏–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ –æ–¥–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–æ—Ç–∏–≤ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥—Ä—É–≥–æ–π. # –Ω–∞–±–ª—é–¥–µ–Ω–∏—è plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) # –∑–∞–≥–æ–ª–æ–≤–æ–∫ title(&#39;–ü–æ–ª–∏–Ω–æ–º —á–µ—Ç–≤—ë—Ä—Ç–æ–π —Å—Ç–µ–ø–µ–Ω–∏&#39;) # –º–æ–¥–µ–ª—å lines(age.grid, preds$fit, lwd = 2, col = &#39;blue&#39;) # –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –ø—Ä–æ–≥–Ω–æ–∑–∞ matlines(x = age.grid, y = se.bands, lwd = 1, col = &#39;blue&#39;, lty = 3) –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ –º–æ–¥–µ–ª—è–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏ poly() —Å–æ–≤–ø–∞–¥–∞—é—Ç. # –ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ –≤—Ç–æ—Ä–æ–º—É –≤—ã–∑–æ–≤—É –º–æ–¥–µ–ª–∏ preds2 &lt;- predict(fit.2, newdata = list(age = age.grid), se = T) # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏ –ø–æ –¥–≤—É–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–∏ max(abs(preds$fit - preds2$fit)) ## [1] 7.389644e-13 –¢–µ–ø–µ—Ä—å –ø–æ–¥–±–∏—Ä–∞–µ–º —Å—Ç–µ–ø–µ–Ω—å –ø–æ–ª–∏–Ω–æ–º–∞, —Å—Ä–∞–≤–Ω–∏–≤–∞—è –º–æ–¥–µ–ª–∏ —Å–æ —Å—Ç–µ–ø–µ–Ω—è–º–∏ –æ—Ç 1 –¥–æ 5 —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (ANOVA). fit.1 &lt;- lm(wage ~ age, data = Wage) fit.2 &lt;- lm(wage ~ poly(age, 2), data = Wage) fit.3 &lt;- lm(wage ~ poly(age, 3), data = Wage) fit.4 &lt;- lm(wage ~ poly(age, 4), data = Wage) fit.5 &lt;- lm(wage ~ poly(age, 5), data = Wage) round(anova(fit.1, fit.2, fit.3, fit.4, fit.5), 2) Res.Df &lt;dbl&gt; RSS &lt;dbl&gt; Df &lt;dbl&gt; Sum of Sq &lt;dbl&gt; F &lt;dbl&gt; Pr(&gt;F) &lt;dbl&gt; 2998 5022216 NA NA NA NA 2997 4793430 1 228786.01 143.59 0.00 2996 4777674 1 15755.69 9.89 0.00 2995 4771604 1 6070.15 3.81 0.05 2994 4770322 1 1282.56 0.80 0.37 5 rows –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—è—Ç—å –º–æ–¥–µ–ª–µ–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–µ–ø–µ–Ω–∏ –ø–æ–ª–∏–Ω–æ–º–∞ –æ—Ç age –∏–¥—É—Ç –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é. –í –∫—Ä–∞–π–Ω–µ–º –ø—Ä–∞–≤–æ–º —Å—Ç–æ–ª–±—Ü–µ —Ç–∞–±–ª–∏—Ü–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è p-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω—É–ª–µ–≤–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã: —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å –Ω–µ –¥–∞—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è RSS –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª—å—é. –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥, —á—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏ 3 –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–∞–ª—å–Ω–µ–π—à–µ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–∏ –Ω–µ –¥–∞—ë—Ç –∑–Ω–∞—á–∏–º–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏. –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—Ç—å –∑–∞—Ä–ø–ª–∞—Ç—É &gt; 250 –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞ –¢–µ–ø–µ—Ä—å –≤–µ—Ä–Ω—ë–º—Å—è –∫ –≥—Ä—É–ø–ø–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π —Å –≤—ã—Å–æ–∫–∏–º wage. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ, —á—Ç–æ –≤–µ–ª–∏—á–∏–Ω–∞ –∑–∞—Ä–ø–ª–∞—Ç—ã –±–æ–ª—å—à–µ 250, –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞. –ü–æ–¥–≥–æ–Ω—è–µ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ –¥–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã, –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±–æ–±—â—ë–Ω–Ω–æ–π –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ glm() –∏ —É–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ binomial: fit &lt;- glm(I(wage &gt; 250) ~ poly(age, 4), data = Wage, family = &#39;binomial&#39;) # –ø—Ä–æ–≥–Ω–æ–∑—ã preds &lt;- predict(fit, newdata = list(age = age.grid), se = T) # –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ –∏—Å—Ö–æ–¥–Ω—ã–µ –ï–ò pfit &lt;- exp(preds$fit) / (1 + exp(preds$fit)) se.bands.logit &lt;- cbind(lower.bound = preds$fit - 2*preds$se.fit, upper.bound = preds$fit + 2*preds$se.fit) se.bands &lt;- exp(se.bands.logit)/(1 + exp(se.bands.logit)) # —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–æ–±—ã—Ç–∏—è # &quot;–ó–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞ –≤—ã—à–µ 250&quot;. round(head(se.bands), 3) ## lower.bound upper.bound ## 1 0 0.002 ## 2 0 0.003 ## 3 0 0.004 ## 4 0 0.005 ## 5 0 0.006 ## 6 0 0.007 –î–æ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ —Å 4 —Å–ª–∞–π–¥–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ (—Ä–∏—Å. 7.1 –∫–Ω–∏–≥–∏). –†–∏—Å—É–µ–º –ø—Ä–∞–≤—É—é –ø–∞–Ω–µ–ª—å. # —Å–µ—Ç–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (–∏–∑–æ–±—Ä–∞–∂–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –ø–æ—ç—Ç–æ–º—É –∏–Ω—Ç–µ—Ä–≤–∞–ª –∏–∑–º–µ–Ω–µ–Ω–∏—è y –º–∞–ª) plot(age, I(wage &gt; 250), xlim = agelims, type = &#39;n&#39;, ylim = c(0, 0.2), ylab = &#39;P(Wage &gt; 250 | Age)&#39;) # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞—Å–µ—á–∫–∞–º–∏ points(jitter(age), I((wage &gt; 250) / 5), cex = 0.5, pch = &#39;|&#39;, col = &#39;darkgrey&#39;) # –º–æ–¥–µ–ª—å lines(age.grid, pfit, lwd = 2, col = &#39;blue&#39;) # –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã matlines(age.grid, se.bands, lwd = 1, col = &#39;blue&#39;, lty = 3) # –∑–∞–≥–æ–ª–æ–≤–æ–∫ title(&#39;–ü–æ–ª–∏–Ω–æ–º —á–µ—Ç–≤—ë—Ä—Ç–æ–π —Å—Ç–µ–ø–µ–Ω–∏&#39;) –°—Ç—É–ø–µ–Ω—á–∞—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –î–ª—è –Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤, –Ω–∞ –∫–∞–∂–¥–æ–º –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥–µ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å wage –æ—Ç age —Å–≤–æ–∏–º —Å—Ä–µ–¥–Ω–∏–º —É—Ä–æ–≤–Ω–µ–º. # –Ω–∞—Ä–µ–∑–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä age –Ω–∞ 4 —Ä–∞–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ table(cut(age, 4)) ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 # –ø–æ–¥–≥–æ–Ω—è–µ–º –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö fit &lt;- lm(wage ~ cut(age, 4), data = Wage) round(coef(summary(fit)), 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 94.16 1.48 63.79 0.00 ## cut(age, 4)(33.5,49] 24.05 1.83 13.15 0.00 ## cut(age, 4)(49,64.5] 23.66 2.07 11.44 0.00 ## cut(age, 4)(64.5,80.1] 7.64 4.99 1.53 0.13 # –ø—Ä–æ–≥–Ω–æ–∑ -- —ç—Ç–æ —Å—Ä–µ–¥–Ω–∏–µ –ø–æ `wage` –Ω–∞ –∫–∞–∂–¥–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ preds.cut &lt;- predict(fit, newdata = list(age = age.grid), se = T) # –∏–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ se.bands.cut &lt;- cbind(lower.bound = preds.cut$fit - 2*preds.cut$se.fit, upper.bound = preds.cut$fit + 2*preds.cut$se.fit) –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥—ë–º –≥—Ä–∞—Ñ–∏–∫ —Å–æ —Å–ª–∞–π–¥–∞ 7 –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ (—Ä–∏—Å. 7.2 –∫–Ω–∏–≥–∏). # –Ω–∞–±–ª—é–¥–µ–Ω–∏—è plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) # –º–æ–¥–µ–ª—å lines(age.grid, preds.cut$fit, lwd = 2, col = &#39;darkgreen&#39;) # –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –ø—Ä–æ–≥–Ω–æ–∑–∞ matlines(x = age.grid, y = se.bands.cut, lwd = 1, col = &#39;darkgreen&#39;, lty = 3) # –∑–∞–≥–æ–ª–æ–≤–æ–∫ title(&#39;–°—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è&#39;) –ü—Ä–∞–≤–∞—è —á–∞—Å—Ç—å –≥—Ä–∞—Ñ–∏–∫–∞, –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ, —á—Ç–æ –∑–∞—Ä–ø–ª–∞—Ç–∞ –≤—ã—à–µ 250. fit &lt;- glm(I(wage &gt; 250) ~ cut(age, 4), data = Wage, family = &#39;binomial&#39;) # –ø—Ä–æ–≥–Ω–æ–∑—ã preds &lt;- predict(fit, newdata = list(age = age.grid), se = T) # –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ –∏—Å—Ö–æ–¥–Ω—ã–µ –ï–ò pfit &lt;- exp(preds$fit) / (1 + exp(preds$fit)) se.bands.logit &lt;- cbind(lower.bound = preds$fit - 2*preds$se.fit, upper.bound = preds$fit + 2*preds$se.fit) se.bands &lt;- exp(se.bands.logit)/(1 + exp(se.bands.logit)) # —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–æ–±—ã—Ç–∏—è # &quot;–ó–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞ –≤—ã—à–µ 250&quot;. round(head(se.bands), 3) ## lower.bound upper.bound ## 1 0.003 0.016 ## 2 0.003 0.016 ## 3 0.003 0.016 ## 4 0.003 0.016 ## 5 0.003 0.016 ## 6 0.003 0.016 # —Å–µ—Ç–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (–∏–∑–æ–±—Ä–∞–∂–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –ø–æ—ç—Ç–æ–º—É –∏–Ω—Ç–µ—Ä–≤–∞–ª –∏–∑–º–µ–Ω–µ–Ω–∏—è y –º–∞–ª) plot(age, I(wage &gt; 250), xlim = agelims, type = &#39;n&#39;, ylim = c(0, 0.2), ylab = &#39;P(Wage &gt; 250 | Age)&#39;) # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞—Å–µ—á–∫–∞–º–∏ points(jitter(age), I((wage &gt; 250) / 5), cex = 0.5, pch = &#39;|&#39;, col = &#39;darkgrey&#39;) # –º–æ–¥–µ–ª—å lines(age.grid, pfit, lwd = 2, col = &#39;darkgreen&#39;) # –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã matlines(age.grid, se.bands, lwd = 1, col = &#39;darkgreen&#39;, lty = 3) # –∑–∞–≥–æ–ª–æ–≤–æ–∫ title(&#39;–°—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è&#39;) –°–ø–ª–∞–π–Ω—ã –ü–æ—Å—Ç—Ä–æ–∏–º –∫—É–±–∏—á–µ—Å–∫–∏–π —Å–ø–ª–∞–π–Ω —Å —Ç—Ä–µ–º—è —É–∑–ª–∞–º–∏. # –∫—É–±–∏—á–µ—Å–∫–∏–π —Å–ø–ª–∞–π–Ω —Å —Ç—Ä–µ–º—è —É–∑–ª–∞–º–∏ fit &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage) # –ø—Ä–æ–≥–Ω–æ–∑ preds.spl &lt;- predict(fit, newdata = list(age = age.grid), se = T) –¢–µ–ø–µ—Ä—å –ø–æ—Å—Ç—Ä–æ–∏–º –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–π –ø–æ —Ç—Ä—ë–º —É–∑–ª–∞–º. –¢—Ä–∏ —É–∑–ª–∞ —ç—Ç–æ 6 —Å—Ç–µ–ø–µ–Ω–µ–π —Å–≤–æ–±–æ–¥—ã. –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏ bs(), –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –º–∞—Ç—Ä–∏—Ü—É —Å –±–∞–∑–∏—Å–æ–º –¥–ª—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–≥–æ —Å–ø–ª–∞–π–Ω–∞, –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ç–µ–ø–µ–Ω–∏ —Å–≤–æ–±–æ–¥—ã, –æ–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç —É–∑–ª—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ. –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –∫–≤–∞—Ä—Ç–∏–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è age. # 3 —É–∑–ª–∞ -- 6 —Å—Ç–µ–ø–µ–Ω–µ–π —Å–≤–æ–±–æ–¥—ã (—Å—Ç–æ–ª–±—Ü—ã –º–∞—Ç—Ä–∏—Ü—ã) dim(bs(age, knots = c(25, 40, 60))) ## [1] 3000 6 # –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ–º —É–∑–ª—ã —è–≤–Ω–æ... dim(bs(age, df = 6)) ## [1] 3000 6 # –æ–Ω–∏ –ø—Ä–∏–≤—è–∑—ã–≤–∞—é—Ç—Å—è –∫ –∫–≤–∞—Ä—Ç–∏–ª—è–º attr(bs(age, df = 6), &#39;knots&#39;) ## 25% 50% 75% ## 33.75 42.00 51.00 # –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–π —Å–ø–ª–∞–π–Ω fit2 &lt;- lm(wage ~ ns(age, df = 4), data = Wage) preds.spl2 &lt;- predict(fit2, newdata = list(age = age.grid), se = T) –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫—É–±–∏—á–µ—Å–∫–æ–≥–æ –∏ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ —Å–ø–ª–∞–π–Ω–æ–≤. par(mfrow = c(1, 1), mar = c(4.5, 4.5, 1, 8.5), oma = c(0, 0, 0, 0), xpd = T) # –Ω–∞–±–ª—é–¥–µ–Ω–∏—è plot(age, wage, col = &#39;grey&#39;) # –º–æ–¥–µ–ª—å –∫—É–±–∏—á–µ—Å–∫–æ–≥–æ —Å–ø–ª–∞–π–Ω–∞ lines(age.grid, preds.spl$fit, lwd = 2) # –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª lines(age.grid, preds.spl$fit + 2*preds.spl$se, lty = &#39;dashed&#39;) lines(age.grid, preds.spl$fit - 2*preds.spl$se, lty = &#39;dashed&#39;) # –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–π —Å–ø–ª–∞–π–Ω lines(age.grid, preds.spl2$fit, col = &#39;red&#39;, lwd = 2) # –ª–µ–≥–µ–Ω–¥–∞ legend(&quot;topright&quot;, inset = c(-0.7, 0), c(&#39;–ö—É–±–∏—á–µ—Å–∫–∏–π \\n —Å 3 —É–∑–ª–∞–º–∏&#39;, &#39;–ù–∞—Ç—É—Ä–∞–ª—å–Ω—ã–π&#39;), lwd = rep(2, 2), col = c(&#39;black&#39;, &#39;red&#39;)) # –∑–∞–≥–æ–ª–æ–≤–æ–∫ title(&quot;–°–ø–ª–∞–π–Ω—ã&quot;) –ü–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ —Å–æ —Å–ª–∞–π–¥–∞ 20 (—Ä–∏—Å—É–Ω–æ–∫ 7.8 –∫–Ω–∏–≥–∏). par(mfrow = c(1, 1), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0)) # –Ω–∞–±–ª—é–¥–µ–Ω–∏—è plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) # –∑–∞–≥–æ–ª–æ–≤–æ–∫ title(&#39;–°–≥–ª–∞–∂–∏–≤–∞—é—â–∏–π —Å–ø–ª–∞–π–Ω&#39;) # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å 16 —Å—Ç–µ–ø–µ–Ω—è–º–∏ —Å–≤–æ–±–æ–¥—ã fit &lt;- smooth.spline(age, wage, df = 16) # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å –ø–æ–¥–±–æ—Ä–æ–º –ª—è–º–±–¥—ã —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ fit2 &lt;- smooth.spline(age, wage, cv = T) ## Warning in smooth.spline(age, wage, cv = T): cross-validation with non- ## unique &#39;x&#39; values seems doubtful fit2$df ## [1] 6.794596 # —Ä–∏—Å—É–µ–º –º–æ–¥–µ–ª—å lines(fit, col = &#39;red&#39;, lwd = 2) lines(fit2, col = &#39;blue&#39;, lwd = 2) legend(&#39;topright&#39;, c(&#39;16 df&#39;, &#39;6.8 df&#39;), col = c(&#39;red&#39;, &#39;blue&#39;), lty = 1, lwd = 2, cex = 0.8) –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ —Å–æ —Å–ª–∞–π–¥–∞ 24 (—Ä–∏—Å. 7.10). plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) title(&#39;–õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è&#39;) # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å c –æ–∫–Ω–æ–º 0.2 fit &lt;- loess(wage ~ age, span = 0.2, data = Wage) # –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å c –æ–∫–Ω–æ–º 0.5 fit2 &lt;- loess(wage ~ age, span = 0.5, data = Wage) # —Ä–∏—Å—É–º –º–æ–¥–µ–ª–∏ lines(age.grid, predict(fit, data.frame(age = age.grid)), col = &#39;red&#39;, lwd = 2) lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = &#39;blue&#39;, lwd = 2) # –ª–µ–≥–µ–Ω–¥–∞ legend(&#39;topright&#39;, c(&#39;s = 0.2&#39;, &#39;s = 0.5&#39;), col = c(&#39;red&#39;, &#39;blue&#39;), lty = 1, lwd = 2, cex = 0.8) –û–±–æ–±—â—ë–Ω–Ω—ã–µ –∞–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (GAM) —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –æ—Ç–∫–ª–∏–∫–æ–º –ü–æ—Å—Ç—Ä–æ–∏–º GAM –Ω–∞ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã—Ö —Å–ø–ª–∞–π–Ω–∞—Ö —Å—Ç–µ–ø–µ–Ω–µ–π 4 (year), 5 (age) —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º edication. # GAM –Ω–∞ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã—Ö —Å–ø–ª–∞–π–Ω–∞—Ö gam.ns &lt;- gam(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage) –¢–∞–∫–∂–µ –ø–æ—Å—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å –Ω–∞ —Å–≥–ª–∞–∂–∏–≤–∞—é—â–∏—Ö —Å–ø–ª–∞–π–Ω–∞—Ö. # GAM –Ω–∞ —Å–≥–ª–∞–∂–∏–≤–∞—é—â–∏—Ö —Å–ø–ª–∞–π–Ω–∞—Ö gam.m3 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage) –ì—Ä–∞—Ñ–∏–∫ —Å–æ —Å–ª–∞–π–¥–∞ 28 (—Ä–∏—Å. 7.12). par(mfrow = c(1, 3)) plot(gam.m3, se = T, col = &#39;blue&#39;) –ì—Ä–∞—Ñ–∏–∫ —Å–æ —Å–ª–∞–π–¥–∞ 27 (—Ä–∏—Å. 7.11). par(mfrow = c(1, 3)) plot(gam.ns, se = T, col = &#39;red&#39;) –ì—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç year –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä—è–º—É—é. –°–¥–µ–ª–∞–µ–º ANOVA, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∞—è —Å—Ç–µ–ø–µ–Ω—å –¥–ª—è year –ª—É—á—à–µ. gam.m1 &lt;- gam(wage ~ s(age, 5) + education, data = Wage) # –±–µ–∑ year gam.m2 &lt;- gam(wage ~ year + s(age, 5) + education, data = Wage) # year^1 anova(gam.m1, gam.m2, gam.m3, test = &#39;F&#39;) Resid. Df &lt;dbl&gt; Resid. Dev &lt;dbl&gt; Df &lt;dbl&gt; Deviance &lt;dbl&gt; F &lt;dbl&gt; Pr(&gt;F) &lt;dbl&gt; 2990 3711731 NA NA NA NA 2989 3693842 1.000000 17889.243 14.477130 0.0001447167 2986 3689770 2.999989 4071.134 1.098212 0.3485661430 3 rows –¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –ª—É—á—à–µ –≤—Ç–æ—Ä–æ–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ–¥–∏–Ω –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ–∑–Ω–∞—á–∏–º. # —Å–≤–æ–¥–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏ gam.m3 summary(gam.m3) ## ## Call: gam(formula = wage ~ s(year, 4) + s(age, 5) + education, data = Wage) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -119.43 -19.70 -3.33 14.17 213.48 ## ## (Dispersion Parameter for gaussian family taken to be 1235.69) ## ## Null Deviance: 5222086 on 2999 degrees of freedom ## Residual Deviance: 3689770 on 2986 degrees of freedom ## AIC: 29887.75 ## ## Number of Local Scoring Iterations: 2 ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## s(year, 4) 1 27162 27162 21.981 2.877e-06 *** ## s(age, 5) 1 195338 195338 158.081 &lt; 2.2e-16 *** ## education 4 1069726 267432 216.423 &lt; 2.2e-16 *** ## Residuals 2986 3689770 1236 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## s(year, 4) 3 1.086 0.3537 ## s(age, 5) 4 32.380 &lt;2e-16 *** ## education ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 –†–∞–±–æ—Ç–∞–µ–º —Å –º–æ–¥–µ–ª—å—é gam.m2. # –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ preds &lt;- predict(gam.m2, newdata = Wage) –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ GAM –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. # GAM –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏—è—Ö gam.lo &lt;- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, data = Wage) par(mfrow = c(1, 3)) plot.gam(gam.lo, se = T, col = &#39;green&#39;) # –º–æ–¥–µ–ª—å —Å–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ year –∏ age gam.lo.i &lt;- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : liv too small. (Discovered by lowesd) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : lv too small. (Discovered by lowesd) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : liv too small. (Discovered by lowesd) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : lv too small. (Discovered by lowesd) plot(gam.lo.i) –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è GAM –ü–æ—Å—Ç—Ä–æ–∏–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é GAM –¥–ª—è –≤—Å–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ, —á—Ç–æ wage –ø—Ä–µ–≤—ã—à–∞–µ—Ç 250. gam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age, df = 5) + education, family = &#39;binomial&#39;, data = Wage) par(mfrow = c(1, 3)) plot(gam.lr, se = T, col = &#39;green&#39;) # —É—Ä–æ–≤–Ω–∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º —Ä–∞–∑–Ω–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–∫–∞ table(education, I(wage &gt; 250)) ## ## education FALSE TRUE ## 1. &lt; HS Grad 268 0 ## 2. HS Grad 966 5 ## 3. Some College 643 7 ## 4. College Grad 663 22 ## 5. Advanced Degree 381 45 –í –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —Å–∞–º—ã–º –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–µ—Ç wage &gt; 250, –ø–æ—ç—Ç–æ–º—É —É–±–∏—Ä–∞–µ–º –µ—ë. gam.lr.s &lt;- gam(I(wage &gt; 250) ~ year + s(age, df = 5) + education, family = &#39;binomial&#39;, data = Wage, subset = (education != &quot;1. &lt; HS Grad&quot;)) # –≥—Ä–∞—Ñ–∏–∫ par(mfrow = c(1, 3)) plot(gam.lr.s, se = T, col = &#39;green&#39;) detach(Wage) # Nonlinear modeling –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ü—Ä–∞–∫—Ç–∏–∫–∞ 8 –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –í –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –Ω–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ –∫–∞–∫: —Å—Ç—Ä–æ–∏—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è; —Å—Ç—Ä–æ–∏—Ç—å –¥–µ—Ä–µ–≤—å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏; –¥–µ–ª–∞—Ç—å –æ–±—Ä–µ–∑–∫—É –¥–µ—Ä–µ–≤–∞; –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ç–≥–≥–∏–Ω–≥, –±—É—Å—Ç–∏–Ω–≥, —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª–∏: –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π. –î–∞–Ω–Ω—ã–µ: Sales {ISLR}, Boston {ISLR} –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∫–æ–¥—É –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö —Å–º. –≤ [1], –≥–ª–∞–≤–∞ 8. library(&#39;tree&#39;) # –¥–µ—Ä–µ–≤—å—è ## Warning: package &#39;tree&#39; was built under R version 3.4.4 library(&#39;ISLR&#39;) # –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö library(&#39;MASS&#39;) library(&#39;randomForest&#39;) # —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å ## Warning: package &#39;randomForest&#39; was built under R version 3.4.4 ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. library(&#39;gbm&#39;) ## Warning: package &#39;gbm&#39; was built under R version 3.4.4 ## Loading required package: survival ## Loading required package: lattice ## Loading required package: splines ## Loading required package: parallel ## Loaded gbm 2.1.3 –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –ó–∞–≥—Ä—É–∑–∏–º —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –¥–µ—Ç—Å–∫–∏—Ö –∫—Ä–µ—Å–µ–ª –∏ –¥–æ–±–∞–≤–∏–º –∫ –Ω–µ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é High ‚Äì ‚Äú–≤—ã—Å–æ–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏‚Äù —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: Yes –µ—Å–ª–∏ –ø—Ä–æ–¥–∞–∂–∏ –±–æ–ª—å—à–µ 8 (—Ç—ã—Å. —à—Ç.); No –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ. ?Carseats ## starting httpd help server ... done attach(Carseats) # –Ω–æ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è High &lt;- ifelse(Sales &lt;= 8, &quot;No&quot;, &quot;Yes&quot;) # –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –∫ —Ç–∞–±–ª–∏—Ü–µ –¥–∞–Ω–Ω—ã—Ö Carseats &lt;- data.frame(Carseats, High) –°—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞ High, –æ—Ç–±—Ä–æ—Å–∏–≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –æ—Ç–∫–ª–∏–∫ Sales. # –º–æ–¥–µ–ª—å –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ tree.carseats &lt;- tree(High ~ . -Sales, Carseats) summary(tree.carseats) ## ## Classification tree: ## tree(formula = High ~ . - Sales, data = Carseats) ## Variables actually used in tree construction: ## [1] &quot;ShelveLoc&quot; &quot;Price&quot; &quot;Income&quot; &quot;CompPrice&quot; &quot;Population&quot; ## [6] &quot;Advertising&quot; &quot;Age&quot; &quot;US&quot; ## Number of terminal nodes: 27 ## Residual mean deviance: 0.4575 = 170.7 / 373 ## Misclassification error rate: 0.09 = 36 / 400 # –≥—Ä–∞—Ñ–∏–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ plot(tree.carseats) # –≤–µ—Ç–≤–∏ text(tree.carseats, pretty=0) # –ø–æ–¥–ø–∏—Å–∏ tree.carseats # –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å—ë –¥–µ—Ä–µ–≤–æ –≤ –∫–æ–Ω—Å–æ–ª–∏ ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 400 541.500 No ( 0.59000 0.41000 ) ## 2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 ) ## 4) Price &lt; 92.5 46 56.530 Yes ( 0.30435 0.69565 ) ## 8) Income &lt; 57 10 12.220 No ( 0.70000 0.30000 ) ## 16) CompPrice &lt; 110.5 5 0.000 No ( 1.00000 0.00000 ) * ## 17) CompPrice &gt; 110.5 5 6.730 Yes ( 0.40000 0.60000 ) * ## 9) Income &gt; 57 36 35.470 Yes ( 0.19444 0.80556 ) ## 18) Population &lt; 207.5 16 21.170 Yes ( 0.37500 0.62500 ) * ## 19) Population &gt; 207.5 20 7.941 Yes ( 0.05000 0.95000 ) * ## 5) Price &gt; 92.5 269 299.800 No ( 0.75465 0.24535 ) ## 10) Advertising &lt; 13.5 224 213.200 No ( 0.81696 0.18304 ) ## 20) CompPrice &lt; 124.5 96 44.890 No ( 0.93750 0.06250 ) ## 40) Price &lt; 106.5 38 33.150 No ( 0.84211 0.15789 ) ## 80) Population &lt; 177 12 16.300 No ( 0.58333 0.41667 ) ## 160) Income &lt; 60.5 6 0.000 No ( 1.00000 0.00000 ) * ## 161) Income &gt; 60.5 6 5.407 Yes ( 0.16667 0.83333 ) * ## 81) Population &gt; 177 26 8.477 No ( 0.96154 0.03846 ) * ## 41) Price &gt; 106.5 58 0.000 No ( 1.00000 0.00000 ) * ## 21) CompPrice &gt; 124.5 128 150.200 No ( 0.72656 0.27344 ) ## 42) Price &lt; 122.5 51 70.680 Yes ( 0.49020 0.50980 ) ## 84) ShelveLoc: Bad 11 6.702 No ( 0.90909 0.09091 ) * ## 85) ShelveLoc: Medium 40 52.930 Yes ( 0.37500 0.62500 ) ## 170) Price &lt; 109.5 16 7.481 Yes ( 0.06250 0.93750 ) * ## 171) Price &gt; 109.5 24 32.600 No ( 0.58333 0.41667 ) ## 342) Age &lt; 49.5 13 16.050 Yes ( 0.30769 0.69231 ) * ## 343) Age &gt; 49.5 11 6.702 No ( 0.90909 0.09091 ) * ## 43) Price &gt; 122.5 77 55.540 No ( 0.88312 0.11688 ) ## 86) CompPrice &lt; 147.5 58 17.400 No ( 0.96552 0.03448 ) * ## 87) CompPrice &gt; 147.5 19 25.010 No ( 0.63158 0.36842 ) ## 174) Price &lt; 147 12 16.300 Yes ( 0.41667 0.58333 ) ## 348) CompPrice &lt; 152.5 7 5.742 Yes ( 0.14286 0.85714 ) * ## 349) CompPrice &gt; 152.5 5 5.004 No ( 0.80000 0.20000 ) * ## 175) Price &gt; 147 7 0.000 No ( 1.00000 0.00000 ) * ## 11) Advertising &gt; 13.5 45 61.830 Yes ( 0.44444 0.55556 ) ## 22) Age &lt; 54.5 25 25.020 Yes ( 0.20000 0.80000 ) ## 44) CompPrice &lt; 130.5 14 18.250 Yes ( 0.35714 0.64286 ) ## 88) Income &lt; 100 9 12.370 No ( 0.55556 0.44444 ) * ## 89) Income &gt; 100 5 0.000 Yes ( 0.00000 1.00000 ) * ## 45) CompPrice &gt; 130.5 11 0.000 Yes ( 0.00000 1.00000 ) * ## 23) Age &gt; 54.5 20 22.490 No ( 0.75000 0.25000 ) ## 46) CompPrice &lt; 122.5 10 0.000 No ( 1.00000 0.00000 ) * ## 47) CompPrice &gt; 122.5 10 13.860 No ( 0.50000 0.50000 ) ## 94) Price &lt; 125 5 0.000 Yes ( 0.00000 1.00000 ) * ## 95) Price &gt; 125 5 0.000 No ( 1.00000 0.00000 ) * ## 3) ShelveLoc: Good 85 90.330 Yes ( 0.22353 0.77647 ) ## 6) Price &lt; 135 68 49.260 Yes ( 0.11765 0.88235 ) ## 12) US: No 17 22.070 Yes ( 0.35294 0.64706 ) ## 24) Price &lt; 109 8 0.000 Yes ( 0.00000 1.00000 ) * ## 25) Price &gt; 109 9 11.460 No ( 0.66667 0.33333 ) * ## 13) US: Yes 51 16.880 Yes ( 0.03922 0.96078 ) * ## 7) Price &gt; 135 17 22.070 No ( 0.64706 0.35294 ) ## 14) Income &lt; 46 6 0.000 No ( 1.00000 0.00000 ) * ## 15) Income &gt; 46 11 15.160 Yes ( 0.45455 0.54545 ) * –¢–µ–ø–µ—Ä—å –ø–æ—Å—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –∏ –æ—Ü–µ–Ω–∏–º –æ—à–∏–±–∫—É –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π. # —è–¥—Ä–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª set.seed(2) # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ train &lt;- sample(1:nrow(Carseats), 200) # —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ Carseats.test &lt;- Carseats[-train,] High.test &lt;- High[-train] # —Å—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ tree.carseats &lt;- tree(High ~ . -Sales, Carseats, subset = train) # –¥–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ tree.pred &lt;- predict(tree.carseats, Carseats.test, type = &quot;class&quot;) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π tbl &lt;- table(tree.pred, High.test) tbl ## High.test ## tree.pred No Yes ## No 86 27 ## Yes 30 57 # –æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ acc.test &lt;- sum(diag(tbl))/sum(tbl) acc.test ## [1] 0.715 –û–±–æ–±—â—ë–Ω–Ω–∞—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏: –¥–æ–ª—è –≤–µ—Ä–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: 0.72. –¢–µ–ø–µ—Ä—å –æ–±—Ä–µ–∑–∞–µ–º –¥–µ—Ä–µ–≤–æ, –∏—Å–ø–æ–ª—å–∑—É—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫—Ä–∏—Ç–µ—Ä–∏—è —á–∞—Å—Ç–æ—Ç—É –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –§—É–Ω–∫—Ü–∏—è cv.tree() –ø—Ä–æ–≤–æ–¥–∏—Ç –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ –¥–µ—Ä–µ–≤–∞, –∞—Ä–≥—É–º–µ–Ω—Ç prune.misclass –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. set.seed(3) cv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass) # –∏–º–µ–Ω–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ names(cv.carseats) ## [1] &quot;size&quot; &quot;dev&quot; &quot;k&quot; &quot;method&quot; # —Å–∞–º –æ–±—ä–µ–∫—Ç cv.carseats ## $size ## [1] 19 17 14 13 9 7 3 2 1 ## ## $dev ## [1] 55 55 53 52 50 56 69 65 80 ## ## $k ## [1] -Inf 0.0000000 0.6666667 1.0000000 1.7500000 2.0000000 ## [7] 4.2500000 5.0000000 23.0000000 ## ## $method ## [1] &quot;misclass&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;prune&quot; &quot;tree.sequence&quot; # –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ—Ç–æ–¥–∞ –ø–æ —Ö–æ–¥—É –æ–±—Ä–µ–∑–∫–∏ –¥–µ—Ä–µ–≤–∞ ################### # 1. –æ—à–∏–±–∫–∞ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —á–∏—Å–ª–∞ —É–∑–ª–æ–≤ par(mfrow = c(1, 2)) plot(cv.carseats$size, cv.carseats$dev, type = &quot;b&quot;, ylab = &#39;–ß–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª. (dev)&#39;, xlab = &#39;–ß–∏—Å–ª–æ —É–∑–ª–æ–≤ (size)&#39;) # —Ä–∞–∑–º–µ—Ä –¥–µ—Ä–µ–≤–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–æ–π opt.size &lt;- cv.carseats$size[cv.carseats$dev == min(cv.carseats$dev)] abline(v = opt.size, col = &#39;red&#39;, &#39;lwd&#39; = 2) # —Å–æ–æ—Ç–≤. –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ø—Ä—è–º–∞—è mtext(opt.size, at = opt.size, side = 1, col = &#39;red&#39;, line = 1) # 2. –æ—à–∏–±–∫–∞ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —à—Ç—Ä–∞—Ñ–∞ –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å plot(cv.carseats$k, cv.carseats$dev, type = &quot;b&quot;, ylab = &#39;–ß–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª. (dev)&#39;, xlab = &#39;–®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å (k)&#39;) –ö–∞–∫ –≤–∏–¥–Ω–æ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ —Å–ª–µ–≤–∞, –º–∏–Ω–∏–º—É–º —á–∞—Å—Ç–æ—Ç—ã –æ—à–∏–±–æ–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –ø—Ä–∏ —á–∏—Å–ª–µ —É–∑–ª–æ–≤ 9. –û—Ü–µ–Ω–∏–º —Ç–æ—á–Ω–æ—Å—Ç—å –¥–µ—Ä–µ–≤–∞ —Å 9 —É–∑–ª–∞–º–∏. # –¥–µ—Ä–µ–≤–æ —Å 9 —É–∑–ª–∞–º–∏ prune.carseats &lt;- prune.misclass(tree.carseats, best = 9) # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è plot(prune.carseats) text(prune.carseats, pretty = 0) # –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É tree.pred &lt;- predict(prune.carseats, Carseats.test, type = &quot;class&quot;) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π tbl &lt;- table(tree.pred, High.test) tbl ## High.test ## tree.pred No Yes ## No 94 24 ## Yes 22 60 # –æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ acc.test &lt;- sum(diag(tbl))/sum(tbl) acc.test ## [1] 0.77 –¢–æ—á–Ω–æ—Å—Ç—å —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ —á—É—Ç—å –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 0.77. –£–≤–µ–ª–∏—á–∏–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤, –ø–æ–ª—É—á–∏–º –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–µ –¥–µ—Ä–µ–≤–æ, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ–µ. # –¥–µ—Ä–µ–≤–æ —Å 13 —É–∑–ª–∞–º–∏ prune.carseats &lt;- prune.misclass(tree.carseats, best = 15) # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è plot(prune.carseats) text(prune.carseats, pretty = 0) # –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É tree.pred &lt;- predict(prune.carseats, Carseats.test, type = &quot;class&quot;) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π tbl &lt;- table(tree.pred, High.test) tbl ## High.test ## tree.pred No Yes ## No 86 22 ## Yes 30 62 # –æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ acc.test &lt;- sum(diag(tbl))/sum(tbl) acc.test ## [1] 0.74 # —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã par(mfrow = c(1, 1)) –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è –í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö Boston. ?Boston # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ set.seed(1) train &lt;- sample(1:nrow(Boston), nrow(Boston)/2) # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ -- 50% –ü–æ—Å—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π medv: –º–µ–¥–∏–∞–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–æ–º–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –∂–∏–≤—É—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∏ (—Ç—ã—Å. –¥–æ–ª–ª.). # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å tree.boston &lt;- tree(medv ~ ., Boston, subset = train) summary(tree.boston) ## ## Regression tree: ## tree(formula = medv ~ ., data = Boston, subset = train) ## Variables actually used in tree construction: ## [1] &quot;lstat&quot; &quot;rm&quot; &quot;dis&quot; ## Number of terminal nodes: 8 ## Residual mean deviance: 12.65 = 3099 / 245 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -14.10000 -2.04200 -0.05357 0.00000 1.96000 12.60000 # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è plot(tree.boston) text(tree.boston, pretty = 0) –°–Ω–æ–≤–∞ —Å–¥–µ–ª–∞–µ–º –æ–±—Ä–µ–∑–∫—É –¥–µ—Ä–µ–≤–∞ –≤ —Ü–µ–ª—è—Ö —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞. cv.boston &lt;- cv.tree(tree.boston) # —Ä–∞–∑–º–µ—Ä –¥–µ—Ä–µ–≤–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–æ–π plot(cv.boston$size, cv.boston$dev, type = &#39;b&#39;) opt.size &lt;- cv.boston$size[cv.boston$dev == min(cv.boston$dev)] abline(v = opt.size, col = &#39;red&#39;, &#39;lwd&#39; = 2) # —Å–æ–æ—Ç–≤. –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ø—Ä—è–º–∞—è mtext(opt.size, at = opt.size, side = 1, col = &#39;red&#39;, line = 1) –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ–º –º–∏–Ω–∏–º—É–º –æ—à–∏–±–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–∞–º–æ–º—É —Å–ª–æ–∂–Ω–æ–º—É –¥–µ—Ä–µ–≤—É, —Å 8 —É–∑–ª–∞–º–∏. –ü–æ–∫–∞–∂–µ–º, –∫–∞–∫ –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å –¥–µ—Ä–µ–≤–æ –¥–æ 7 —É–∑–ª–æ–≤ (–æ—à–∏–±–∫–∞ –Ω–µ–Ω–∞–º–Ω–æ–≥–æ –≤—ã—à–µ, —á–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è). # –¥–µ—Ä–µ–≤–æ —Å 7 —É–∑–ª–∞–º–∏ prune.boston = prune.tree(tree.boston, best = 7) # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è plot(prune.boston) text(prune.boston, pretty = 0) –ü—Ä–æ–≥–Ω–æ–∑ —Å–¥–µ–ª–∞–µ–º –ø–æ –Ω–µ–æ–±—Ä–µ–∑–∞–Ω–Ω–æ–º—É –¥–µ—Ä–µ–≤—É, —Ç.–∫. —Ç–∞–º –æ—à–∏–±–∫–∞, –æ—Ü–µ–Ω–µ–Ω–Ω–∞—è –ø–æ –º–µ—Ç–æ–¥—É –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏, –º–∏–Ω–∏–º–∞–ª—å–Ω–∞. # –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (8 —É–∑–ª–æ–≤) yhat &lt;- predict(tree.boston, newdata = Boston[-train, ]) boston.test &lt;- Boston[-train, &quot;medv&quot;] # –≥—Ä–∞—Ñ–∏–∫ &quot;–ø—Ä–æ–≥–Ω–æ–∑ -- —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è&quot; plot(yhat, boston.test) # –ª–∏–Ω–∏—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ abline(0, 1) # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ mse.test &lt;- mean((yhat - boston.test)^2) MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–≤–Ω–∞ 25.05 (—Ç—ã—Å.–¥–æ–ª–ª.). –ë—ç–≥–≥–∏–Ω–≥ –∏ –º–µ—Ç–æ–¥ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤–∞. –ë—ç–≥–≥–∏–Ω–≥ ‚Äì —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ —Å m=p, –ø–æ—ç—Ç–æ–º—É –∏ —Ç–æ, –∏ –¥—Ä—É–≥–æ–µ –º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–µ–π randomForest(). –î–ª—è –Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—ç–≥–≥–∏–Ω–≥, –ø—Ä–∏—á—ë–º –≤–æ–∑—å–º—ë–º –≤—Å–µ 13 –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ (–∞—Ä–≥—É–º–µ–Ω—Ç mtry). # –±—ç–≥–≥–∏–Ω–≥ —Å 13 –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞–º–∏ set.seed(1) bag.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE) bag.boston ## ## Call: ## randomForest(formula = medv ~ ., data = Boston, mtry = 13, importance = TRUE, subset = train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 11.15723 ## % Var explained: 86.49 # –ø—Ä–æ–≥–Ω–æ–∑ yhat.bag = predict(bag.boston, newdata = Boston[-train, ]) # –≥—Ä–∞—Ñ–∏–∫ &quot;–ø—Ä–æ–≥–Ω–æ–∑ -- —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è&quot; plot(yhat.bag, boston.test) # –ª–∏–Ω–∏—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ abline(0, 1) # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π mse.test &lt;- mean((yhat.bag - boston.test)^2) mse.test ## [1] 13.50808 –û—à–∏–±–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–≤–Ω–∞ 13.51. –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤ —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ ntree. bag.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, ntree = 25) # –ø—Ä–æ–≥–Ω–æ–∑ yhat.bag &lt;- predict(bag.boston, newdata = Boston[-train, ]) # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π mse.test &lt;- mean((yhat.bag - boston.test)^2) mse.test ## [1] 13.94835 –ù–æ, –∫–∞–∫ –≤–∏–¥–Ω–æ, —ç—Ç–æ —Ç–æ–ª—å–∫–æ —É—Ö—É–¥—à–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑. –¢–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–µ–º –≤—ã—Ä–∞—Å—Ç–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å. –ë–µ—Ä—ë–º 6 –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å set.seed(1) rf.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE) # –ø—Ä–æ–≥–Ω–æ–∑ yhat.rf &lt;- predict(rf.boston, newdata = Boston[-train, ]) # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ mse.test &lt;- mean((yhat.rf - boston.test)^2) # –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ importance(rf.boston) # –æ—Ü–µ–Ω–∫–∏ ## %IncMSE IncNodePurity ## crim 12.132320 986.50338 ## zn 1.955579 57.96945 ## indus 9.069302 882.78261 ## chas 2.210835 45.22941 ## nox 11.104823 1044.33776 ## rm 31.784033 6359.31971 ## age 10.962684 516.82969 ## dis 15.015236 1224.11605 ## rad 4.118011 95.94586 ## tax 8.587932 502.96719 ## ptratio 12.503896 830.77523 ## black 6.702609 341.30361 ## lstat 30.695224 7505.73936 varImpPlot(rf.boston) # –≥—Ä–∞—Ñ–∏–∫–∏ –û—à–∏–±–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ —Ä–∞–≤–Ω–∞ 11.66, —á—Ç–æ –Ω–∏–∂–µ, —á–µ–º –¥–ª—è –±—ç–≥–≥–∏–Ω–≥–∞. –ë—É—Å—Ç–∏–Ω–≥ –ü–æ—Å—Ç—Ä–æ–∏–º 5000 —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤ —Å –≥–ª—É–±–∏–Ω–æ–π 4. set.seed(1) boost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4) # –≥—Ä–∞—Ñ–∏–∫ –∏ —Ç–∞–±–ª–∏—Ü–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö summary(boost.boston) # –≥—Ä–∞—Ñ–∏–∫–∏ —á–∞—Å—Ç–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –¥–≤—É—Ö –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ par(mfrow = c(1, 2)) plot(boost.boston, i = &quot;rm&quot;) plot(boost.boston, i = &quot;lstat&quot;) # –ø—Ä–æ–≥–Ω–æ–∑ yhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000) # MSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π mse.test &lt;- mean((yhat.boost - boston.test)^2) mse.test ## [1] 11.84434 –ù–∞—Å—Ç—Ä–æ–π–∫—É –±—É—Å—Ç–∏–Ω–≥–∞ –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ Œª (–∞—Ä–≥—É–º–µ–Ω—Ç shrinkage). –£—Å—Ç–∞–Ω–æ–≤–∏–º –µ–≥–æ —Ä–∞–≤–Ω—ã–º 0.2. # –º–µ–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ (lambda) –Ω–∞ 0.2 -- –∞—Ä–≥—É–º–µ–Ω—Ç shrinkage boost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F) # –ø—Ä–æ–≥–Ω–æ–∑ yhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000) # MSE –∞ —Ç–µ—Å—Ç–æ–≤–æ–π mse.test &lt;- mean((yhat.boost - boston.test)^2) mse.test ## [1] 11.51109 –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏–∑–º–µ–Ω–∏–≤ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä, –º—ã –µ—â—ë –Ω–µ–º–Ω–æ–≥–æ —Å–Ω–∏–∑–∏–ª–∏ –æ—à–∏–±–∫—É –ø—Ä–æ–≥–Ω–æ–∑–∞. "],["multiple-linear-regression.html", "Chapter 29 Multiple linear regression", " Chapter 29 Multiple linear regression Source: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤ R. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è # Dataset swiss ?swiss swiss &lt;- data.frame(swiss) str(swiss) # Histogram of fertility hist(swiss$Fertility, col=&#39;red&#39;) # Numeric predictors for Fertility prediction fit &lt;- lm(Fertility ~ Examination + Catholic, data = swiss) summary(fit) # the principal predictor is an &#39;examination&#39; with negative correlation. # Interaction of variables &#39;examination&#39; and &#39;catholics&#39; &#39;*&#39; fit2 &lt;- lm(Fertility ~ Examination*Catholic, data = swiss) summary(fit2) confint(fit2) # Categorical predictors # Histogram obviously have two parts -&gt; we can split data for two factors hist(swiss$Catholic, col = &#39;red&#39;) # Lets split &#39;Catholics&#39; for two groups: with many &#39;lots&#39; and few &#39;few&#39; swiss$religious &lt;- ifelse(swiss$Catholic &gt; 60, &#39;Lots&#39;, &#39;Few&#39;) swiss$religious &lt;- as.factor(swiss$religious) fit3 &lt;- lm(Fertility ~ Examination + religious, data = swiss) summary(fit3) # Interaction of variables fit4 &lt;- lm(Fertility ~ religious*Examination, data = swiss) summary(fit4) # plots ggplot(swiss, aes(x = Examination, y = Fertility)) + geom_point() ggplot(swiss, aes(x = Examination, y = Fertility)) + geom_point() + geom_smooth() ggplot(swiss, aes(x = Examination, y = Fertility)) + geom_point() + geom_smooth(method = &#39;lm&#39;) ggplot(swiss, aes(x = Examination, y = Fertility, col = religious)) + geom_point() ggplot(swiss, aes(x = Examination, y = Fertility, col = religious)) + geom_point() + geom_smooth() ggplot(swiss, aes(x = Examination, y = Fertility, col = religious)) + geom_point() + geom_smooth(method = &#39;lm&#39;) # fit5 &lt;- lm(Fertility ~ religious*Infant.Mortality*Examination, data = swiss) summary(fit5) # model comparison rm(swiss) swiss &lt;- data.frame(swiss) fit_full &lt;- lm(Fertility ~ ., data = swiss) summary(fit_full) fit_reduced1 &lt;- lm(Fertility ~ Infant.Mortality + Examination + Catholic + Education, data = swiss) summary(fit_reduced1) anova(fit_full, fit_reduced1) fit_reduced2 &lt;- lm(Fertility ~ Infant.Mortality + Education + Catholic + Agriculture, data = swiss) summary(fit_reduced2) anova(fit_full, fit_reduced2) # model selection optimal_fit &lt;- step(fit_full, direction = &#39;backward&#39;) summary(optimal_fit) "],["spline-model.html", "Chapter 30 Spline model 30.1 Splines 30.2 Area under the curve using spline method 30.3 Set data using given function and predict curve using spline method 30.4 Generate dataset from a given function 30.5 Split data for train and test 30.6 Diagram of the given function and generated datasets 30.7 Build a model using splines 30.8 Diagram of MSE for train and test data 30.9 Build optimal model and plot for the model 30.10 Bibliograpy", " Chapter 30 Spline model 30.1 Splines 30.2 Area under the curve using spline method Area under the plasma drug concentration-time curve In the field of pharmacokinetics, the area under the curve (AUC) is the definite integral in a plot of drug concentration in blood plasma vs.¬†time. In practice, the drug concentration is measured at certain discrete points in time and the trapezoidal rule is used to estimate AUC. We can find spline curve and calculate area under the curve to improve the result and compair with trapezoidal method. The AUC represents the total drug exposure over time (Wiki) . Data: Dependency of concentration (mg/L) from time (h). Aim: Plot curve and calculate the area under the curve library(MESS) time &lt;- c(0.5,1,2,4,8,12,18,24,36,48,72) conc &lt;- c(5.36,9.35,17.18,25.78,29.78,26.63,19.4,13.26,5.88,2.56,0.49) mod &lt;- smooth.spline(time, conc) predict.xy &lt;- predict(mod, x=seq(from=min(time), to=max(time), by=(max(time)-min(time))/((length(time)*10)))) plot(conc ~ time, pch=19, xlab=&#39;Time, h&#39;, ylab=&#39;Concentration, mg/L&#39;) lines(time, conc, col=&#39;green&#39;) lines(predict.xy, col=&#39;red&#39;) # Calculate area under the curve (auc) # trapezoidal method MESS::auc(time, conc, from=min(time), to=max(time), type=&#39;linear&#39;) ## [1] 721.9925 # spline with given data MESS::auc(time, conc, from=min(time), to=max(time), type=&#39;spline&#39;) ## [1] 712.7034 # trapezoidal method with expanded set from predicted spline MESS::auc(predict.xy[[1]], from=min(time), to=max(time), predict.xy[[2]], type=&#39;linear&#39;) ## [1] 712.4289 # spline from expanded with spline MESS::auc(predict.xy[[1]], from=min(time), to=max(time), predict.xy[[2]], type=&#39;spline&#39;) ## [1] 712.7106 30.3 Set data using given function and predict curve using spline method Aim: Generate random data with noise \\(y = f(x) + \\epsilon\\) from function \\(f(x)=4-0.02x+0.0055x^2-4.9*10^{-5}x^3; \\epsilon ~ N(0,1)\\) Make a model using spline method. my.seed &lt;- 1 n.all &lt;- 100 # amount of elements # set x values set.seed(my.seed) x.min &lt;-5 x.max &lt;- 105 x &lt;- runif(n.all, x.min, x.max) # set rundom x from 5 to 105 with n elements # noise set.seed(my.seed) noise.sd &lt;-1 # noise standard deviation noise &lt;- rnorm(mean=0, sd=noise.sd, n=n.all) # subset train data set.seed(my.seed) train.percent &lt;- 0.85 # percent of train data inTrain &lt;- sample(seq_along(x), size=(train.percent*n.all)) # true function y=f(x) y.func &lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3} # for plot of true values x.line &lt;- seq(x.min, x.max, length=n.all) y.line &lt;- y.func(x.line) # true function with noise y &lt;- y.func(x) + noise # train values x.train &lt;- x[inTrain] y.train &lt;- y[inTrain] # test values x.test &lt;- x[-inTrain] y.test &lt;- y[-inTrain] # plot x.lim &lt;- c(x.min, x.max) y.lim &lt;- c(min(y), max(y)) plot(x.train, y.train, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = x.lim, ylim = y.lim, cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # test data points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # true function lines(x.line, y.line, lwd = 2, lty = 2) # legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;), pch = c(16, 16, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;), lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2) ## Splain model As a model of given data we will use splains with degree of freedom from 2 (line) to 60 (number of knots is equal 2/3 from number of values). # build spline model with degrees of freedom df=6 mod &lt;- smooth.spline(x = x.train, y = y.train, df = 6) # model values for calculation of errors y.model.train &lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test &lt;- predict(mod, data.frame(x = x.test))$y[, 1] # sum of squared errors MSE &lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) names(MSE) &lt;- c(&#39;train&#39;, &#39;test&#39;) round(MSE, 2) ## train test ## 0.71 1.00 # build models with degree of freedoms from df 2 to 60 # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç–µ–ø–µ–Ω–µ–π —Å–≤–æ–±–æ–¥—ã –¥–ª—è –º–æ–¥–µ–ª–∏ —Å–ø–ª–∞–π–Ω–∞ max.df &lt;- 60 tbl &lt;- data.frame(df = 2:max.df) # —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –æ—à–∏–±–æ–∫ tbl$MSE.train &lt;- 0 # —Å—Ç–æ–ª–±–µ—Ü: –æ—à–∏–±–∫–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ tbl$MSE.test &lt;- 0 # —Å—Ç–æ–ª–±–µ—Ü: –æ—à–∏–±–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ # for each degree of freedom for (i in 2:max.df) { mod &lt;- smooth.spline(x = x.train, y = y.train, df = i) # model for train and test data y.model.train &lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test &lt;- predict(mod, data.frame(x = x.test))$y[, 1] # errors for train and learn data MSE &lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) tbl[tbl$df == i, c(&#39;MSE.train&#39;, &#39;MSE.test&#39;)] &lt;- MSE } head(tbl) ## df MSE.train MSE.test ## 1 2 3.6484333 3.3336892 ## 2 3 1.5185881 1.1532857 ## 3 4 0.8999800 0.8874002 ## 4 5 0.7477105 0.9483290 ## 5 6 0.7127908 1.0038393 ## 6 7 0.7000429 1.0300354 # Diagram: dependence of MSE from moedel flexibility plot(x = tbl$df, y = tbl$MSE.test, type = &#39;l&#39;, col = &#39;red&#39;, lwd = 2, xlab = &#39;Degree of freedom&#39;, ylab = &#39;MSE&#39;, ylim = c(min(tbl$MSE.train, tbl$MSE.test), max(tbl$MSE.train, tbl$MSE.test)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # head of plot mtext(&#39;Dependence of MSE from degree of freedom&#39;, side = 3) points(x = tbl$df, y = tbl$MSE.test, pch = 21, col = &#39;red&#39;, bg = &#39;red&#39;) lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) # –Ω–µ—É—Å—Ç—Ä–∞–Ω–∏–º–∞—è –æ—à–∏–±–∫–∞ abline(h = noise.sd, lty = 2, col = grey(0.4), lwd = 2) # –ª–µ–≥–µ–Ω–¥–∞ legend(&#39;topleft&#39;, legend = c(&#39;–æ–±—É—á–∞—é—â–∞—è&#39;, &#39;—Ç–µ—Å—Ç–æ–≤–∞—è&#39;), pch = c(NA, 16), col = c(grey(0.2), &#39;red&#39;), lty = c(1, 1), lwd = c(2, 2), cex = 1.2) # —Å—Ç–µ–ø–µ–Ω–∏ —Å–≤–æ–±–æ–¥—ã —É –Ω–∞–∏–º–µ–Ω—å—à–µ–π –æ—à–∏–±–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ min.MSE.test &lt;- min(tbl$MSE.test) df.min.MSE.test &lt;- tbl[tbl$MSE.test == min.MSE.test, &#39;df&#39;] # –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ—Å—Ç–æ—Ç–æ–π –º–æ–¥–µ–ª–∏ –ø–æ –≥—Ä–∞—Ñ–∏–∫—É df.my.MSE.test &lt;- 6 my.MSE.test &lt;- tbl[tbl$df == df.my.MSE.test, &#39;MSE.test&#39;] # —Å—Ç–∞–≤–∏–º —Ç–æ—á–∫—É –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ abline(v = df.my.MSE.test, lty = 2, lwd = 2) points(x = df.my.MSE.test, y = my.MSE.test, pch = 15, col = &#39;blue&#39;) mtext(df.my.MSE.test, side = 1, line = -1, at = df.my.MSE.test, col = &#39;blue&#39;, cex = 1.2) # –ì—Ä–∞—Ñ–∏–∫ 3: –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å (–∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –≥–∏–±–∫–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é) ############ mod.MSE.test &lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) # –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –º–æ–¥–µ–ª–∏ x.model.plot &lt;- seq(x.min, x.max, length = 250) y.model.plot &lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] # —É–±–∏—Ä–∞–µ–º —à–∏—Ä–æ–∫–∏–µ –ø–æ–ª—è —Ä–∏—Å—É–Ω–∫–∞ par(mar = c(4, 4, 1, 1)) # –Ω–∞–∏–º–µ–Ω—å—à–∏–µ/–Ω–∞–∏–±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –æ—Å—è–º x.lim &lt;- c(x.min, x.max) y.lim &lt;- c(min(y), max(y)) # –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å —à—É–º–æ–º (–æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞) plot(x.train, y.train, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = x.lim, ylim = y.lim, cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # head of the plot mtext(&#39;Initial data and the best fit model&#39;, side = 3) # test values points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # true function lines(x.line, y.line, lwd = 2, lty = 2) # model lines(x.model.plot, y.model.plot, lwd = 2, col = &#39;blue&#39;) # legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;, &#39;model&#39;), pch = c(16, 16, NA, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;, &#39;blue&#39;), lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2) In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding Runge‚Äôs phenomenon for higher degrees. ======= In this example we will generate data from a given function and then build a model using splines and estimate quality of the model. 30.4 Generate dataset from a given function # parameters to generate a dataset n.all &lt;- 100 # number of observations train.percent &lt;- 0.85 # portion of the data for training res.sd &lt;- 1 # standard deviation of noise x.min &lt;- 5 # min limit of the data x.max &lt;- 105 # max limit of the data # generate x set.seed(1) # to get reproducible results by randomizer x &lt;- runif(x.min, x.max, n = n.all) # noise from normal destibution set.seed(1) res &lt;- rnorm(mean = 0, sd = res.sd, n = n.all) # generate y using a given function y.func &lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3} # add noise y &lt;- y.func(x) + res 30.5 Split data for train and test # split dataset for training and test set.seed(1) # generate vector of chosen x for train data inTrain &lt;- sample(seq_along(x), size = train.percent*n.all) # train data set x.train &lt;- x[inTrain] y.train &lt;- y[inTrain] # test data set x.test &lt;- x[-inTrain] y.test &lt;- y[-inTrain] 30.6 Diagram of the given function and generated datasets # lines of generated data for plot x.line &lt;- seq(x.min, x.max, length = n.all) y.line &lt;- y.func(x.line) # PLOT # generate plot by train data par(mar = c(4, 4, 1, 1)) # reduce margins (optional) plot(x.train, y.train, main = &#39;Generated data and original function&#39;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points of test data points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # add the given function lines(x.line, y.line, lwd = 2, lty = 2) # add legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;), pch = c(16, 16, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;), lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2) 30.7 Build a model using splines We will compair sevaral models with degree of freedoms (df) from 2 to 40, where 2 correspond to a linear model. max.df &lt;- 40 # max degree of freedom (df) # tbl &lt;- data.frame(df = 2:max.df) # data frame for writing errors tbl$MSE.train &lt;- 0 # column 1: errors of train data tbl$MSE.test &lt;- 0 # —Åcolumn 2: errors of test data # generate models using for cycle for (i in 2:max.df) { mod &lt;- smooth.spline(x = x.train, y = y.train, df = i) # predicted values for train and test data using built model y.model.train &lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test &lt;- predict(mod, data.frame(x = x.test))$y[, 1] # MSE errors for train and test data MSE &lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) # write errors to the previously created data frame tbl[tbl$df == i, c(&#39;MSE.train&#39;, &#39;MSE.test&#39;)] &lt;- MSE } # view first rows of the table head(tbl, 4) ## df MSE.train MSE.test ## 1 2 3.6484333 3.3336892 ## 2 3 1.5185881 1.1532857 ## 3 4 0.8999800 0.8874002 ## 4 5 0.7477105 0.9483290 30.8 Diagram of MSE for train and test data # plot MSE from our table plot(x = tbl$df, y = tbl$MSE.test, main = &quot;Changes of MSE from degrees of freedom&quot;, type = &#39;l&#39;, col = &#39;red&#39;, lwd = 2, xlab = &#39;spline degree of freedom&#39;, ylab = &#39;MSE&#39;, ylim = c(min(tbl$MSE.train, tbl$MSE.test), max(tbl$MSE.train, tbl$MSE.test)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points(x = tbl$df, y = tbl$MSE.test, pch = 21, col = &#39;red&#39;, bg = &#39;red&#39;) lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) # minimal MSE abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) # add legend legend(&#39;topright&#39;, legend = c(&#39;train&#39;, &#39;test&#39;), pch = c(NA, 16), col = c(grey(0.2), &#39;red&#39;), lty = c(1, 1), lwd = c(2, 2), cex = 1.2) # df of minimal MSE for test data min.MSE.test &lt;- min(tbl$MSE.test) df.min.MSE.test &lt;- tbl[tbl$MSE.test == min.MSE.test, &#39;df&#39;] # optimal df for precise model and maximal simplicity df.my.MSE.test &lt;- 6 my.MSE.test &lt;- tbl[tbl$df == df.my.MSE.test, &#39;MSE.test&#39;] # show the optimal solution abline(v = df.my.MSE.test, lty = 2, lwd = 2) points(x = df.my.MSE.test, y = my.MSE.test, pch = 15, col = &#39;blue&#39;) mtext(df.my.MSE.test, side = 1, line = -1, at = df.my.MSE.test, col = &#39;blue&#39;, cex = 1.2) 30.9 Build optimal model and plot for the model mod.MSE.test &lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) # predict data for 250 x&#39;s to get smoothed curve x.model.plot &lt;- seq(x.min, x.max, length = 250) y.model.plot &lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] # plot train data par(mar = c(4, 4, 1, 1)) plot(x.train, y.train, main = &quot;Initial data and the best fit model&quot;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add test data points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # function lines(x.line, y.line,lwd = 2, lty = 2) # add model lines(x.model.plot, y.model.plot, lwd = 2, col = &#39;blue&#39;) # legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;, &#39;model&#39;), pch = c(16, 16, NA, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;, &#39;blue&#39;), lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2) 30.10 Bibliograpy An Introduction to Statistical Learning by Gareth James "],["logistic-regression.html", "Chapter 31 Logistic Regression 31.1 Confusion matrix 31.2 Next part 31.3 NExt part", " Chapter 31 Logistic Regression The logistic model (or logit model) is a statistical model with input (independent variable) a continuous variable and output (dependent variable) a binary variable (discret choice, e.g.¬†yes/no or 1/0). 31.1 Confusion matrix A confusion matrix is a table that is often used to describe the performance of a classification model (or ‚Äúclassifier‚Äù) on a set of test data for which the true values are known. n=165 Predicted: NO Predicted: Yes- Actual: No TN = 50 FP = 10 Actual: Yes FN = 5 TP = 100 TN - true negatives TP - true positives FN - false negatives FP - false posistives Accuracy - Overall, how often is the classifier correct? (TP+TN)/total = (100+50)/165 = 0.91 Misclassification Rate - Overall, how often is it wrong? (FP+FN)/total = (10+5)/165 = 0.09 equivalent to 1 - Accuracy also known as ‚ÄúError Rate‚Äù True Positive Rate: When it‚Äôs actually yes, how often does it predict yes? TP/actual yes = 100/105 = 0.95 also known as ‚ÄúSensitivity‚Äù or ‚ÄúRecall‚Äù False Positive Rate: When it‚Äôs actually no, how often does it predict yes? FP/actual no = 10/60 = 0.17 True Negative Rate: When it‚Äôs actually no, how often does it predict no? TN/actual no = 50/60 = 0.83 equivalent to 1 minus False Positive Rate also known as ‚ÄúSpecificity‚Äù Precision: When it predicts yes, how often is it correct? TP/predicted yes = 100/110 = 0.91 Prevalence: How often does the yes condition actually occur in our sample? actual yes/total = 105/165 = 0.64 Null Error Rate: This is how often you would be wrong if you always predicted the majority class. (In our example, the null error rate would be 60/165=0.36 because if you always predicted yes, you would only be wrong for the 60 ‚Äúno‚Äù cases.) This can be a useful baseline metric to compare your classifier against. However, the best classifier for a particular application will sometimes have a higher error rate than the null error rate, as demonstrated by the Accuracy Paradox. Cohen‚Äôs Kappa: This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. In other words, a model will have a high Kappa score if there is a big difference between the accuracy and the null error rate. (More details about Cohen‚Äôs Kappa.) F Score: This is a weighted average of the true positive rate (recall) and precision. (More details about the F Score.) ROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. (More details about ROC Curves.) Example: A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam? hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5) pass &lt;- c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1) model = glm(pass ~ hours, family = binomial) newdat &lt;- data.frame(hours=seq(min(hours), max(hours),len=100)) newdat$pass = predict(model, newdata=newdat, type=&quot;response&quot;) # plot plot(pass ~ hours) lines(pass ~ hours, newdat, col=&quot;red&quot;) # data data &lt;- data.frame(hours=c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5), pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1), pass.predic = rep(NA, 20), # slot for predicted pass pass.logit = rep(NA, 20)) # slot logit prediction # model model &lt;- glm(data$pass ~ data$hours, family = binomial) # predict values of logit function data$pass.logit &lt;- predict(model, newdata=data, type=&#39;response&#39;) # predict pass for threshold = 0.5 data$pass.predic &lt;- ifelse(data$pass.logit &gt; 0.5, 1, 0) # Confusion matrix library(caret) caret::confusionMatrix(data = factor(data$pass.predic), reference = factor(data$pass)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 8 2 ## 1 2 8 ## ## Accuracy : 0.8 ## 95% CI : (0.5634, 0.9427) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 0.005909 ## ## Kappa : 0.6 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8 ## Specificity : 0.8 ## Pos Pred Value : 0.8 ## Neg Pred Value : 0.8 ## Prevalence : 0.5 ## Detection Rate : 0.4 ## Detection Prevalence : 0.5 ## Balanced Accuracy : 0.8 ## ## &#39;Positive&#39; Class : 0 ## p &lt;- seq(0,1, by=0.05) data &lt;- data.frame(probability=p, odds=p/(1-p)) head(data) ## probability odds ## 1 0.00 0.00000000 ## 2 0.05 0.05263158 ## 3 0.10 0.11111111 ## 4 0.15 0.17647059 ## 5 0.20 0.25000000 ## 6 0.25 0.33333333 plot(data$odds~data$probability, type=&#39;o&#39;, pch=19, xlab=&#39;Probability&#39;, ylab=&#39;Odds&#39;) plot(log(data$odds)~data$odds, type=&#39;o&#39;, pch=19, xlab=&#39;Odds&#39;, ylab=&#39;log(odds)&#39;) plot(data$probability~log(data$odds), type=&#39;o&#39;, pch=19, xlab=&#39;log(odds)&#39;, ylab=&#39;Probability&#39;) 31.2 Next part library(data.table) df &lt;- fread(&#39;https://raw.githubusercontent.com/suvarzz/data/master/data_classification.csv&#39;, header=T, sep=&quot;,&quot;) head(df) plot(df[pass==1][,!3], col=&#39;red&#39;) points(df[pass==0][,!3], col=&#39;blue&#39;) model.logit &lt;- glm(pass ~ studied + slept, data = df, family = &#39;binomial&#39;) summary(model.logit) p.lda &lt;- predict(model.logit, df, type = &#39;response&#39;) df$predicted &lt;- ifelse(p.lda &gt; 0.5, 1, 0) head(df) a=-coef(model.logit)[1]/coef(model.logit)[2], b=-coef(model.logit)[1]/coef(model.logit)[3]) b0 = coef(model.logit)[1] b1 = mymodel$coefficients[[2]] b2 = mymodel$coefficients[[3]] z = b0 + (b1 * 1) + (b2 * 4) p = 1 / (1 + exp(-z)) if p=0.5 =&gt; z = 0 =&gt; b0 + b1*x + b2*y =&gt; segments(0,10.87,9.26,0) slept=(3.77-0.474*studied)/0.338 (0, 3.77/0.474) = (9.2, 0) (3.77/0.474,0) = (0, 10.87) segments(9.2,0, 0,10.87, lwd=2) 31.3 NExt part # Example of logistic regression # Source: 17 - –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤ R. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è by Anatoliy Karpov # Read data set train.csv: # Statistics of students in a school # gender - male/femail # read, write, math - points for subjects # hon - if honorary degree Y/N # FIX combine train and test into one csv file. Split train and test inside this script setwd(&quot;~/RData&quot;) df &lt;- read.csv(&quot;train.csv&quot;, sep=&quot;;&quot;) # Visual inspection of the dataset head(df) str(df) View(df) # N-not the best mark, Y-the best mark library(ggplot2) ggplot(df, aes(read,math,col=gender))+geom_point()+facet_grid(.~hon)+ theme(axis.text=element_text(size=25), axis.title=element_text(size=25, face=&#39;bold&#39;)) # Apply logistic regression # How hon depends on different variables: read, math, gender fit &lt;- glm(hon ~ read + math + gender, df, family = &quot;binomial&quot;) summary(fit) # Meanings of coefficients: # read-estimate: 0.06677 - if female, math is fixed, if read change to 1, then ln(odds) will be changed to 0.06677 # Get data from fit exp(fit$coefficients) # Predict model - ln(odds) head(predict(object=fit)) # Predict model - return probability to get the best mark for every person head(predict(object = fit, type = &quot;response&quot;)) # Add probabilities to get the best mark for every person in df df$prob &lt;- predict(object = fit, type = &quot;response&quot;) df # Part 2 # ROC-curve of predicted model library(ROCR) # Predicted values and real values pred_fit &lt;- prediction(df$prob, df$hon) # Calculate tpr - true positive rate and fpr - false positive rate perf_fit &lt;- performance(pred_fit, &quot;tpr&quot;, &quot;fpr&quot;) # plot ROC-curve plot(perf_fit, colorize=T, print.cutoffs.at = seq(0,1, by=0.1)) # Area under the curve: 0.87 auc &lt;- performance(pred_fit, measure = &quot;auc&quot;) str(auc) # How to detect the border and make a decision if student will get honorary degree # Specificity - how good we can predict negative results perf3 &lt;- performance(pred_fit, x.measure = &quot;cutoff&quot;, measure = &quot;spec&quot;) # Sencitivity - how good we can predict positive results perf4 &lt;- performance(pred_fit, x.measure = &quot;cutoff&quot;, measure = &quot;sens&quot;) # –û–±—â–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ perf5 &lt;- performance(pred_fit, x.measure = &quot;cutoff&quot;, measure = &quot;acc&quot;) plot(perf3, col = &#39;red&#39;, lwd = 2) plot(add=T, perf4, col = &quot;green&quot;, lwd = 2) plot(add=T, perf5, lwd = 2) legend(x = 0.6, y = 0.3, c(&quot;spec&quot;, &quot;sens&quot;, &quot;accur&quot;), lty=1, col=c(&quot;red&quot;, &quot;green&quot;, &quot;black&quot;), bty=&#39;n&#39;, cex=1, lwd=2) abline(v=0.255, lwd=2) # The border is the intersection of all three curves # Add column with prediced values Y/N df$pred_resp &lt;- factor(ifelse(df$prob &gt; 0.255, 1, 0), labels=c(&quot;N&quot;, &quot;Y&quot;)) # 1 if prediction is correct, 0 if not correct df$correct &lt;- ifelse(df$pred_resp == df$hon, 1, 0) df # blue - correct classified, red - incorrect classified # it is more difficult to predict positive result ggplot(df, aes(prob, fill = factor(correct)))+ geom_dotplot()+ theme(axis.text=element_text(size=25), axis.title=element_text(size=25, face=&quot;bold&quot;)) # Percent of positive predictions mean(df$correct) # Part 3 - Prediction using test data test_df &lt;- read.csv(&quot;test.csv&quot;, sep=&quot;;&quot;) test_df # Predict honorary members test_df$prob_predict &lt;- predict(fit, newdata=test_df, type=&quot;response&quot;) test_df$pred_resp &lt;- factor(ifelse(test_df$prob_predict &gt; 0.255, 1, 0), labels=c(&quot;N&quot;, &quot;Y&quot;)) test_df "],["models-for-binary-data.html", "Chapter 32 Models for binary Data", " Chapter 32 Models for binary Data For binary dependent variables build models: 1. LR - Logistic regression 2. LDA - Linear discriminant analysis 3. QDA - quadrat discriminant analysis How to detect the border of decision, use ROC-curves library(&#39;ISLR&#39;) library(&#39;GGally&#39;) library(&#39;MASS&#39;) my.seed &lt;- 12345 train.percent &lt;- 0.85 options(&quot;ggmatrix.progress.bar&quot; = FALSE) # Data set from ISLR: is credits returned Y/N dependent on: student, average balance and income. head(Default) str(Default) ### Primary analysis # Scatter plots for primary analysis ggp &lt;- ggpairs(Default) print(ggp, progress = FALSE) # rate of variables in default column table(Default$default) / sum(table(Default$default)) ### Train data subset set.seed(my.seed) inTrain &lt;- sample(seq_along(Default$default), nrow(Default)*train.percent) df &lt;- Default[inTrain, ] # actual values on the train data fact &lt;- df$default tail(fact, n=20) ### 1. Logistic regression modeling model.logit &lt;- glm(default ~ balance, data = df, family = &#39;binomial&#39;) summary(model.logit) # prognosis of probability belong to the class &#39;Yes&#39; p.logit &lt;- predict(model.logit, df, type = &#39;response&#39;) prognosis &lt;- factor(ifelse(p.logit &gt; 0.5, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # confusion matrix conf.m &lt;- table(fact, prognosis) conf.m # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) # spesifisity conf.m[1, 1] / sum(conf.m[1, ]) # accuracy sum(diag(conf.m)) / sum(conf.m) ###. 2. LDA model model.lda &lt;- lda(default ~ balance, data = Default[inTrain, ]) model.lda # prognosis of probability belong to the class &#39;Yes&#39; p.lda &lt;- predict(model.lda, df, type = &#39;response&#39;) prognosis &lt;- factor(ifelse(p.lda$posterior[, &#39;Yes&#39;] &gt; 0.5, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # confusion matrix conf.m &lt;- table(fact, prognosis) conf.m conf.m[2, 2] / sum(conf.m[2, ]) # sensitivity conf.m[1, 1] / sum(conf.m[1, ]) # specitivity sum(diag(conf.m)) / sum(conf.m) # accuracy ### 3. QDA modeling model.qda &lt;- qda(default ~ balance, data = Default[inTrain, ]) model.qda # prognosis of probability belong to the class &#39;Yes&#39; p.qda &lt;- predict(model.qda, df, type = &#39;response&#39;) –ü—Ä–æ–≥–Ω–æ–∑ &lt;- factor(ifelse(p.qda$posterior[, &#39;Yes&#39;] &gt; 0.5, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # confusion matrix conf.m &lt;- table(–§–∞–∫—Ç, –ü—Ä–æ–≥–Ω–æ–∑) conf.m conf.m[2, 2] / sum(conf.m[2, ]) # sensitivity conf.m[1, 1] / sum(conf.m[1, ]) # specitivity sum(diag(conf.m)) / sum(conf.m) # accuracy ### 4. ROC-curve and detection of decision border # calculate 1-SPC and TPR for all varients for clipping boundary x &lt;- NULL # for (1 - SPC) y &lt;- NULL # for TPR # template for the confusion matrix tbl &lt;- as.data.frame(matrix(rep(0, 4), 2, 2)) rownames(tbl) &lt;- c(&#39;fact.No&#39;, &#39;fact.Yes&#39;) colnames(tbl) &lt;- c(&#39;predict.No&#39;, &#39;predict.Yes&#39;) # vector of probabilities p.vector &lt;- seq(0, 1, length = 501) # go through the vector of probabilities for (p in p.vector){ # prognosis prognosis &lt;- factor(ifelse(p.lda$posterior[, &#39;Yes&#39;] &gt; p, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # data frame containing fact and prognosis df.compare &lt;- data.frame(fact = fact, prognosis = prognosis) # fill the confusion matrix tbl[1, 1] &lt;- nrow(df.compare[df.compare$fact == &#39;No&#39; &amp; df.compare$prognosis == &#39;No&#39;, ]) tbl[2, 2] &lt;- nrow(df.compare[df.compare$fact == &#39;Yes&#39; &amp; df.compare$prognosis == &#39;Yes&#39;, ]) tbl[1, 2] &lt;- nrow(df.compare[df.compare$fact == &#39;No&#39; &amp; df.compare$prognosis == &#39;Yes&#39;, ]) tbl[2, 1] &lt;- nrow(df.compare[df.compare$fact == &#39;Yes&#39; &amp; df.compare$prognosis == &#39;No&#39;, ]) # calculate characteristics TPR &lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1]) y &lt;- c(y, TPR) SPC &lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2]) x &lt;- c(x, 1 - SPC) } # build ROC-curve par(mar = c(5, 5, 1, 1)) # curve plot(x, y, type = &#39;l&#39;, col = &#39;blue&#39;, lwd = 3, xlab = &#39;(1 - SPC)&#39;, ylab = &#39;TPR&#39;, xlim = c(0, 1), ylim = c(0, 1)) # line for the classifier abline(a = 0, b = 1, lty = 3, lwd = 2) # point for the probability 0.5 points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16) text(x[p.vector == 0.5], y[p.vector == 0.5], &#39;p = 0.5&#39;, pos = 4) # point for the probability 0.2 points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16) text(x[p.vector == 0.2], y[p.vector == 0.2], &#39;p = 0.2&#39;, pos = 4) prognosis &lt;- factor(ifelse(p.lda$posterior[, &#39;Yes&#39;] &gt; 0.2, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) conf.m &lt;- table(fact, prognosis) conf.m # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) # specificity conf.m[1, 1] / sum(conf.m[1, ]) # accuracy sum(diag(conf.m)) / sum(conf.m) Sources Course ‚ÄòMath modeling‚Äô practical work, State University of Management, Moscow "],["support-vector-machine.html", "Chapter 33 Support Vector Machine", " Chapter 33 Support Vector Machine Definition: Support vector machine is a representation of the training data as points in space separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. SVMs are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Advantages: Effective in high dimensional spaces and uses a subset of training points in the decision function so it is also memory efficient. Disadvantages: The algorithm does not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. Parameters of SVM: * Type of kernel * Gamma value * C value ### Support Vector Machine # Classification library(e1071) # Generate data set.seed(10111) x = matrix(rnorm(40), 20, 2) y = rep(c(-1, 1), c(10, 10)) x[y == 1,] = x[y == 1,] + 1 # Plot generated data plot(x, col = y + 3, pch = 19) dat = data.frame(x, y = as.factor(y)) svmfit = svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) print(svmfit) plot(svmfit, dat) make.grid = function(x, n = 75) { grange = apply(x, 2, range) x1 = seq(from = grange[1,1], to = grange[2,1], length = n) x2 = seq(from = grange[1,2], to = grange[2,2], length = n) expand.grid(X1 = x1, X2 = x2) } xgrid = make.grid(x) xgrid[1:10,] ygrid = predict(svmfit, xgrid) plot(xgrid, col = c(&quot;red&quot;,&quot;blue&quot;)[as.numeric(ygrid)], pch = 20, cex = .2) points(x, col = y + 3, pch = 19) points(x[svmfit$index,], pch = 5, cex = 2) beta = drop(t(svmfit$coefs)%*%x[svmfit$index,]) beta0 = svmfit$rho plot(xgrid, col = c(&quot;red&quot;, &quot;blue&quot;)[as.numeric(ygrid)], pch = 20, cex = .2) points(x, col = y + 3, pch = 19) points(x[svmfit$index,], pch = 5, cex = 2) abline(beta0 / beta[2], -beta[1] / beta[2]) abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2) abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2) ### 2. Non-Linear SVM Classifier load(file = &quot;ESL.mixture.rda&quot;) names(ESL.mixture) rm(x, y) attach(ESL.mixture) plot(x, col = y + 1) dat = data.frame(y = factor(y), x) fit = svm(factor(y) ~ ., data = dat, scale = FALSE, kernel = &quot;radial&quot;, cost = 5) xgrid = expand.grid(X1 = px1, X2 = px2) ygrid = predict(fit, xgrid) plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) points(x, col = y + 1, pch = 19) func = predict(fit, xgrid, decision.values = TRUE) func = attributes(func)$decision xgrid = expand.grid(X1 = px1, X2 = px2) ygrid = predict(fit, xgrid) plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) points(x, col = y + 1, pch = 19) contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE) contour(px1, px2, matrix(func, 69, 99), level = 0.5, add = TRUE, col = &quot;blue&quot;, lwd = 2) Support vector machine –í –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –Ω–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ –∫–∞–∫: –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å SVM —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞–º–∏ —è–¥–µ—Ä–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç—Ä–æ–∏—Ç—å ROC-–∫—Ä–∏–≤—ã–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ R –ú–æ–¥–µ–ª–∏: SVM –î–∞–Ω–Ω—ã–µ: Khan {ISLR} –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∫–æ–¥—É –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö —Å–º. –≤ [1], –≥–ª–∞–≤–∞ 9. library(&#39;e1071&#39;) # SVM library(&#39;ROCR&#39;) # ROC-–∫—Ä–∏–≤—ã–µ library(&#39;ISLR&#39;) # –¥–∞–Ω–Ω—ã–µ –ø–æ —ç–∫—Å–ø—Ä–µ—Å—Å–∏–∏ –≥–µ–Ω–æ–≤ my.seed &lt;- 1 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö C–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –¥–≤–∞ –ª–∏–Ω–µ–π–Ω–æ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –∫–ª–∞—Å—Å–∞ # —Å–æ–∑–¥–∞—ë–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è set.seed(my.seed) x &lt;- matrix(rnorm(20*2), ncol = 2) y &lt;- c(rep(-1, 10), rep(1, 10)) x[y == 1, ] &lt;- x[y == 1, ] + 1 # –¥–∞–Ω–Ω—ã–µ –Ω–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ plot(x, pch = 19, col = (3 - y)) # —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏, –æ—Ç–∫–ª–∏–∫ -- —Ñ–∞–∫—Ç–æ—Ä dat &lt;- data.frame(x = x, y = as.factor(y)) # –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö —Å –ª–∏–Ω–µ–π–Ω–æ–π –≥—Ä–∞–Ω–∏—Ü–µ–π svmfit &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) # –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ –æ–ø–æ—Ä–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞–Ω—ã –∫—Ä–µ—Å—Ç–∏–∫–∞–º–∏ plot(svmfit, dat) # —Å–ø–∏—Å–æ–∫ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ svmfit$index ## [1] 1 2 5 7 14 16 17 # —Å–≤–æ–¥–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏ summary(svmfit) ## ## Call: ## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10, ## scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 10 ## gamma: 0.5 ## ## Number of Support Vectors: 7 ## ## ( 4 3 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 # —É–º–µ–Ω—å—à–∞–µ–º —à—Ç—Ä–∞—Ñ–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä svmfit &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 0.1, scale = FALSE) plot(svmfit, dat) svmfit$index ## [1] 1 2 3 4 5 7 9 10 12 13 14 15 16 17 18 20 # –¥–µ–ª–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É, –∏–∑–º–µ–Ω—è—è —à—Ç—Ä–∞—Ñ (–∞—Ä–≥—É–º–µ–Ω—Ç cost) set.seed(1) tune.out &lt;- tune(svm, y ~ ., data = dat, kernel = &quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))) summary(tune.out) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 0.1 ## ## - best performance: 0.1 ## ## - Detailed performance results: ## cost error dispersion ## 1 1e-03 0.70 0.4216370 ## 2 1e-02 0.70 0.4216370 ## 3 1e-01 0.10 0.2108185 ## 4 1e+00 0.15 0.2415229 ## 5 5e+00 0.15 0.2415229 ## 6 1e+01 0.15 0.2415229 ## 7 1e+02 0.15 0.2415229 # –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å -- —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–æ–π bestmod &lt;- tune.out$best.model summary(bestmod) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.1 ## gamma: 0.5 ## ## Number of Support Vectors: 16 ## ## ( 8 8 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ xtest &lt;- matrix(rnorm(20*2), ncol = 2) ytest &lt;- sample(c(-1,1), 20, rep = TRUE) xtest[ytest == 1, ] &lt;- xtest[ytest == 1, ] + 1 testdat &lt;- data.frame(x = xtest, y = as.factor(ytest)) # –¥–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ypred &lt;- predict(bestmod, testdat) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π table(predict = ypred, truth = testdat$y) ## truth ## predict -1 1 ## -1 11 1 ## 1 0 8 # –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –º–æ–¥–µ–ª–∏ —Å cost = 0.01 svmfit &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = .01, scale = FALSE) ypred &lt;- predict(svmfit, testdat) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π table(predict = ypred, truth = testdat$y) ## truth ## predict -1 1 ## -1 11 2 ## 1 0 7 –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –¥–≤–∞ –ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –∫–ª–∞—Å—Å–∞ # —Å–æ–∑–¥–∞—ë–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è x[y == 1, ] &lt;- x[y == 1, ] + 0.5 plot(x, col = (y+5)/2, pch = 19) # —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏, –æ—Ç–∫–ª–∏–∫ -- —Ñ–∞–∫—Ç–æ—Ä dat &lt;- data.frame(x = x, y = as.factor(y)) # –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π cost (–º–∞–ª–µ–Ω—å–∫–∏–π –∑–∞–∑–æ—Ä, –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏) svmfit &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 1e5) summary(svmfit) ## ## Call: ## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 1e+05) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1e+05 ## gamma: 0.5 ## ## Number of Support Vectors: 3 ## ## ( 1 2 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 plot(svmfit, dat) svmfit &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 1) summary(svmfit) ## ## Call: ## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 1) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## gamma: 0.5 ## ## Number of Support Vectors: 7 ## ## ( 4 3 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 plot(svmfit,dat) –ú–∞—à–∏–Ω–∞ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ # —Å–æ–∑–¥–∞—ë–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è set.seed(1) x &lt;- matrix(rnorm(200*2), ncol = 2) x[1:100, ] &lt;- x[1:100, ] + 2 x[101:150, ] &lt;- x[101:150, ] - 2 y &lt;- c(rep(1, 150), rep(2, 50)) # —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏, –æ—Ç–∫–ª–∏–∫ -- —Ñ–∞–∫—Ç–æ—Ä dat &lt;- data.frame(x = x, y = as.factor(y)) plot(x, col = y, pch = 19) # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ train &lt;- sample(200, 100) # SVM —Å —Ä–∞–¥–∏–∞–ª—å–Ω—ã–º —è–¥—Ä–æ–º –∏ –º–∞–ª–µ–Ω—å–∫–∏–º cost svmfit &lt;- svm(y ~ ., data = dat[train, ], kernel = &quot;radial&quot;, gamma = 1, cost = 1) plot(svmfit, dat[train, ]) summary(svmfit) ## ## Call: ## svm(formula = y ~ ., data = dat[train, ], kernel = &quot;radial&quot;, ## gamma = 1, cost = 1) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## gamma: 1 ## ## Number of Support Vectors: 37 ## ## ( 17 20 ) ## ## ## Number of Classes: 2 ## ## Levels: ## 1 2 # SVM —Å —Ä–∞–¥–∏–∞–ª—å–Ω—ã–º —è–¥—Ä–æ–º –∏ –±–æ–ª—å—à–∏–º cost svmfit &lt;- svm(y ~ ., data = dat[train, ], kernel = &quot;radial&quot;, gamma = 1, cost = 1e5) plot(svmfit, dat[train, ]) # –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ set.seed(1) tune.out &lt;- tune(svm, y ~ ., data = dat[train, ], kernel = &quot;radial&quot;, ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4))) summary(tune.out) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost gamma ## 1 2 ## ## - best performance: 0.12 ## ## - Detailed performance results: ## cost gamma error dispersion ## 1 1e-01 0.5 0.27 0.11595018 ## 2 1e+00 0.5 0.13 0.08232726 ## 3 1e+01 0.5 0.15 0.07071068 ## 4 1e+02 0.5 0.17 0.08232726 ## 5 1e+03 0.5 0.21 0.09944289 ## 6 1e-01 1.0 0.25 0.13540064 ## 7 1e+00 1.0 0.13 0.08232726 ## 8 1e+01 1.0 0.16 0.06992059 ## 9 1e+02 1.0 0.20 0.09428090 ## 10 1e+03 1.0 0.20 0.08164966 ## 11 1e-01 2.0 0.25 0.12692955 ## 12 1e+00 2.0 0.12 0.09189366 ## 13 1e+01 2.0 0.17 0.09486833 ## 14 1e+02 2.0 0.19 0.09944289 ## 15 1e+03 2.0 0.20 0.09428090 ## 16 1e-01 3.0 0.27 0.11595018 ## 17 1e+00 3.0 0.13 0.09486833 ## 18 1e+01 3.0 0.18 0.10327956 ## 19 1e+02 3.0 0.21 0.08755950 ## 20 1e+03 3.0 0.22 0.10327956 ## 21 1e-01 4.0 0.27 0.11595018 ## 22 1e+00 4.0 0.15 0.10801234 ## 23 1e+01 4.0 0.18 0.11352924 ## 24 1e+02 4.0 0.21 0.08755950 ## 25 1e+03 4.0 0.24 0.10749677 # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ table(true = dat[-train, &quot;y&quot;], pred = predict(tune.out$best.model, newdata = dat[-train, ])) ## pred ## true 1 2 ## 1 74 3 ## 2 7 16 ROC-–∫—Ä–∏–≤—ã–µ # —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è ROC-–∫—Ä–∏–≤–æ–π: pred -- –ø—Ä–æ–≥–Ω–æ–∑, truth -- —Ñ–∞–∫—Ç rocplot &lt;- function(pred, truth, ...){ predob = prediction(pred, truth) perf = performance(predob, &quot;tpr&quot;, &quot;fpr&quot;) plot(perf,...)} # –ø–æ—Å–ª–µ–¥–Ω—è—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å svmfit.opt &lt;- svm(y ~ ., data = dat[train, ], kernel = &quot;radial&quot;, gamma = 2, cost = 1, decision.values = T) # –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å fitted &lt;- attributes(predict(svmfit.opt, dat[train, ], decision.values = TRUE))$decision.values # –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ par(mfrow = c(1, 2)) rocplot(fitted, dat[train, &quot;y&quot;], main = &quot;Training Data&quot;) # –±–æ–ª–µ–µ –≥–∏–±–∫–∞—è –º–æ–¥–µ–ª—å (gamma –≤—ã—à–µ) svmfit.flex = svm(y ~ ., data = dat[train, ], kernel = &quot;radial&quot;, gamma = 50, cost = 1, decision.values = T) fitted &lt;- attributes(predict(svmfit.flex, dat[train, ], decision.values = T))$decision.values rocplot(fitted, dat[train,&quot;y&quot;], add = T, col = &quot;red&quot;) # –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ fitted &lt;- attributes(predict(svmfit.opt, dat[-train, ], decision.values = T))$decision.values rocplot(fitted, dat[-train, &quot;y&quot;], main = &quot;Test Data&quot;) fitted &lt;- attributes(predict(svmfit.flex, dat[-train, ], decision.values = T))$decision.values rocplot(fitted, dat[-train, &quot;y&quot;], add = T, col = &quot;red&quot;) SVM —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ set.seed(1) x &lt;- rbind(x, matrix(rnorm(50*2), ncol = 2)) y &lt;- c(y, rep(0, 50)) x[y == 0, 2] &lt;- x[y == 0, 2] + 2 dat &lt;- data.frame(x = x, y = as.factor(y)) # –≥—Ä–∞—Ñ–∏–∫ –∏ –º–æ–¥–µ–ª—å –ø–æ –º–µ—Ç–æ–¥—É &quot;–æ–¥–∏–Ω –ø—Ä–æ—Ç–∏–≤ –æ–¥–Ω–æ–≥–æ&quot; par(mfrow = c(1, 1)) plot(x, col = (y + 1)) svmfit = svm(y ~ ., data = dat, kernel = &quot;radial&quot;, cost = 10, gamma = 1) plot(svmfit, dat) –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–æ —É—Ä–æ–≤–Ω—é —ç–∫—Å–ø—Ä–µ—Å—Å–∏–∏ –≥–µ–Ω–æ–≤ # –¥–∞–Ω–Ω—ã–µ –ø–æ –æ–±—Ä–∞–∑—Ü–∞–º —Ç–∫–∞–Ω–µ–π —á–µ—Ç—ã—Ä—ë—Ö —Ç–∏–ø–æ–≤ —Å–∞—Ä–∫–æ–º—ã names(Khan) ## [1] &quot;xtrain&quot; &quot;xtest&quot; &quot;ytrain&quot; &quot;ytest&quot; dim(Khan$xtrain) # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞, –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã ## [1] 63 2308 dim(Khan$xtest) # —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞, –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã ## [1] 20 2308 length(Khan$ytrain) # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞, –æ—Ç–∫–ª–∏–∫ ## [1] 63 length(Khan$ytest) # —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞, –æ—Ç–∫–ª–∏–∫ ## [1] 20 table(Khan$ytrain) ## ## 1 2 3 4 ## 8 23 12 20 table(Khan$ytest) ## ## 1 2 3 4 ## 3 6 6 5 dat &lt;- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain)) # SVM —Å –ª–∏–Ω–µ–π–Ω—ã–º —è–¥—Ä–æ–º out &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10) summary(out) ## ## Call: ## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 10 ## gamma: 0.0004332756 ## ## Number of Support Vectors: 58 ## ## ( 20 20 11 7 ) ## ## ## Number of Classes: 4 ## ## Levels: ## 1 2 3 4 # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π table(out$fitted, dat$y) ## ## 1 2 3 4 ## 1 8 0 0 0 ## 2 0 23 0 0 ## 3 0 0 12 0 ## 4 0 0 0 20 # —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ dat.te &lt;- data.frame(x = Khan$xtest, y = as.factor(Khan$ytest)) # –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ pred.te &lt;- predict(out, newdata = dat.te) # –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π table(pred.te, dat.te$y) Sources https://www.datacamp.com/community/tutorials/support-vector-machines-r "],["clustering.html", "Chapter 34 Clustering 34.1 Finding distances using factoextra 34.2 Example of choosing clustering model 34.3 K-means clustering 34.4 k-Means 34.5 Hierarchical clustering 34.6 KNN", " Chapter 34 Clustering 5 classes of clustering methods: 1. Partitioning methods - split into k-groups (k-means, k-dedoids (PAM), CLARA) 2. Hierarchical clustering 3. Fuzzy clustering 4. Density-based clustering 5. Model-based clustering Clustering methods Representative-based Each cluster has a representative point May exist in the data or not. Also called centroid, center or prototype Algorithms (K-means, Fuzzy C-Means, K-medoids or PAM, Expectation maximization) Classification Connectivity-based Create clusters based on their distance (closest points, farthest points, ect) Hierarchical clustering agglomerative (bottom-up) divisive (top-down) Algorithms (Ward, SLINK, CLINK, etc) Density-based Cluster: area of higher density in the data Data points can be categorized as core, border, noise Algorithms (DBSCAN, OPTICS, Mean-shift) Evaluation of clustering algorithms Types of validataion methods: Internal: A quality metric computed on the data that was clustered (e.g., cluster compactness and separation) External: Clustering is compared to ‚Äúground truth‚Äù classification. Manual: Validated by a human expert. Indirect: Evaluating its utility in the intended application. 34.1 Finding distances using factoextra #Distances: stats::dist() factoextra::get_dist() # compute a distance matrix between the rows of a data matrix factoextra::fviz_dist() # visualize distance matrix cluster::daisy() # handle both numeric and not numeric (nominal, ordinal,...) data types 34.2 Example of choosing clustering model library(cluster) library(factoextra) d &lt;- factoextra::get_dist(USArrests, stand = TRUE, method = &#39;pearson&#39;) factoextra::fviz_dist(d, gradient = list(low=&#39;blue&#39;, mid=&#39;white&#39;, high=&#39;red&#39;)) ##### library(tidyverse) library(cluster) library(factoextra) data &lt;- USArrests %&gt;% na.omit() %&gt;% scale() data factoextra::fviz_nbclust(data, kmeans, method = &#39;gap_stat&#39;) km.res &lt;- kmeans(data, 3, nstart = 25) factoextra::fviz_cluster(km.res, data = data, ellipse.type = &#39;convex&#39;, palette = &#39;jco&#39;, repel = TRUE, ggtheme = theme_minimal()) # PAM clustering pam.res &lt;- cluster::pam(data, 4) factoextra::fviz_cluster(pam.res) # CLARA clustering clara.res &lt;- clara(df, 2, samples = 50, pamLike = TRUE) clara.res dd &lt;- cbind(df, cluster = clara.res$cluster) # Medoids clara.res$medoids # Clustering head(clara.res$clustering,10) 34.3 K-means clustering K-means assumptions Variables are all continuous Variables have a symmetric distribution (i.e., not skewed) Variables have similar means Variables have similar variance These assumptions come from the Euclidean distance Learned clusters will have a hyperspherical shape. For variables taking only positive values -&gt; apply logarithmic transformation For variable swith negative values -&gt; add a constant / calculate cubic root / Box-Cox transform If variables have different ranges of numbers -&gt; normalization is required scale() base R function will standardize the data. K - number of clusters in K-means clustering. Optimal K produces compact, well-separated clusters. Compactness: Within-Cluster Sum of Squares (WSS). minimize WSS: compact clusters! Separation: Between-Cluster Sum of Squares (BSS). maximize BSS: well-separated clusters! Problem: WSS decreases as K increases. The elbow method * Compute WSS, BSS, TSS = WSS + BSS * Select smallest k such that WSS / TSS &lt; 0.2 kmeans() function in the stats package 34.4 k-Means library(datasets) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species id ## 1 5.1 3.5 1.4 0.2 setosa 1 ## 2 4.9 3.0 1.4 0.2 setosa 2 ## 3 4.7 3.2 1.3 0.2 setosa 3 ## 4 4.6 3.1 1.5 0.2 setosa 4 ## 5 5.0 3.6 1.4 0.2 setosa 5 ## 6 5.4 3.9 1.7 0.4 setosa 6 # Plot Petal.Length ~ Petal.Width data plot(iris$Petal.Length ~ iris$Petal.Width) set.seed(20) # Find number of clusters using wss wss &lt;- (nrow(iris[, 3:4])-1)*sum(apply(iris[, 3:4],2,var)) for (i in 2:15) wss[i] &lt;- sum(kmeans(iris[, 3:4], i)$withinss) plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;) #More than 3 clusters give no obvious advantages # Make k-means with 3 clasters ncl &lt;- 3 irisCluster &lt;- kmeans(iris[, 3:4], ncl, nstart = 20) irisCluster ## K-means clustering with 3 clusters of sizes 48, 50, 52 ## ## Cluster means: ## Petal.Length Petal.Width ## 1 5.595833 2.037500 ## 2 1.462000 0.246000 ## 3 4.269231 1.342308 ## ## Clustering vector: ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [45] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 ## [89] 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 1 1 1 ## [133] 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 16.29167 2.02200 13.05769 ## (between_SS / total_SS = 94.3 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; ## [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; # Compair result of clustering with real data (3 species of iris are in analysis) table(irisCluster$cluster, iris$Species) ## ## setosa versicolor virginica ## 1 0 2 46 ## 2 50 0 0 ## 3 0 48 4 # Plot data clusters &lt;- split.data.frame(iris, irisCluster$cluster) xlim &lt;- c(min(iris$Petal.Width), max(iris$Petal.Width)) ylim &lt;- c(min(iris$Petal.Length), max(iris$Petal.Length)) col &lt;- c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;) plot(0, xlab=&#39;Petal width&#39;, ylab=&#39;Petal length&#39;, xlim=xlim, ylim=ylim) for ( i in 1:ncl ) { points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim) } 34.5 Hierarchical clustering bv &lt;- read.table(&quot;./DATA/beverage.csv&quot;, header=T, sep=&quot;;&quot;) head(bv) # no needs to normalize because all data is binary (0,1) # Hierarchical clustering # dist - calculate distances # hclust - hierarchical clustering clust.bv &lt;- hclust(dist(bv[,2:9]), &quot;ward.D&quot;) clust.bv # Plot clusters plot(clust.bv) plot(clust.bv, hang = -1) rect.hclust(clust.bv, k=3, border=&quot;red&quot;) # Group data by clusters groups &lt;- cutree(clust.bv, k=3) groups # Percentage in broups by drinking different beverages colMeans(bv[groups==1, 2:9])*100 colMeans(bv[groups==2, 2:9])*100 colMeans(bv[groups==3, 2:9])*100 # Interpretation # 1. People who does not have specific preference # 2. People who prefers cola and pepsi # 3. Not clear (others) # atributes of cluster analysis names(clust.bv) # chronic of combining clust.bv$merge clust.bv[1] clust.bv$height clust.bv$order clust.bv$labels clust.bv$method clust.bv$call clust.bv$dist.method # Detect the best choice for number of cluster by elbow-plot plot(1:33, clust.bv$height, type=&quot;l&quot;) ### Task. Analyse data and find groups of people # Scores (0,10) of 10 tests for candidates to get a job. # 1. Memorizing numbers # 2. Math task # 3. Solving tasks in dialoge # 4. Algorithms # 5. Self confidence # 6. Work in group # 7. Find solution # 8. Collaboration # 9. Acceptance by others setwd(&quot;~/DataAnalysis&quot;) job &lt;- read.table(&quot;DATA/assess.dat&quot;, header=T, sep=&quot;\\t&quot;) job # Clustering clust.job &lt;- hclust(dist(job[,3:ncol(job)]), &quot;ward.D&quot;) # no needs to normalize, because all numbers have the same min, max plot(clust.job) # visual number of clusters is 4 # Group data by clusters groups &lt;- cutree(clust.job, k=4) groups colMeans(job[groups==1, 3:12])*100 ### Find clusters using k-means method setwd(&quot;~/DataAnalysis&quot;) bv &lt;- read.table(&quot;DATA/beverage.csv&quot;, header=T, sep=&quot;;&quot;) bv dim(bv) names(bv) # k-means clustering, with initial 3 clusters # nstart = x - run x times with different initial clusters summ.1 = kmeans(bv[,2:9], 3, iter.max = 100) names(summ.1) # Objects by clusters summ.1$cluster # Centers of clusters summ.1$centers # 2 digits after point options(digits=2) t(summ.1$centers) options(digits=7) # Square summs summ.1$withinss # Summ of elements of vector summ.1$tot.withinss # sum(33*(apply(bv[,2:9], 2, sd))^2) summ.1$totss summ.1$tot.betweenss # Size of clusters summ.1$size # Elbow plot to detect optimal number of clusters wss &lt;- (nrow(bv[,2:9])-1)*sum(apply(bv[,2:9],2,var)) for (i in 2:15) { wss[i] &lt;- kmeans(bv[,2:9], centers=i)$tot.withinss } plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;) # We can see that diagram is rough. This is because clusters are not allways optimal # To improve situation, we have to run many initiall start coordinates and choose the best # option (add nstart=500): wss &lt;- (nrow(bv[,2:9])-1)*sum(apply(bv[,2:9],2,var)) for (i in 2:15) { wss[i] &lt;- kmeans(bv[,2:9], centers=i, nstart=500)$tot.withinss } plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;) # Warnings means that iterations were not finished for some cases. # Let&#39;s compair results for 3 and 4 clusters summ.1 = kmeans(bv[,2:9], 3, iter.max=100) summ.2 = kmeans(bv[,2:9], 4, iter.max=100) # Compair clusters. How many elements in each cluster # We can see how elements move if we take more clusters table(summ.1$cluster, summ.2$cluster) # Multidimentional scaling # Project multidimentional data to 2d bv.dist &lt;- dist(bv[,2:9]) bv.mds &lt;- cmdscale(bv.dist) plot(bv.mds, col = summ.1$cluster, xlab=&quot;Index&quot;, ylab=&quot;&quot;) # Detect optimal number of clusters install.packages(&quot;NbClust&quot;) library(&quot;NbClust&quot;) Best &lt;- NbClust(bv[,2:9], # data distance=&quot;euclidean&quot;, # distance method min.nc=2, # min number of clusters max.nc=8, # max number of clusters method=&quot;ward.D&quot;, # ward methodes index = &quot;alllong&quot; ) # choose indices 34.6 KNN # K-Nearest Neighbors or KNN is a clustering algorithm # k is known number of clusters (usually sqrt(N), between 3-10, but may be different) # samples must be normalized x = (x - min(x))/(max(x)-min(x)) head(iris) summary(iris) # detailed view of the data set str(iris) # view data types, sample values, categorical values, etc plot(iris) #normalization function min_max_normalizer &lt;- function(x) { num &lt;- x - min(x) denom &lt;- max(x) - min(x) return (num/denom) } #normalizing iris data set normalized_iris &lt;- as.data.frame(lapply(iris[1:4], min_max_normalizer)) #viewing normalized data summary(normalized_iris) #checking the data constituency table(iris$Species) #set seed for randomization set.seed(1234) # setting the training-test split to 67% and 33% respectively random_samples &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33)) # training data set iris.training &lt;- iris[ random_samples ==1, 1:4] #training labels iris.trainLabels &lt;- iris[ random_samples ==1, 5] # test data set iris.test &lt;- iris[ random_samples ==2, 1:4] #testing labels iris.testLabels &lt;- iris[ random_samples ==2, 5] #setting library library(class) #executing knn for k=3 iris_model &lt;- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3) #summary of the model learnt iris_model "],["regularization.html", "Chapter 35 Regularization", " Chapter 35 Regularization Regularization is a technique to discourage learning complex models, thus helping to avoid overfitting. Idea is to shrink the coefficient estimates towards zero. Three major types of regularization: L1 regularization (lasso regression) L2 regularization (ridge regression) Elastic net (combines lasso and ridge regression) Ridge regression adds fa shrinkage term to the RSS (Residual sum of squares) objective function. \\[RSS + ||\\overrightarrow{\\beta}||_2 = RSS + \\lambda \\sum_j \\beta_j^2\\] Shrinkage term uses the L2 norm of the coefficient vector. \\(\\lambda\\) regularization parameter (how much should model complexity be penalized?) \\(\\labda = 0\\) - original RSS function Need to standardize regressors before applying ridge regression Lasso regression LASSO = Least Absolute Shrinkage and Selection Operator \\[RSS + ||\\overrightarrow{\\beta}||_1 = RSS + \\lambda \\sum_j |\\beta_j|\\] Shrinkage term uses the L1 norm of the coefficient vector. Penalizes large coefficients more severely. More coefficients are likely becoming zero. Elastic net \\[RSS + \\lambda _1 ||\\overrightarrow{\\beta}||_1 + \\lambda _2 \\overrightarrow{\\beta} ||_2\\] Special cases: - \\(\\lambda _1 = lambda, \\lambda _2 = 0\\) - lasso regression - $_1 = 0, _2 = - ridge regression - \\(\\lambda _1 = lambda _2 = 0\\) - ordinary least squares (OLS) Boss lasso and ridge regression: Both methods improve generalization by penalizing model complexity. Their computational complexity is quite similar. Penalization hyperparameter \\(\\lambda\\) must be carefully set. Differences between lasso and ridge regressions: Ridge regression shrinks large coefficients but does not perform feature selection. Lasso regression performs both shrinkage and selection. L1 norm turns some coefficients to zero. Produces a more interpretable model. "],["factor-analysis.html", "Chapter 36 Factor analysis", " Chapter 36 Factor analysis ### FACTOR ANALYSIS # Data - 54 males stickleback behavior: # LUNGES - The number of lunges towards the model male. # BITES - The number of times that the male model was bitten. # ZIGZAGS - The &#39;Zig-Zag&#39; display is part of display behaviour, designed to attract females. # NEST - The number of nest building behaviours. # SPINES -The number of times the fish raised the &#39;spines&#39; on its back. # DNEST - The duration of nest building activities. # BOUT - The number of &#39;bout-facing&#39; behaviours (male-male interaction). df &lt;- read.table (&quot;/home/suvar/DataAnalysis/Boltengagen_Course/DATA/stickleback.csv&quot;, header=T, as.is=T, sep = &quot;\\t&quot;) # Primary data analysis df summary(df) # data is variable round(apply(df, 2, sd), 2) # variation plot(df) # obvious correlation is visible for LUNGES~BITES # outliers should be ideally deleted # Correlation round(cor(df),2) # Check for correlation between two variables: # Pearson correlation should be used with a caution: normality is not proven. cor.test(df$LUNGES, df$SPINES, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot;, exact = NULL, conf.level = 0.95) # Conclusions of primary analysis # 1. Fish like fight more than court # 2. Averages and variance are different # 3. NESt/DNEST more than have of samples are 0. # Factor Analysis # more than 3 factors will show error &quot;two many factors&quot;: # between 2 and 3 factors 2 shows better logical result and # split fish into two groups: agressive and peaceful: fa = factanal(df, factors=2, method = &quot;mls&quot;, scores = &quot;regression&quot;) fa # BOUT and SPINES are unique and not linked with other data, can be excluded # value close to 0 are omitted # Parameters fa$loadings # Loadings fa$uniquenesses fa$correlation fa$criteria fa$factors # how many factors fa$dof # degrees of freedom fa$method # method of factor weights fa$scores # factors: strength of signals for each sample fa$n.obs # number of observations fa$call # command plot(fa$scores[,1], fa$scores[,2]) # no correlation between factors # Correlation matrix between variables using factors # Find difference between two correlation matrix. Result is a model error. # Matrix multiplication loadings(fa) %*% t(loadings(fa)) # the same as fa$loadings %*% t(fa$loadings) # Diagonal matrix of correlation of unique factors diag(fa$uniquenesses) # Correlation matrix of initial data (prediction) fa$correlation # Compaire with initial data zzz = fa$correlation - fa$loadings %*% t(fa$loadings) - diag(fa$uniquenesses) round(zzz,2) # It&#39; a miricle, we can almost recover (predict) correlation using factors "],["principal-component-analysis.html", "Chapter 37 Principal Component Analysis", " Chapter 37 Principal Component Analysis "],["principal-component-analysis-1.html", "Chapter 38 Principal component analysis 38.1 Basic statistics 38.2 Basic linear algebra (matrices)", " Chapter 38 Principal component analysis 38.1 Basic statistics Standard deviation (SD) and Variance (\\(s^2\\) are measures of the spread of data in a data set. Standard deviation (SD): \\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (X_i - \\overline{X})^2}{(n-1)}}\\] Variance (\\(s^2, var(X)\\)): \\[s^2 = \\frac{\\sum_{i=1}^{n} (X_i - \\overline{X})^2}{(n-1)} = \\frac{\\sum_{i=1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})}{(n-1)}\\] Covariance (\\(cov(X,Y)\\)): \\[cov(X,Y) = \\frac{\\sum_{i=1}^{n} (X_i - \\overline{X})(Y_i - \\overline{Y})}{(n-1)}\\] Covariance matrix for a set of data with n dimensions: \\[C^{n \\times n} = (C_{i,j}, c_{i,j} = cov(Dim_{i}, Dim_{j})),\\] where \\(C^{n \\times n}\\) is a matrix with \\(n\\) rows and \\(n\\) columns, and \\(Dim_x\\) is the \\(x\\)th dimension. For n-dimentional data set, the matrix has n rows and columns and each entry in the matrix is the result of calculating the covariance between two separate dimensions. Eg. the entry on row 2, column 3, is the covariance value calculated between the 2nd dimension and the 3rd dimension. Example for 3 dimensional data set, using dimensions \\(x\\), \\(y\\) and \\(z\\): \\[ C = \\begin{pmatrix} cov(x,x) &amp; cov(x,y) &amp; cov(x,z)\\\\ cov(y,x) &amp; cov(y,y) &amp; cov(y,z)\\\\ cov(z,x) &amp; cov(z,y) &amp; cov(z,z) \\end{pmatrix} \\] Covariations of the main diagonal turn to variance: \\(cov(a,a) = var(a)\\) The matrix is symmetrical about the main diagonal since \\(cov(a,b) = cov(b,a)\\). 38.2 Basic linear algebra (matrices) Example of non-eigenvector: \\[ \\begin{pmatrix} 2 &amp; 3\\\\ 2 &amp; 1\\\\ \\end{pmatrix} \\times \\begin{pmatrix} 1\\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2\\cdot1+3\\cdot3\\\\ 2\\cdot1+1\\cdot3 \\end{pmatrix} = \\begin{pmatrix} 11\\\\ 5 \\end{pmatrix} \\] Eigenvector: \\[ \\begin{pmatrix} 2 &amp; 3\\\\ 2 &amp; 1\\\\ \\end{pmatrix} \\times \\begin{pmatrix} 3\\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 12\\\\ 8 \\end{pmatrix} = 4 \\times \\begin{pmatrix} 3\\\\ 2 \\end{pmatrix} \\] ### Principal Component Analysis # Source: https://www.datacamp.com/community/tutorials/pca-analysis-r # PCA allows to visualize datasets with many variables # PCA is a linear transformation which simplify many variables into a smaller number of &quot;Principal Components&quot;. ### Example 1 &quot;mtcars&quot; mtcars.pca &lt;- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE) summary(mtcars.pca) str(mtcars.pca) library(devtools) install_github(&quot;vqv/ggbiplot&quot;) library(ggbiplot) ggbiplot(mtcars.pca) ggbiplot(mtcars.pca, labels=rownames(mtcars)) ggbiplot(mtcars.pca,ellipse=TRUE,choices=c(3,4), labels=rownames(mtcars), groups=mtcars.country) ggbiplot(mtcars.pca,ellipse=TRUE,circle=TRUE, labels=rownames(mtcars), groups=mtcars.country) ggbiplot(mtcars.pca,ellipse=TRUE,obs.scale = 1, var.scale = 1, labels=rownames(mtcars), groups=mtcars.country) ggbiplot(mtcars.pca,ellipse=TRUE,obs.scale = 1, var.scale = 1,var.axes=FALSE, labels=rownames(mtcars), groups=mtcars.country) ### Example 2 &quot;USArrests&quot; ?USArrests # Violent Crime Rates by US State dim(USArrests) dimnames(USArrests) apply(USArrests,2,mean) # finding mean of all # big variance between samples, need to standartize pca.out&lt;-prcomp(USArrests,scale=TRUE) pca.out summary(pca.out) names(pca.out) biplot(pca.out,scale = 0, cex=0.65) # Principal component analysis states=row.names(USArrests) states names(USArrests) apply(USArrests, 2, mean) apply(USArrests, 2, var) pr.out=prcomp(USArrests, scale=TRUE) names(pr.out) pr.out$center pr.out$scale pr.out$rotation dim(pr.out$x) biplot(pr.out, scale=0) pr.out$rotation=-pr.out$rotation pr.out$x=-pr.out$x biplot(pr.out, scale=0) pr.out$sdev pr.var=pr.out$sdev^2 pr.var pve=pr.var/sum(pr.var) pve plot(pve, xlab=&quot;Principal Component&quot;, ylab=&quot;Proportion of Variance Explained&quot;, ylim=c(0,1),type=&#39;b&#39;) plot(cumsum(pve), xlab=&quot;Principal Component&quot;, ylab=&quot;Cumulative Proportion of Variance Explained&quot;, ylim=c(0,1),type=&#39;b&#39;) a=c(1,2,8,-3) cumsum(a) # –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 2 –∫ –≥–ª–∞–≤–µ 10: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –º–µ—Ç–æ–¥—É –ö —Å—Ä–µ–¥–Ω–∏—Ö set.seed(2) x=matrix(rnorm(50*2), ncol=2) x[1:25,1]=x[1:25,1]+3 x[1:25,2]=x[1:25,2]-4 km.out=kmeans(x,2,nstart=20) km.out$cluster plot(x, col=(km.out$cluster+1), main=&quot;K-Means Clustering Results with K=2&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, pch=20, cex=2) set.seed(4) km.out=kmeans(x,3,nstart=20) km.out plot(x, col=(km.out$cluster+1), main=&quot;K-Means Clustering Results with K=3&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, pch=20, cex=2) set.seed(3) km.out=kmeans(x,3,nstart=1) km.out$tot.withinss km.out=kmeans(x,3,nstart=20) km.out$tot.withinss # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è hc.complete=hclust(dist(x), method=&quot;complete&quot;) hc.average=hclust(dist(x), method=&quot;average&quot;) hc.single=hclust(dist(x), method=&quot;single&quot;) par(mfrow=c(1,3)) plot(hc.complete,main=&quot;Complete Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, cex=.9) plot(hc.average, main=&quot;Average Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, cex=.9) plot(hc.single, main=&quot;Single Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, cex=.9) cutree(hc.complete, 2) cutree(hc.average, 2) cutree(hc.single, 2) cutree(hc.single, 4) xsc=scale(x) plot(hclust(dist(xsc), method=&quot;complete&quot;), main=&quot;Hierarchical Clustering with Scaled Features&quot;) x=matrix(rnorm(30*3), ncol=3) dd=as.dist(1-cor(t(x))) plot(hclust(dd, method=&quot;complete&quot;), main=&quot;Complete Linkage with Correlation-Based Distance&quot;, xlab=&quot;&quot;, sub=&quot;&quot;) # –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3 –∫ –≥–ª–∞–≤–µ 10: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö NCI60 # –î–∞–Ω–Ω—ã–µ NCI60 library(ISLR) nci.labs=NCI60$labs nci.data=NCI60$data dim(nci.data) nci.labs[1:4] table(nci.labs) # PCA –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∫ –¥–∞–Ω–Ω—ã–º NCI60 pr.out=prcomp(nci.data, scale=TRUE) Cols=function(vec){ cols=rainbow(length(unique(vec))) return(cols[as.numeric(as.factor(vec))]) } par(mfrow=c(1,2)) plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,xlab=&quot;Z1&quot;,ylab=&quot;Z2&quot;) plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,xlab=&quot;Z1&quot;,ylab=&quot;Z3&quot;) summary(pr.out) plot(pr.out) pve=100*pr.out$sdev^2/sum(pr.out$sdev^2) par(mfrow=c(1,2)) plot(pve, type=&quot;o&quot;, ylab=&quot;PVE&quot;, xlab=&quot;Principal Component&quot;, col=&quot;blue&quot;) plot(cumsum(pve), type=&quot;o&quot;, ylab=&quot;Cumulative PVE&quot;, xlab=&quot;Principal Component&quot;, col=&quot;brown3&quot;) # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏–∑ –Ω–æ–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö NCI60 sd.data=scale(nci.data) par(mfrow=c(1,3)) data.dist=dist(sd.data) plot(hclust(data.dist), labels=nci.labs, main=&quot;Complete Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;,ylab=&quot;&quot;) plot(hclust(data.dist, method=&quot;average&quot;), labels=nci.labs, main=&quot;Average Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;,ylab=&quot;&quot;) plot(hclust(data.dist, method=&quot;single&quot;), labels=nci.labs, main=&quot;Single Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;,ylab=&quot;&quot;) hc.out=hclust(dist(sd.data)) hc.clusters=cutree(hc.out,4) table(hc.clusters,nci.labs) par(mfrow=c(1,1)) plot(hc.out, labels=nci.labs) abline(h=139, col=&quot;red&quot;) hc.out set.seed(2) km.out=kmeans(sd.data, 4, nstart=20) km.clusters=km.out$cluster table(km.clusters,hc.clusters) hc.out=hclust(dist(pr.out$x[,1:5])) plot(hc.out, labels=nci.labs, main=&quot;Hier. Clust. on First Five Score Vectors&quot;) table(cutree(hc.out,4), nci.labs) 38.2.1 t-SNE - Stochastic Neighbor Embedding library(Rtsne) # data loading train&lt;- read.csv(&quot;~/github/dar/012_t-SNE/DATA/train.csv&quot;) head(train)[1:20] dim(train) ## get labels as factors labels &lt;- train$label train$label&lt;-as.factor(train$label) ## for plotting colors = rainbow(length(unique(train$label))) names(colors) = unique(train$label) ## Executing the algorithm on curated data tsne &lt;- Rtsne(train[,-1], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500) ## Execution time exeTimeTsne&lt;- system.time(Rtsne(train[,-1], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)) ## Plotting plot(tsne$Y, t=&#39;n&#39;, main=&quot;tsne&quot;) text(tsne$Y, labels=train$label, col=colors[train$label]) Sources https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/ "],["learning-vector-quantization.html", "Chapter 39 Learning Vector Quantization", " Chapter 39 Learning Vector Quantization Learning Vector Quantiztion (LVQ) is a supervised classification algorithm for binary and multiclass problems. LVQ is a special case of a neural network. LVQ model creates codebook vectors by learning training dataset. Codebook vectors represent class regions. They contain elements that placed around the respective class according to their matching level. If the element matches, it comes closer to the target class, if it does not match, it moves farther from it. With this codebooks, the model classifies new data. Here is a nice explanation how it works. There are several versions of LVQ function: lvq1(), olvq1(), lvq2(), lvq3(), dlvq(). library(class) # olvq1() library(caret) # to split data # generate dataset df &lt;- iris id = caret::createDataPartition(df$Species, p = .8, list = F) train = df[id, ] test = df[-id, ] # initialize an LVQ codebook cb = class::lvqinit(train[1:4], train$Species) # training set in a codebook. build.cb = class::olvq1(train[1:4], train$Species, cb) # classify test set from LVQ Codebook for test data predict = class::lvqtest(build.cb, test[1:4]) # confusion matrix. caret::confusionMatrix(test$Species, predict) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 10 0 ## virginica 0 1 9 ## ## Overall Statistics ## ## Accuracy : 0.9667 ## 95% CI : (0.8278, 0.9992) ## No Information Rate : 0.3667 ## P-Value [Acc &gt; NIR] : 4.476e-12 ## ## Kappa : 0.95 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9091 1.0000 ## Specificity 1.0000 1.0000 0.9524 ## Pos Pred Value 1.0000 1.0000 0.9000 ## Neg Pred Value 1.0000 0.9500 1.0000 ## Prevalence 0.3333 0.3667 0.3000 ## Detection Rate 0.3333 0.3333 0.3000 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 0.9545 0.9762 "],["tree-based-models.html", "Chapter 40 Tree-based models 40.1 Classification Tree example 40.2 Regression Tree example", " Chapter 40 Tree-based models Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively. CART is both a generic term to describe tree algorithms and also a specific name for Breiman‚Äôs original algorithm for constructing classification and regression trees. Decision Tree: A tree-shaped graph or model of decisions used to determine a course of action or show a statistical probability. Classification Tree: A decision tree that performs classification (predicts a categorical response). Regression Tree: A decision tree that performs regression (predicts a numeric response). Split Point: A split point occurs at each node of the tree where a decision is made (e.g.¬†x &gt; 7 vs.¬†x ‚â§ 7). Terminal Node: A terminal node is a node which has no descendants (child nodes). Also called a ‚Äúleaf node.‚Äù Properties of Trees * Can handle huge datasets. * Can handle mixed predictors implicitly ‚Äì numeric and categorical. * Easily ignore redundant variables. * Handle missing data elegantly through surrogate splits. * Small trees are easy to interpret. * Large trees are hard to interpret. * Prediction performance is often poor (high variance). Tree Algorithms There are a handful of different tree algorithms in addition to Breiman‚Äôs original CART algorithm. Namely, ID3, C4.5 and C5.0, all created by Ross Quinlan. C5.0 is an improvement over C4.5, however, the C4.5 algorithm is still quite popular since the multi-threaded version of C5.0 is proprietary (although the single threaded is released as GPL). CART vs C4.5 Here are some of the differences between CART and C4.5: Tests in CART are always binary, but C4.5 allows two or more outcomes. CART uses the Gini diversity index to rank tests, whereas C4.5 uses information-based criteria. CART prunes trees using a cost-complexity model whose parameters are estimated by cross-validation; C4.5 uses a single-pass algorithm derived from binomial confidence limits. With respect to missing data, CART looks for surrogate tests that approximate the outcomes when the tested attribute has an unknown value, but C4.5 apportions the case probabilistically among the outcomes. Decision trees are formed by a collection of rules based on variables in the modeling data set: Rules based on variables‚Äô values are selected to get the best split to differentiate observations based on the dependent variable. Once a rule is selected and splits a node into two, the same process is applied to each ‚Äúchild‚Äù node (i.e.¬†it is a recursive procedure). Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.) Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules. 40.1 Classification Tree example Let‚Äôs use the data frame kyphosis to predict a type of deformation (kyphosis) after surgery, from age in months (Age), number of vertebrae involved (Number), and the highest vertebrae operated on (Start). # Classification Tree with rpart library(rpart) # grow tree fit &lt;- rpart(Kyphosis ~ Age + Number + Start, method=&quot;class&quot;, data=kyphosis) printcp(fit) # display the results ## ## Classification tree: ## rpart(formula = Kyphosis ~ Age + Number + Start, data = kyphosis, ## method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] Age Start ## ## Root node error: 17/81 = 0.20988 ## ## n= 81 ## ## CP nsplit rel error xerror xstd ## 1 0.176471 0 1.00000 1.0000 0.21559 ## 2 0.019608 1 0.82353 1.1176 0.22433 ## 3 0.010000 4 0.76471 1.1176 0.22433 plotcp(fit) # visualize cross-validation results summary(fit) # detailed summary of splits ## Call: ## rpart(formula = Kyphosis ~ Age + Number + Start, data = kyphosis, ## method = &quot;class&quot;) ## n= 81 ## ## CP nsplit rel error xerror xstd ## 1 0.17647059 0 1.0000000 1.000000 0.2155872 ## 2 0.01960784 1 0.8235294 1.117647 0.2243268 ## 3 0.01000000 4 0.7647059 1.117647 0.2243268 ## ## Variable importance ## Start Age Number ## 64 24 12 ## ## Node number 1: 81 observations, complexity param=0.1764706 ## predicted class=absent expected loss=0.2098765 P(node) =1 ## class counts: 64 17 ## probabilities: 0.790 0.210 ## left son=2 (62 obs) right son=3 (19 obs) ## Primary splits: ## Start &lt; 8.5 to the right, improve=6.762330, (0 missing) ## Number &lt; 5.5 to the left, improve=2.866795, (0 missing) ## Age &lt; 39.5 to the left, improve=2.250212, (0 missing) ## Surrogate splits: ## Number &lt; 6.5 to the left, agree=0.802, adj=0.158, (0 split) ## ## Node number 2: 62 observations, complexity param=0.01960784 ## predicted class=absent expected loss=0.09677419 P(node) =0.7654321 ## class counts: 56 6 ## probabilities: 0.903 0.097 ## left son=4 (29 obs) right son=5 (33 obs) ## Primary splits: ## Start &lt; 14.5 to the right, improve=1.0205280, (0 missing) ## Age &lt; 55 to the left, improve=0.6848635, (0 missing) ## Number &lt; 4.5 to the left, improve=0.2975332, (0 missing) ## Surrogate splits: ## Number &lt; 3.5 to the left, agree=0.645, adj=0.241, (0 split) ## Age &lt; 16 to the left, agree=0.597, adj=0.138, (0 split) ## ## Node number 3: 19 observations ## predicted class=present expected loss=0.4210526 P(node) =0.2345679 ## class counts: 8 11 ## probabilities: 0.421 0.579 ## ## Node number 4: 29 observations ## predicted class=absent expected loss=0 P(node) =0.3580247 ## class counts: 29 0 ## probabilities: 1.000 0.000 ## ## Node number 5: 33 observations, complexity param=0.01960784 ## predicted class=absent expected loss=0.1818182 P(node) =0.4074074 ## class counts: 27 6 ## probabilities: 0.818 0.182 ## left son=10 (12 obs) right son=11 (21 obs) ## Primary splits: ## Age &lt; 55 to the left, improve=1.2467530, (0 missing) ## Start &lt; 12.5 to the right, improve=0.2887701, (0 missing) ## Number &lt; 3.5 to the right, improve=0.1753247, (0 missing) ## Surrogate splits: ## Start &lt; 9.5 to the left, agree=0.758, adj=0.333, (0 split) ## Number &lt; 5.5 to the right, agree=0.697, adj=0.167, (0 split) ## ## Node number 10: 12 observations ## predicted class=absent expected loss=0 P(node) =0.1481481 ## class counts: 12 0 ## probabilities: 1.000 0.000 ## ## Node number 11: 21 observations, complexity param=0.01960784 ## predicted class=absent expected loss=0.2857143 P(node) =0.2592593 ## class counts: 15 6 ## probabilities: 0.714 0.286 ## left son=22 (14 obs) right son=23 (7 obs) ## Primary splits: ## Age &lt; 111 to the right, improve=1.71428600, (0 missing) ## Start &lt; 12.5 to the right, improve=0.79365080, (0 missing) ## Number &lt; 3.5 to the right, improve=0.07142857, (0 missing) ## ## Node number 22: 14 observations ## predicted class=absent expected loss=0.1428571 P(node) =0.1728395 ## class counts: 12 2 ## probabilities: 0.857 0.143 ## ## Node number 23: 7 observations ## predicted class=present expected loss=0.4285714 P(node) =0.08641975 ## class counts: 3 4 ## probabilities: 0.429 0.571 # plot tree plot(fit, uniform=TRUE, main=&quot;Classification Tree for Kyphosis&quot;) text(fit, use.n=TRUE, all=TRUE, cex=.8) # create attractive postscript plot of tree post(fit, title = &quot;Classification Tree for Kyphosis&quot;) # prune the tree pfit&lt;- prune(fit, cp=fit$cptable[which.min(fit$cptable[,&quot;xerror&quot;]),&quot;CP&quot;]) # plot the pruned tree #FIXME: pfit is not a tree just a root error #plot(pfit, uniform=TRUE, # main=&quot;Pruned Classification Tree for Kyphosis&quot;) #text(pfit, use.n=TRUE, all=TRUE, cex=.8) #post(pfit, file = &quot;c:/ptree.ps&quot;, # title = &quot;Pruned Classification Tree for Kyphosis&quot;) 40.2 Regression Tree example # Regression Tree Example library(rpart) # grow tree fit &lt;- rpart(Mileage~Price + Country + Reliability + Type, method=&quot;anova&quot;, data=cu.summary) printcp(fit) # display the results ## ## Regression tree: ## rpart(formula = Mileage ~ Price + Country + Reliability + Type, ## data = cu.summary, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] Price Type ## ## Root node error: 1354.6/60 = 22.576 ## ## n=60 (57 observations deleted due to missingness) ## ## CP nsplit rel error xerror xstd ## 1 0.622885 0 1.00000 1.03257 0.175924 ## 2 0.132061 1 0.37711 0.53199 0.105050 ## 3 0.025441 2 0.24505 0.36948 0.082559 ## 4 0.011604 3 0.21961 0.36433 0.077849 ## 5 0.010000 4 0.20801 0.37518 0.080670 plotcp(fit) # visualize cross-validation results summary(fit) # detailed summary of splits ## Call: ## rpart(formula = Mileage ~ Price + Country + Reliability + Type, ## data = cu.summary, method = &quot;anova&quot;) ## n=60 (57 observations deleted due to missingness) ## ## CP nsplit rel error xerror xstd ## 1 0.62288527 0 1.0000000 1.0325697 0.17592351 ## 2 0.13206061 1 0.3771147 0.5319912 0.10505027 ## 3 0.02544094 2 0.2450541 0.3694811 0.08255880 ## 4 0.01160389 3 0.2196132 0.3643252 0.07784854 ## 5 0.01000000 4 0.2080093 0.3751818 0.08066999 ## ## Variable importance ## Price Type Country ## 48 42 10 ## ## Node number 1: 60 observations, complexity param=0.6228853 ## mean=24.58333, MSE=22.57639 ## left son=2 (48 obs) right son=3 (12 obs) ## Primary splits: ## Price &lt; 9446.5 to the right, improve=0.6228853, (0 missing) ## Type splits as LLLRLL, improve=0.5044405, (0 missing) ## Reliability splits as LLLRR, improve=0.1263005, (11 missing) ## Country splits as --LRLRRRLL, improve=0.1243525, (0 missing) ## Surrogate splits: ## Type splits as LLLRLL, agree=0.950, adj=0.750, (0 split) ## Country splits as --LLLLRRLL, agree=0.833, adj=0.167, (0 split) ## ## Node number 2: 48 observations, complexity param=0.1320606 ## mean=22.70833, MSE=8.498264 ## left son=4 (23 obs) right son=5 (25 obs) ## Primary splits: ## Type splits as RLLRRL, improve=0.43853830, (0 missing) ## Price &lt; 12154.5 to the right, improve=0.25748500, (0 missing) ## Country splits as --RRLRL-LL, improve=0.13345700, (0 missing) ## Reliability splits as LLLRR, improve=0.01637086, (10 missing) ## Surrogate splits: ## Price &lt; 12215.5 to the right, agree=0.812, adj=0.609, (0 split) ## Country splits as --RRLRL-RL, agree=0.646, adj=0.261, (0 split) ## ## Node number 3: 12 observations ## mean=32.08333, MSE=8.576389 ## ## Node number 4: 23 observations, complexity param=0.02544094 ## mean=20.69565, MSE=2.907372 ## left son=8 (10 obs) right son=9 (13 obs) ## Primary splits: ## Type splits as -LR--L, improve=0.515359600, (0 missing) ## Price &lt; 14962 to the left, improve=0.131259400, (0 missing) ## Country splits as ----L-R--R, improve=0.007022107, (0 missing) ## Surrogate splits: ## Price &lt; 13572 to the right, agree=0.609, adj=0.1, (0 split) ## ## Node number 5: 25 observations, complexity param=0.01160389 ## mean=24.56, MSE=6.4864 ## left son=10 (14 obs) right son=11 (11 obs) ## Primary splits: ## Price &lt; 11484.5 to the right, improve=0.09693168, (0 missing) ## Reliability splits as LLRRR, improve=0.07767167, (4 missing) ## Type splits as L--RR-, improve=0.04209834, (0 missing) ## Country splits as --LRRR--LL, improve=0.02201687, (0 missing) ## Surrogate splits: ## Country splits as --LLLL--LR, agree=0.80, adj=0.545, (0 split) ## Type splits as L--RL-, agree=0.64, adj=0.182, (0 split) ## ## Node number 8: 10 observations ## mean=19.3, MSE=2.21 ## ## Node number 9: 13 observations ## mean=21.76923, MSE=0.7928994 ## ## Node number 10: 14 observations ## mean=23.85714, MSE=7.693878 ## ## Node number 11: 11 observations ## mean=25.45455, MSE=3.520661 # create additional plots par(mfrow=c(1,2)) # two plots on one page rsq.rpart(fit) # visualize cross-validation results ## ## Regression tree: ## rpart(formula = Mileage ~ Price + Country + Reliability + Type, ## data = cu.summary, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] Price Type ## ## Root node error: 1354.6/60 = 22.576 ## ## n=60 (57 observations deleted due to missingness) ## ## CP nsplit rel error xerror xstd ## 1 0.622885 0 1.00000 1.03257 0.175924 ## 2 0.132061 1 0.37711 0.53199 0.105050 ## 3 0.025441 2 0.24505 0.36948 0.082559 ## 4 0.011604 3 0.21961 0.36433 0.077849 ## 5 0.010000 4 0.20801 0.37518 0.080670 # plot tree plot(fit, uniform=TRUE, main=&quot;Regression Tree for Mileage &quot;) text(fit, use.n=TRUE, all=TRUE, cex=.8) # create attractive postcript plot of tree post(fit, file = &quot;tree2.ps&quot;, title = &quot;Regression Tree for Mileage &quot;) # prune the tree pfit&lt;- prune(fit, cp=0.01160389) # from cptable # plot the pruned tree plot(pfit, uniform=TRUE, main=&quot;Pruned Regression Tree for Mileage&quot;) text(pfit, use.n=TRUE, all=TRUE, cex=.8) post(pfit, file = &quot;ptree2.ps&quot;, title = &quot;Pruned Regression Tree for Mileage&quot;) Sources Tree-Based Models at Quick-R by datacamp UseR! Machine Learnign Turorial "],["random-forest.html", "Chapter 41 Random forest", " Chapter 41 Random forest data &lt;- iris[,-5] fit &lt;- randomForest(iris[,5] ~ ., data=data, ntree=200) fit require(randomForest) require(MASS) attach(Boston) head(Boston) ?Boston str(Boston) library(mlbench) data(PimaIndiansDiabetes) str(PimaIndiansDiabetes) df &lt;- PimaIndiansDiabetes inTrain = sample(nrow(df), nrow(df)*0.8) # split data train = df[inTrain, ] test = df[-inTrain, ] fit &lt;- randomForest(diabetes ~ . , data=train, ntree=100, proximity=TRUE) fit table(predict(fit), train$diabetes) plot(fit) importance(fit) varImpPlot(fit) pred &lt;- predict(fit, newdata=test) table(pred, test$diabetes) plot(margin(fit, test$Species)) # Tune Random Forest tune.rf &lt;- tuneRF(iris[,-5], iris[,5], stepFactor=.5) print(tune.rf) # Random Forest # Methods of improving classifiers: # 1. Stacking # 2. Bagging (bootstrap aggregation) # 3. Boosting # Random Forest = Bootstrap aggregation # N samples, M characteristics install.packages(&quot;randomForest&quot;) library(randomForest) # Read data zzz &lt;- read.table(&quot;~/DataAnalysis/R_data_analysis/DATA/Wine.txt&quot;, header=T, sep=&quot;&quot;, dec=&quot;.&quot;) zzz names(zzz) &lt;- c(&quot;Alcohol&quot;, &quot;Malic_acid&quot;, &quot;Ash&quot;, &quot;Alcalinity_of_ash&quot;, &quot;Magnesium&quot;, &quot;Total_phenols&quot;, &quot;Flavanoids&quot;, &quot;Nonflavanoid_phenols&quot;, &quot;Proanthocyanins&quot;, &quot;Color_intensity&quot;, &quot;Hue&quot;, &quot;OD280_OD315_of_diluted_wines&quot;, &quot;Proline&quot;, &quot;Wine_type&quot;) zzz # Predictors x &lt;- zzz[,1:13] # Responce y &lt;- zzz[,14] y.1 &lt;- as.factor(y) table(y) set.seed(123) ntree.1 &lt;- 500 # number of trees in the forest nodesize.1 &lt;- 1 # minimum size of terminal nodes keep.forest.1 &lt;- TRUE # results include forest rf.res &lt;- randomForest(x, y=y.1, ntree=ntree.1, mtry=floor(sqrt(ncol(zzz))), replace=FALSE, nodesize = nodesize.1, importance=TRUE, localImp=FALSE, proximity=FALSE, norm.votes=TRUE, do.trace=ntree.1/10, keep.forest=keep.forest.1, corr.bias=FALSE, keep.inbag=FALSE) # Detect number of trees zzz.predict &lt;- predict(rf.res, newdata = x) # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ –≤–∞—Ä–∏–∞–Ω—Ç # zzz.predict &lt;- predict(rf.res, newdata = x, type = &quot;prob&quot;) # –û—Ü–µ–Ω–∏–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ table(y, zzz.predict) # zzz.predict # y 0 1 2 # 0 59 0 0 # 1 0 48 0 # 2 0 0 71 # import.wine &lt;- importance(rf.res, type=NULL, class=1, scale=TRUE) # import.wine.2 &lt;- as.data.frame(import.wine) varImpPlot(rf.res, sort=F) varUsed(rf.res, by.tree=FALSE, count=TRUE) "],["gradient-boosted-trees.html", "Chapter 42 Gradient boosted trees", " Chapter 42 Gradient boosted trees "],["markov-chain-monte-carlo-mcmc.html", "Chapter 43 Markov Chain Monte Carlo (MCMC)", " Chapter 43 Markov Chain Monte Carlo (MCMC) We have a seq of conditional probabilities: We know conditional probabilities for the weather tomorrow (t+1) depending of the weather today (t). R for raily day and S for sunny day. \\(P(S_{t+1} | R_t) = 0.5\\) \\(P(R_{t+1} | R_t) = 0.5\\) \\(P(R_{t+1} | S_t) = 0.1\\) \\(P(S_{t+1} | S_t) = 0.9\\) What is the probability of sunny (S) or rainy (R) day? Simulate data using conditional probabilities: Markov Chain 1: S-S-R-R-S-S-S-S-R-R-R-S-S-S Markov Chain 2: R-S-S-S-S-S-R-S-S-S-S-S-R-R After simulation we calculate probabilities of sunny (S) and rainy (R) days: P(S) = 0.833 P(R) = 0.167 "],["simple-markov-process.html", "Chapter 44 Simple Markov process", " Chapter 44 Simple Markov process Here, we will consider a simple example of Markov process with implementation in R. The following example is taken from Bodo Winter website. A Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states. Let‚Äôs consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there‚Äôs a certain probability of an alert student becoming bored (say 0.2), and there‚Äôs a probability of a bored student becoming alert (say 0.25). Let‚Äôs say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point \\(t\\). Given the transition probabilities above, what‚Äôs the number of alert and bored students at the next point in time, \\(t+1\\)? Multiply 20 by 0.2 (=4) and these will be the alert students that turn bored. And then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert. So, at \\(t+1\\), there‚Äôs going to be 20-4+20 alert students. And there‚Äôs going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert. A handy way of representing this Markov process is by defining a transition probability matrix: A B A\\(_{t+1}\\) 0.8 0.25 B\\(_{t+1}\\) 0.2 0.75 What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point \\(t+1\\). And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way: \\[ \\begin{bmatrix} 0.8 &amp; 0.25 \\\\ 0.2 &amp; 0.75 \\end{bmatrix}\\times\\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix} = \\begin{bmatrix} 0.8\\times0.2 + 0.25\\times0.8 \\\\ 0.2\\times0.2 + 0.75\\times0.8 \\end{bmatrix} = \\begin{bmatrix} 0.36 \\\\ 0.64 \\end{bmatrix} \\] The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students. Now, you might ask yourself: What happens if this process continues? What happens at \\(t+2\\), \\(t+3\\) etc.? Will it be the case that at one point there are no bored students any more? Let‚Äôs simulate this in R and find out! Let‚Äôs call this tpm for transition probability matrix: tpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE) colnames(tpm) = c(&#39;A&#39;,&#39;B&#39;) rownames(tpm) = c(&#39;At+1&#39;, &#39;Bt+1&#39;) tpm ## A B ## At+1 0.8 0.25 ## Bt+1 0.2 0.75 Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at \\(t+1\\). And 0.25 students who were in state B at time point t will be in state A at \\(t+1\\). The second row has a similar interpretation for alert and bored students becoming bored at \\(t+1\\). Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we‚Äôll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions ‚Äì and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2). Let‚Äôs start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called sm (short for student matrix): sm = as.matrix(c(0.1, 0.9)) rownames(sm)= c(&#39;A&#39;, &#39;B&#39;) sm ## [,1] ## A 0.1 ## B 0.9 Now let‚Äôs repeat by looping: for(i in 1:10){ sm = tpm %*% sm } Here, we‚Äôre looping 10 times and on each iteration, we multiply the matrix tpm with the student matrix sm. We take this result and store it in sm. This means that at the next iteration, our fixed transition probability matrix will be multiplied by a different student matrix, allowing for the proportions to slowly change over time. R operator ‚Äô%*%‚Äô is used for matrix multiplication Outcome of our ten loop iterations: sm ## [,1] ## At+1 0.5544017 ## Bt+1 0.4455983 So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations. Let‚Äôs reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations. for(i in 1:1000){ sm = tpm %*% sm } sm ## [,1] ## At+1 0.5555556 ## Bt+1 0.4444444 A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called Markov convergence. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It‚Äôs not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we‚Äôre dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What‚Äôs so cool about this is that it is, at first sight, fairly counterintuitive. At least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that‚Äôs not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium ‚Äì albeit sometimes a little faster or slower. You can play around with different values for the sm object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert ‚Äì the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn‚Äôt matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity. 44.0.1 Sources Bodo Winter website "],["bayesian-inference.html", "Chapter 45 Bayesian inference 45.1 Simple model with one binary parameter 45.2 Grid approximation 45.3 Grid approximation 45.4 Model of birth weights using normal distribution 45.5 A Bayesian model of Zombie IQ 45.6 The BEST models", " Chapter 45 Bayesian inference Bayesian data analysis the use of Bayesian inference to learn from data. Bayesian inference is a method for figuring out unobservable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be. Bayesian inference is conditioning on data, in order to learn about parameter values. Bayesian inference is a method of statistical inference in which Bayes‚Äô theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayes‚Äô theorem \\[P( \\theta | D ) = frac{P( D | \\theta) \\pdot P( \\theta)}{\\sum{P( D | \\theta) \\pdot P( \\theta ) }}\\] \\(\\theta\\) - Parameter values \\(D\\) - New data \\(P(D | \\theta)\\) - Likelihood (the relative) probability of the data given different parameter value. \\(P( \\theta )\\) - Prior \\(\\sum{P( D | \\theta) \\pdot P( \\theta ) }\\) - The total sum of the likelihod weighted by the prior. A prior is a probability distribution that represents what the model knows before seeing the data. A posterior is a probability distribution that represents what the model knows after having seen the data. Bayesian inference components: Data Bayesian Model: Generative model (dbinom, dnorm, poisson etc.) Priors Computational methods: Rejection sampling Grid approximation Markov Chain Monte Carlo (MCMC) # Visualization function of Bayesian inference for binary data library(ggjoy) library(ggridges) library(ggExtra) library(tidyverse) # prop_model function to visualize bayesian posterior distributions prop_model &lt;- function(data = c(), prior_prop = c(1, 1), n_draws = 10000, show_plot = TRUE) { data &lt;- as.logical(data) proportion_success &lt;- c(0, seq(0, 1, length.out = 100), 1) data_indices &lt;- round(seq(0, length(data), length.out = min(length(data) + 1, 20))) post_curves &lt;- map_dfr(data_indices, function(i) { value &lt;- ifelse(i == 0, &quot;Prior&quot;, ifelse(data[i], &quot;Success&quot;, &quot;Failure&quot;)) label &lt;- paste0(&quot;n=&quot;, i) probability &lt;- dbeta(proportion_success, prior_prop[1] + sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)])) probability &lt;- probability/max(probability) tibble(value, label, proportion_success, probability) }) post_curves$label &lt;- fct_rev(factor(post_curves$label, levels = paste0(&quot;n=&quot;, data_indices))) post_curves$value &lt;- factor(post_curves$value, levels = c(&quot;Prior&quot;, &quot;Success&quot;, &quot;Failure&quot;)) p &lt;- ggplot(post_curves, aes(x = proportion_success, y = label, height = probability, fill = value)) + geom_joy(stat = &quot;identity&quot;, color = &quot;white&quot;, alpha = 1, panel_scaling = TRUE, size = 1) + scale_y_discrete(&quot;&quot;, expand = c(0.01, 0)) + scale_x_continuous(&quot;Underlying proportion of success&quot;) + scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = &quot;&quot;, drop = FALSE, labels = c(&quot;Prior &quot;, &quot;Success &quot;, &quot;Failure &quot;)) + theme_light(base_size = 18) + theme(legend.position = &quot;top&quot;) if (show_plot) { print(p) } invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data))) } 45.1 Simple model with one binary parameter # Generate 50 random binary data with P(1)=0.75 data &lt;- sample(c(1, 0), prob = c(0.75, 0.25), size = 10, replace = TRUE) # Visualize posteriors posterior &lt;- prop_model(data) # View posterior head(posterior) ## [1] 0.6885118 0.9954317 0.9157077 0.8484793 0.8414593 0.8479106 # Center of the posterior median(posterior) ## [1] 0.8518252 # The credible interval (CI) quantile(posterior, c(0.05, 0.95)) ## 5% 95% ## 0.6327662 0.9674745 # Probability of successes &gt; 0.3 sum(posterior &gt; 0.3) / length(posterior) ## [1] 1 Generate binomial data p &lt;- 0.42 # probability of success n &lt;- 100 # number of observations # Create logical vector, TRUE if numeric value from uniform distribution &lt; p data &lt;- c() for (i in 1:n) { data[i] &lt;- runif(1, min = 0, max = 1) &lt; p } # convert logical to numeric 0/1 data &lt;- as.numeric(data) data ## [1] 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 ## [45] 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 ## [89] 0 0 0 0 0 0 1 0 0 0 0 1 # the same vector using rbinom distribution n = 100 # number of observations size = 1 # 0 - fail, 1 - success, for 1 trial p = 0.42 # probability of success rbinom(n, size, p) ## [1] 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 ## [45] 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 ## [89] 1 1 0 0 1 0 0 0 0 1 1 0 45.2 Grid approximation To get more visitors to your website you are considering paying for an ad to be shown 100 times on a popular social media site. According to the social media site, their ads get clicked on 10% of the time. How many visitors could your site get? We can fix % of clicks as 0.1 but we do not know exactly how many clicks people do (uncertain). We suggest the % of clicks is uniformly distributed between 0 and 20. Define model: \\(n_{ads} = 100\\) \\(p_{clicks} ~ Uniform(0.0, 0.2)\\) \\(p_{visitors} ~ Binomial(n_{ads}, p_{clicks})\\) n &lt;- 100000 # number of simulations n_ads_shown &lt;- 100 # number of shown ads # probability that add will be clicked (assume uniform [0,0.2]) p_clicks &lt;- runif(n, min = 0.0, max = 0.2) # MODEL: We assume that the binomial distribution is a reasonable generative model n_visitors &lt;- rbinom(n = n, size = n_ads_shown, prob = p_clicks) # Visualize the prior model simulation library(ggplot2) df &lt;- data.frame(x = n_visitors, y = p_clicks) p &lt;- ggplot(df, aes(n_visitors, p_clicks)) + geom_point() + theme_classic() ggExtra::ggMarginal(p, type = &quot;histogram&quot;, bins = 18) Generate posterior distribution After the first day of showing ads we registered that 13 people clicked and visited site when the ad was shown 100 times. # Prior distribution binomial model with uniform clicks prior &lt;- data.frame(p_clicks, n_visitors) # To get posterior we subset from prior 13 visitors posterior &lt;- prior[prior$n_visitors == 13, ] head(posterior) ## p_clicks n_visitors ## 39 0.13318400 13 ## 57 0.15608873 13 ## 64 0.09112852 13 ## 75 0.11852016 13 ## 76 0.12653570 13 ## 106 0.10590670 13 # visualize posterior p &lt;- ggplot(posterior, aes(n_visitors, p_clicks)) + geom_point() + theme_classic() ggExtra::ggMarginal(p, type = &quot;histogram&quot;, bins = 18) For the next iteration we make prior from posterior and include new data for subsetting from prior to get posterior. # Assign posterior to prior for the next iteration prior &lt;- posterior # next iteration posterior ~ f(prior) n &lt;- nrow(prior) n_ads_shown &lt;- 100 prior$n_visitors &lt;- rbinom(n, size = n_ads_shown, prob = prior$p_clicks) # plot of prior distributions p &lt;- ggplot(prior, aes(n_visitors, p_clicks)) + geom_point() + theme_classic() ggExtra::ggMarginal(p, type = &quot;histogram&quot;, bins = 18) Use result of the model for prediction: # Calculate the probability that you will get 5 or more visitors. sum(prior$n_visitors &gt;= 5) / length(prior$n_visitors) ## [1] 0.9889606 # Median number of visitors median(prior$n_visitors) ## [1] 13 Updated model We will change prior distribution with knowledge we learn from people that the number of clicks are 5% and sometimes 2 or 8%. We will use beta distribution to setup number of clicks to update prior in our model. n &lt;- 100000 n_ads_shown &lt;- 100 # Change the prior on proportion_clicks p_clicks &lt;- rbeta(n, shape1 = 5, shape2 = 95) # Updated model n_visitors &lt;- rbinom(n = n, size = n_ads_shown, prob = p_clicks) prior &lt;- data.frame(p_clicks, n_visitors) posterior &lt;- prior[prior$n_visitors == 13, ] # Plots the prior and the posterior par(mfcol = c(1, 2)) hist(prior$p_clicks, xlim = c(0, 0.25)) hist(posterior$p_clicks, xlim = c(0, 0.25)) We tested for video banner and now we would like to compair distributions for: - video ads (13 out of 100 clicked) - text ads (6 out of a 100 clicked). n &lt;- 100000 n_ads_shown &lt;- 100 p_clicks &lt;- runif(n, min = 0.0, max = 0.2) n_visitors &lt;- rbinom(n, size = n_ads_shown, prob = p_clicks) prior &lt;- data.frame(p_clicks, n_visitors) # Create the posteriors for video and text ads posterior_video &lt;- prior[prior$n_visitors == 13, ] posterior_text &lt;- prior[prior$n_visitors == 6, ] par(mfrow=c(1,2)) # Visualize the posteriors hist(posterior_video$p_clicks, xlim = c(0, 0.25)) hist(posterior_text$p_clicks, xlim = c(0, 0.25)) # combine data into dataframe # Make sizes of distrib. the same (4000) to fit to dataframe posterior &lt;- data.frame(video_prop = posterior_video$p_clicks[1:4000], text_prop = posterior_text$p_clicks[1:4000]) # Calculate the posterior difference: video_prop - text_prop posterior$prop_diff &lt;- posterior$video_prop - posterior$text_prop # Calculate the median of prop_diff median(posterior$prop_diff) ## [1] 0.06535207 # Calculate the proportion mean(posterior$prop_diff &gt; 0.0) ## [1] 0.94825 # Visualize prop_diff hist(posterior$prop_diff) Decision analysis Our analysis indicated that the video ads are clicked more often. We would like to estimate probable profit video ads vs text ads if we know: Each visitor spends $2.53 on average on our website. Video ad costs us $0.25 per click. Text ad costs us $0.05 per click. visitor_spend &lt;- 2.53 video_cost &lt;- 0.25 text_cost &lt;- 0.05 # Add the column posterior$video_profit posterior$video_profit &lt;- posterior$video_prop * visitor_spend - video_cost # Add the column posterior$text_profit posterior$text_profit &lt;- posterior$text_prop * visitor_spend - text_cost head(posterior) ## video_prop text_prop prop_diff video_profit text_profit ## 1 0.1591797 0.04933518 0.10984449 0.152724569 0.07481801 ## 2 0.1639834 0.13350068 0.03048271 0.164877969 0.28775671 ## 3 0.1342420 0.04359866 0.09064333 0.089632257 0.06030462 ## 4 0.1007833 0.03186180 0.06892146 0.004981632 0.03061034 ## 5 0.1434389 0.12331007 0.02012884 0.112900432 0.26197447 ## 6 0.1953154 0.14542950 0.04988593 0.244148032 0.31793663 # Visualize the video_profit and text_profit columns par(mfrow=c(1,2)) hist(posterior$video_profit) hist(posterior$text_profit) # Difference between video and text ad profits posterior$profit_diff &lt;- posterior$video_profit - posterior$text_profit # Visualize posterior$profit_diff hist(posterior$profit_diff) # Calculate a &quot;best guess&quot; for the difference in profits median(posterior$profit_diff) ## [1] -0.03465928 # Calculate the probability that text ads are better than video ads mean(posterior$profit_diff &lt; 0) ## [1] 0.63525 Our analysis showed that text ads will bring probably more profit but the result is too uncertain and we need more data for making the correct decision. Another model for another case We would like to put up an ad on the site and pay for it per day. The site admin promise that we will get 19 clicks per day. How many daily clicks should we expect on average? We are going to use Poisson distribution for our new model which correspond to number of successes in a period of time. n &lt;- 100000 mean_clicks &lt;- runif(n, min = 0, max = 80) # uniform dist between 0 and 80 clicks # model n_visitors &lt;- rpois(n = n, mean_clicks) prior &lt;- data.frame(mean_clicks, n_visitors) # first day trial showed 13 clicks posterior &lt;- prior[prior$n_visitors == 13, ] # visualize prior and posterior par(mfrow=c(1,2)) hist(prior$mean_clicks) hist(posterior$mean_clicks) 45.3 Grid approximation The following example shows how we can build the previous model more effective. We calculating the distribution using dbinom instead of simulating using rbinom. We can directly include condition in our model using dbinom instead of rbinom. Instead of using resul of our trial 13 visitors after showing ads 100 times we can condition for all posible number of visitors seq(0,100). n_ads_shown &lt;- 100 p_clicks &lt;- 0.1 n_visitors &lt;- seq(0, 100) # instead of 13 we get for all possible n visitors # model prob &lt;- dbinom(n_visitors, size = n_ads_shown, prob = p_clicks) head(prob) ## [1] 0.0000265614 0.0002951267 0.0016231966 0.0058916025 0.0158745955 0.0338658038 # Plot the distribution plot(n_visitors, prob, type = &quot;h&quot;) Calculating a joint distribution n_ads_shown &lt;- 100 p_clicks &lt;- seq(0, 1, by = 0.01) n_visitors &lt;- seq(0, 100, by = 1) # Define a grid over all the parameter combinations you need to evaluate pars &lt;- expand.grid(proportion_clicks = p_clicks, n_visitors = n_visitors) pars$prior &lt;- dunif(pars$proportion_clicks, min = 0, max = 0.2) pars$likelihood &lt;- dbinom(pars$n_visitors, size = n_ads_shown, prob = pars$proportion_clicks) ### According to Bayes&#39; theorem: # Combined probability by the rule of probabilities multiplication pars$probability &lt;- pars$likelihood * pars$prior # normalize to the total number to get sum of all probabilities eq 1 pars$probability &lt;- pars$probability / sum(pars$probability) ### # Conditioning on the data for n_visitors == 6 pars &lt;- pars[pars$n_visitors == 6, ] # Normalize again to get sum of all probabilities eq 1 pars$probability &lt;- pars$probability / sum(pars$probability) # Plot the posterior pars$probability plot(pars$proportion_clicks, pars$probability, type = &quot;h&quot;) We can directly condition and change n_visitors &lt;- 6. In this case we do not need to subset by pars[pars$n_visitors == 6, ]. Result will be the same. 45.4 Model of birth weights using normal distribution Let‚Äôs assume that the Normal distribution is a decent model of birth weight data. # Assign mu and sigma m &lt;- 3500 # central value (mean weight) s &lt;- 600 # deviation weight_dist &lt;- rnorm(n = 100000, mean = m, sd = s) hist(weight_dist, 60, xlim = c(0, 6000)) We calculating the distribution using dnorm instead of simulating using rnorm. # Create weight weight &lt;- seq(0, 6000, by = 100) # 100 g increment # Calculate likelihood likelihood &lt;- dnorm(weight, m, s) # Plot the distribution of weight plot(weight, likelihood, type = &quot;h&quot;) Here is a small data set with the birth weights of six newborn babies in grams. c(3164, 3362, 4435, 3542, 3578, 4529) Mark: What to do with the data? Should we condition it to get posterior? 45.5 A Bayesian model of Zombie IQ Check video temp &lt;- c(19, 23, 20, 17, 23) mu &lt;- seq(8, 30, by = 0.5) sigma &lt;- seq(0.1, 10, by = 0.3) pars &lt;- expand.grid(mu = mu, sigma = sigma) pars$mu_prior &lt;- dnorm(pars$mu, mean = 18, sd = 5) pars$sigma_prior &lt;- dunif(pars$sigma, min = 0, max = 10) pars$prior &lt;- pars$mu_prior * pars$sigma_prior for (i in 1:nrow(pars)) { likelihoods &lt;- dnorm(temp, pars$mu[i], pars$sigma[i]) pars$likelihood[i] &lt;- prod(likelihoods) } pars$probability &lt;- pars$likelihood * pars$prior pars$probability &lt;- pars$probability / sum(pars$probability) sample_indices &lt;- sample(1:nrow(pars), size = 10000, replace = TRUE, prob = pars$probability) head(sample_indices) ## [1] 524 705 478 749 298 296 pars_sample &lt;- pars[sample_indices, c(&quot;mu&quot;, &quot;sigma&quot;)] head(pars_sample) ## mu sigma ## 524 22.0 3.4 ## 705 22.5 4.6 ## 478 21.5 3.1 ## 749 22.0 4.9 ## 298 21.5 1.9 ## 296 20.5 1.9 hist(pars_sample$mu, 30) quantile(pars_sample$mu, c(0.05, 0.95)) ## 5% 95% ## 17.5 22.5 pred_temp &lt;- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma) hist(pred_temp, 30) # Probability of 18C sum(pred_temp &gt;= 18) / length(pred_temp) ## [1] 0.7315 for the model of temperature but use zombies IQs instead. Define model: \\(\\mu = Normal(mean:18, sd:5)\\) \\(\\sigma = Uniform(min:0, max:10)\\) \\(temp = 19,23,...\\) How much we can learn about the mean zombie IQ from this data. We need to calculate the probability of each parameter combination in pars. Use Bayes Theorem to calculate these probabilities and assign them to pars$probability to complete the model. # The IQ of a bunch of zombies iq &lt;- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46) # Defining the parameter grid pars &lt;- expand.grid(mu = seq(0, 150, length.out = 100), sigma = seq(0.1, 50, length.out = 100)) # Defining and calculating the prior density for each parameter combination pars$mu_prior &lt;- dnorm(pars$mu, mean = 100, sd = 100) pars$sigma_prior &lt;- dunif(pars$sigma, min = 0.1, max = 50) pars$prior &lt;- pars$mu_prior * pars$sigma_prior # Calculating the likelihood for each parameter combination for(i in 1:nrow(pars)) { likelihoods &lt;- dnorm(iq, pars$mu[i], pars$sigma[i]) pars$likelihood[i] &lt;- prod(likelihoods) } # Calculate the probability of each parameter combination pars$probability &lt;- pars$likelihood * pars$prior / sum(pars$likelihood * pars$prior) Calculate new parameters. head(pars) ## mu sigma mu_prior sigma_prior prior likelihood probability ## 1 0.000000 0.1 0.002419707 0.02004008 4.849113e-05 0 0 ## 2 1.515152 0.1 0.002456367 0.02004008 4.922578e-05 0 0 ## 3 3.030303 0.1 0.002493009 0.02004008 4.996010e-05 0 0 ## 4 4.545455 0.1 0.002529617 0.02004008 5.069373e-05 0 0 ## 5 6.060606 0.1 0.002566174 0.02004008 5.142633e-05 0 0 ## 6 7.575758 0.1 0.002602661 0.02004008 5.215754e-05 0 0 sample_indices &lt;- sample(nrow(pars), size = 10000, replace = TRUE, prob = pars$probability) head(sample_indices) ## [1] 2429 2729 2631 3333 2732 2030 # Sample from pars to calculate some new measures pars_sample &lt;- pars[sample_indices, c(&quot;mu&quot;, &quot;sigma&quot;)] # Calculate quantiles quantile(pars_sample$mu, c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## 34.84848 42.42424 50.00000 head(pars_sample) ## mu sigma ## 2429 42.42424 12.19697 ## 2729 42.42424 13.70909 ## 2631 45.45455 13.20505 ## 3333 48.48485 16.73333 ## 2732 46.96970 13.70909 ## 2030 43.93939 10.18081 pred_iq &lt;- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma) # Calculate the probability that zombi has IQ &gt; 60 sum(pred_iq &gt;= 60) / length(pred_iq) ## [1] 0.0837 par(mfrow=c(1,2)) # Visualize the mean IQ hist(pars_sample$mu, 100) # Visualize pred_iq hist(pred_iq) 45.6 The BEST models The t-test is a classical statistical procedure used to compare the means of two data sets. In 2013 John Kruschke developed a souped-up Bayesian version of the t-test he named BEST (standing for Bayesian Estimation Supersedes the t-test). We would like to compair IQ of two groups of 10 patiens on different diets (a and b). # The IQ of patients. iq_a &lt;- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46) iq_b &lt;- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51) # Calculate the mean difference in IQ between the two groups mean(iq_b) - mean(iq_a) ## [1] 9.3 require(BEST) # Fit the BEST model to the data from both groups require(BEST) best_posterior &lt;- BEST::BESTmcmc(iq_b, iq_a) # Plot the model result plot(best_posterior) The Bayesian model behind BEST assumes that the generative model for the data is a t-distribution; a more flexible distribution than the normal distribution as it assumes that data points might be outliers to some degree. This makes BEST‚Äôs estimate of the mean difference robust to outliers in the data. Sources Fundamentals of Bayesian Data Analysis in R on Datacamp "],["naive-bayes-classifiers.html", "Chapter 46 Naive Bayes classifiers", " Chapter 46 Naive Bayes classifiers Naive Bayes classifiers are a family of simple ‚Äúprobabilistic classifiers‚Äù based on applying Bayes‚Äô theorem with strong (na√Øve) independence assumptions between the features. They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve higher accuracy levels. Na√Øve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. \\(P(c|x) = \\frac{P(x|c)(P(c))}{P(x)}\\), where \\(P(c|x)\\) - posteriour probability \\(P(x|c)\\) - Likelihood \\(P(c)\\) - Class Prior Probbility \\(P(x)\\) - Predictor Prior Probability "],["modeling-with-r-caret.html", "Chapter 47 Modeling with R caret", " Chapter 47 Modeling with R caret library(caret) library(&#39;ISLR&#39;) df &lt;- Default head(df) # Check for parameters of model to test modelLookup(&#39;c5.0&#39;) # Test parameters of models set.seed(123) m &lt;- caret::train(default ~ ., data = df, method = &quot;C5.0&quot;) p &lt;- predict(m, df) table(p, df$default) head(predict(m, df, type = &quot;prob&quot;)) # Resampling methods ?trainControl() ## TUNING PARAMETERS OF THE MODEL # example ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;) # grid to test several parameters grid &lt;- expand.grid(.model = &quot;tree&quot;, .trials = c(1, 5, 10, 15, 20, 25, 30, 35), .winnow = &quot;FALSE&quot;) set.seed(300) m &lt;- train(default ~ ., data = credit, method = &quot;C5.0&quot;, metric = &quot;Kappa&quot;, trControl = ctrl, tuneGrid = grid) PCA The predictors should be centered and scaled before applying this transformation. # See available algorithms in caret modelnames &lt;- paste(names(getModelInfo()), collapse=&#39;, &#39;) modelnames modelLookup(algo) "],["modeling-with-r-tensorflow.html", "Chapter 48 Modeling with R Tensorflow", " Chapter 48 Modeling with R Tensorflow install.packages(&#39;tensorflow&#39;) install.packages(&#39;keras&#39;) normalize &lt;- function(x) { num &lt;- x - min(x) denom &lt;- max(x) - min(x) return (num/denom) iris &lt;- as.data.frame(lapply(iris[1:4], normalize) # Determine sample size ind &lt;- sample(2, nrow(....), replace=TRUE, prob=c(0.67, 0.33)) # Split the `iris` data iris.training &lt;- ....[ind==1, 1:4] iris.test &lt;- iris[ind==2, 1:4] # Split the class attribute iris.trainingtarget &lt;- iris[...==1, 5] iris.testtarget &lt;- iris[ind==2, 5] # One hot encode training target values iris.trainLabels &lt;- to_categorical(..................) # One hot encode test target values iris.testLabels &lt;- to_categorical(...............) # Print out the iris.testLabels to double check the result print(.............) # Initialize a sequential model model &lt;- ...................... # Add layers to the model model %&gt;% layer_dense(units = 8, activation = &#39;relu&#39;, input_shape = c(4)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) # Print a summary of a model summary(model) # Get model configuration get_config(model) # Get layer configuration get_layer(model, index = 1) # List the model&#39;s layers model$layers # List the input tensors model$inputs # List the output tensors model$outputs # Compile the model model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39; ) # Fit the model model %&gt;% fit( iris.training, iris.trainLabels, epochs = 200, batch_size = 5, validation_split = 0.2 ) # Store the fitting history in `history` history &lt;- model %&gt;% fit( iris.training, iris.trainLabels, epochs = 200, batch_size = 5, validation_split = 0.2 ) # Plot the history plot(history) # Plot the accuracy of the training data plot(history$metrics$acc, main=&quot;Model Accuracy&quot;, xlab = &quot;epoch&quot;, ylab=&quot;accuracy&quot;, col=&quot;blue&quot;, type=&quot;l&quot;) # Plot the accuracy of the validation data lines(history$metrics$val_acc, col=&quot;green&quot;) # Add Legend legend(&quot;bottomright&quot;, c(&quot;train&quot;,&quot;test&quot;), col=c(&quot;blue&quot;, &quot; # Evaluate on test data and labels score &lt;- model %&gt;% evaluate(iris.test, iris.testLabels, batch_size = 128) # Print the score print(score) # Initialize a sequential model model &lt;- keras_model_sequential() # Build up your model by adding layers to it model %&gt;% layer_dense(units = 8, activation = &#39;relu&#39;, input_shape = c(4)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) # Define an optimizer sgd &lt;- optimizer_sgd(lr = 0.01) # Use the optimizer to compile the model model %&gt;% compile(optimizer=sgd, loss=&#39;categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) # Fit the model to the training data model %&gt;% fit( iris.training, iris.trainLabels, epochs = 200, batch_size = 5, validation_split = 0.2 ) # Evaluate the model score &lt;- model %&gt;% evaluate(iris.test, iris.testLabels, batch_size = 128) # Print the loss and accuracy metrics print(score) # Define an optimizer sgd &lt;- optimizer_sgd(lr = 0.01) # Compile the model model %&gt;% compile(optimizer=sgd, loss=&#39;categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) # Fit the model to the training data history &lt;- model %&gt;% fit( iris.training, iris.trainLabels, epochs = 200, batch_size = 5, validation_split = 0.2 ) # Plot the model loss plot(history$metrics$loss, main=&quot;Model Loss&quot;, xlab = &quot;epoch&quot;, ylab=&quot;loss&quot;, col=&quot;blue&quot;, type=&quot;l&quot;) lines(history$metrics$val_loss, col=&quot;green&quot;) legend(&quot;topright&quot;, c(&quot;train&quot;,&quot;test&quot;), col=c(&quot;blue&quot;, &quot;green&quot;), lty=c(1,1)) # Plot the model accuracy plot(history$metrics$acc, main=&quot;Model Accuracy&quot;, xlab = &quot;epoch&quot;, ylab=&quot;accuracy&quot;, col=&quot;blue&quot;, type=&quot;l&quot;) lines(history$metrics$val_acc, col=&quot;green&quot;) legend(&quot;bottomright&quot;, c(&quot;train&quot;,&quot;test&quot;), col=c(&quot;blue&quot;, &quot;green&quot;), lty=c(1,1)) "],["perceptron.html", "Chapter 49 Perceptron", " Chapter 49 Perceptron x1 &lt;- runif(30,-1,1) x2 &lt;- runif(30,-1,1) #form the input vector x x &lt;- cbind(x1,x2) #helper function to calculate distance from hyperplane calculate_distance = function(x,w,b) { sum(x*w) + b } #linear classifier linear_classifier = function(x,w,b) { distances =apply(x, 1, calculate_distance, w, b) return(ifelse(distances &lt; 0, -1, +1)) } #function to calculate 2nd norm second_norm = function(x) {sqrt(sum(x * x))} #perceptron training algorithm perceptron = function(x, y, learning_rate=1) { w = vector(length = ncol(x)) # initialize w b = 0 # Initialize b k = 0 # count iterations #constant with value greater than distance of furthest point R = max(apply(x, 1, second_norm)) incorrect = TRUE # flag to identify classifier #initialize plot plot(x,cex=0.2) #loop till correct classifier is not found while (incorrect ) { incorrect = FALSE #classify with current weights yc &lt;- linear_classifier(x,w,b) #Loop over each point in the input x for (i in 1:nrow(x)) { #update weights if point not classified correctly if (y[i] != yc[i]) { w &lt;- w + learning_rate * y[i]*x[i,] b &lt;- b + learning_rate * y[i]*R^2 k &lt;- k+1 #currect classifier&#39;s components # update plot after ever 5 iterations if(k%%5 == 0){ intercept &lt;- - b / w[[2]] slope &lt;- - w[[1]] / w[[2]] #plot the classifier hyper plane abline(intercept,slope,col=&quot;red&quot;) #wait for user input cat (&quot;Iteration # &quot;,k,&quot;\\n&quot;) cat (&quot;Press [enter] to continue&quot;) line &lt;- readline() } incorrect = TRUE } } } s = second_norm(w) #scale the classifier with unit vector return(list(w=w/s,b=b/s,updates=k)) } #train the perceptron p &lt;- perceptron(x,Y) #classify based on calculated y &lt;- linear_classifier(x,p$w,p$b) plot(x,cex=0.2) #zoom into points near the separator and color code them #marking data points as + which have y=1 and ‚Äì for others points(subset(x,Y==1),col=&quot;black&quot;,pch=&quot;+&quot;,cex=2) points(subset(x,Y==-1),col=&quot;red&quot;,pch=&quot;-&quot;,cex=2) # compute intercept on y axis of separator # from w and b intercept &lt;- - p$b / p$w[[2]] # compute slope of separator from w slope &lt;- - p$w[[1]] /p$ w[[2]] # draw separating boundary abline(intercept,slope,col=&quot;green&quot;) "],["deeplearning-r-h2o.html", "Chapter 50 Deeplearning R H2O", " Chapter 50 Deeplearning R H2O library(h2o) cl &lt;- h2o.init(max_mem_size = &quot;32G&quot;, nthreads = 16) d &lt;- as.h2o(droplevels(iris)) d h2o.levels(d) d &lt;- h2o.importFile(path = &quot;http://www.web/file.csv&quot;) d &lt;- h2o.importFile(path = &quot;file.csv&quot;) ## NNET package # Neural Network digits.train &lt;- read.csv(&quot;~/kaggle/train.csv&quot;) dim(digits.train) head(colnames(digits.train), 4) tail(colnames(digits.train), 4) head(digits.train[, 1:4]) ## convert to factor digits.train$label &lt;- factor(digits.train$label, levels = 0:9) i &lt;- 1:5000 digits.X &lt;- digits.train[i, -1] digits.y &lt;- digits.train[i, 1] # digits distibution barplot(table(digits.y)) set.seed(1234) digits.m1 &lt;- train(x = digits.X, y = digits.y, method = &quot;nnet&quot;, tuneGrid = expand.grid( .size = c(5), .decay = 0.1), trControl = trainControl(method = &quot;none&quot;), MaxNWts = 10000, maxit = 100) digits.yhat1 &lt;- predict(digits.m1) barplot(table(digits.yhat1)) caret::confusionMatrix(xtabs(~digits.yhat1 + digits.y)) # 10 hidden neurons set.seed(1234) digits.m2 &lt;- train(digits.X, digits.y, method = &quot;nnet&quot;, tuneGrid = expand.grid( .size = c(10), .decay = 0.1), trControl = trainControl(method = &quot;none&quot;), MaxNWts = 50000, maxit = 100) digits.yhat2 &lt;- predict(digits.m2) barplot(table(digits.yhat2)) caret::confusionMatrix(xtabs(~digits.yhat2 + digits.y)) # Increasing from 5 to 10 hidden neurons improved our in-sample performance from an overall accuracy of 44.3% to 57.6% # 40 hidden neurons set.seed(1234) digits.m3 &lt;- train(digits.X, digits.y, method = &quot;nnet&quot;, tuneGrid = expand.grid( .size = c(40), .decay = 0.1), trControl = trainControl(method = &quot;none&quot;), MaxNWts = 50000, maxit = 100) digits.yhat3 &lt;- predict(digits.m3) barplot(table(digits.yhat3)) caret::confusionMatrix(xtabs(~digits.yhat3 + digits.y)) Confusion Matrix and Statistics # Stuttgart Neural Network Simulator (SNNS) set.seed(1234) digits.m4 &lt;- mlp(as.matrix(digits.X), decodeClassLabels(digits.y), size = 40, learnFunc = &quot;Rprop&quot;, shufflePatterns = FALSE, maxit = 60) digits.yhat4 &lt;- fitted.values(digits.m4) digits.yhat4 &lt;- encodeClassLabels(digits.yhat4) barplot(table(digits.yhat4)) caret::confusionMatrix(xtabs(~ I(digits.yhat4 - 1) + digits.y)) "]]
