[["index.html", "R statistics Chapter 1 Introduction", " R statistics Mark Goldberg 2021-04-16 Chapter 1 Introduction This is the collection of statistical methods in R. "],["statistics-r-functions-reference.html", "Chapter 2 Statistics R functions reference 2.1 Get data 2.2 Data inspection 2.3 Plots 2.4 Analysis of the distribution 2.5 t-Test 2.6 ANOVA", " Chapter 2 Statistics R functions reference 2.1 Get data 2.2 Data inspection head(df) typeof(df) # data type dim(df) # dimention nrow(df) # number of rows ncol(df) # number of columns str(df) # data structure summary(df) # data summary names(df) # names of columns colnames(df) # also column names 2.3 Plots plot(x ~ y) barplot(df) hist(x) pie(groupsize, labels, col, ...) 2.4 Analysis of the distribution # mode getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } getmode(v) # mode mean(v) # mean mean(v, trim=0.1) # trimmed mean median(v) # median min(v) max(v) range(v) # max(v) - min(v) sort(v) rank(v) sample(v) sample(v, size) var(v) # variance sd(v) # standard deviation cor(v) # correlation cov(v) # covariation scale(v) # z-scores quantile(v) IQR(v) # interquantile range: IQR = Q3 – Q1 qqnorm(v) # normal probability plot qqline(v) # adds a line to a normal probability plot passing through 1Q and 3Q 2.5 t-Test t.test(x, mu = 0, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95) t.test(v, mu) # one-sample t-test, mu - null hypothesized value t.test(v1, v2) # two-sample t-test t.test(v1, v2, var.equal=T) t.test(var1, var2, paired=T) wilcox.test(v1, v2, paired=T) 2.6 ANOVA # One way ANOVA oneway.test(x ~ f) aov(x ~ f) anova(m1, m2) # compair two models "],["basic-statistics.html", "Chapter 3 Basic Statistics 3.1 Definitions 3.2 Primary analysis 3.3 Outliers 3.4 Normality 3.5 Confidence interval", " Chapter 3 Basic Statistics 3.1 Definitions population - all existing samples sample - subset of statistical population simple random sample - random subset stratified sample - fist clustering, than random sample from cluster sample - random choosing from several existing clusters variables - discret, continuous, ordinal (ранговая) 3.2 Primary analysis # sample of random integers v &lt;- round(rnorm(n=50, sd=5, mean=100)) par(mfrow=c(1,2)) stripchart(v, method = &quot;stack&quot;, pch=19, cex=2, offset=.5, at=.15, main = &quot;Dotplot of random value&quot;, xlab = &quot;Random value&quot;) hist(v) # add density x &lt;- density(v)$x y &lt;- (10/max(density(v)$y))*density(v)$y # scale y to plot with histogram lines(x, y, col=&quot;red&quot;, lwd=2) # mode function getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } getmode(v) # mode mean(v) # mean mean(v, trim=0.1) # trimmed mean median(v) # median min(v) max(v) range(v) max(v)-min(v) # range quantile(v) boxplot(v) # median, 50% quantile and center of boxplot is the same value. 3.3 Outliers Outliers are rare values that appear far away from the majority of the data. Outliers can bias the results and potentially lead to incorrect conclusions if not handled properly. One method for dealing with outliers is to simply remove them. However, removing data points can introduce other types of bias into the results, and potentially result in losing critical information. If outliers seem to have a lot of influence on the results, a nonparametric test such as the Wilcoxon Signed Rank Test may be appropriate to use instead. Outliers can be identified visually using a boxplot. 3.4 Normality It is possible to use histogram to estimate normality of the distribution. # QQ-plot - fit normal distibution qqnorm(v); qqline(v) var(v) # variance: sd = sqrt(var) ## [1] 20.76898 sd(v) # standard deviation ## [1] 4.5573 sd(v)/sqrt(length(v)) # standard error sd/sqrt(n) ## [1] 0.6444995 # Z-score (standartization) # transform distribution to mean=0, variance=1 # z = (x - mean(n))/sd scale(v) # z-score ## [,1] ## [1,] -0.64073031 ## [2,] 1.55355158 ## [3,] 1.11469520 ## [4,] -1.73787126 ## [5,] -1.29901488 ## [6,] 0.01755426 ## [7,] -1.07958669 ## [8,] 0.23698244 ## [9,] -1.51844307 ## [10,] 0.23698244 ## [11,] 0.45641063 ## [12,] 1.55355158 ## [13,] 1.11469520 ## [14,] -1.73787126 ## [15,] 0.23698244 ## [16,] 0.01755426 ## [17,] 0.01755426 ## [18,] -0.86015850 ## [19,] -0.42130212 ## [20,] 0.23698244 ## [21,] 0.67583882 ## [22,] 0.23698244 ## [23,] 1.33412339 ## [24,] -0.42130212 ## [25,] -0.86015850 ## [26,] 0.23698244 ## [27,] -0.86015850 ## [28,] 0.45641063 ## [29,] 0.89526701 ## [30,] 2.21183615 ## [31,] -2.17672764 ## [32,] -0.86015850 ## [33,] 0.01755426 ## [34,] 0.45641063 ## [35,] -0.42130212 ## [36,] 0.45641063 ## [37,] -0.42130212 ## [38,] 0.45641063 ## [39,] -0.86015850 ## [40,] 1.33412339 ## [41,] -1.07958669 ## [42,] -0.42130212 ## [43,] 1.99240796 ## [44,] -0.42130212 ## [45,] 0.89526701 ## [46,] 0.45641063 ## [47,] 0.01755426 ## [48,] 1.11469520 ## [49,] -0.64073031 ## [50,] -1.29901488 ## attr(,&quot;scaled:center&quot;) ## [1] 101.92 ## attr(,&quot;scaled:scale&quot;) ## [1] 4.5573 vs &lt;- scale(v)[,1] vs ## [1] -0.64073031 1.55355158 1.11469520 -1.73787126 -1.29901488 0.01755426 -1.07958669 0.23698244 -1.51844307 0.23698244 0.45641063 1.55355158 ## [13] 1.11469520 -1.73787126 0.23698244 0.01755426 0.01755426 -0.86015850 -0.42130212 0.23698244 0.67583882 0.23698244 1.33412339 -0.42130212 ## [25] -0.86015850 0.23698244 -0.86015850 0.45641063 0.89526701 2.21183615 -2.17672764 -0.86015850 0.01755426 0.45641063 -0.42130212 0.45641063 ## [37] -0.42130212 0.45641063 -0.86015850 1.33412339 -1.07958669 -0.42130212 1.99240796 -0.42130212 0.89526701 0.45641063 0.01755426 1.11469520 ## [49] -0.64073031 -1.29901488 par(mfrow=c(1,2)) hist(v) hist(vs) 3.5 Confidence interval # sample of random integers x &lt;- round(rnorm(n=50, sd=5, mean=100)) # Confidence interval for normal distribution with p=0.95 m &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(0.95)*s/sqrt(n) confidence &lt;- c(m-error, m+error) confidence ## [1] 98.42104 100.85896 # Confidence interval for t-distribution with p=0.95 a &lt;- 5 s &lt;- 2 n &lt;- 20 error &lt;- qt(0.975,df=n-1)*s/sqrt(n) # confidence interval c(left=a-error, right=a+error) ## left right ## 4.063971 5.936029 "],["primary-analysis-1.html", "Chapter 4 Primary analysis", " Chapter 4 Primary analysis Abbakumov, 2016, lectures # GET DATA df &lt;- read.table(&quot;./DATA/Swiss_Bank_Notes.csv&quot;, header=T, sep=&quot; &quot;, dec=&quot;,&quot;) head(df) # Data explanation: parameters of Swiss Banknotes # Size of data: 200 (100 are real, 100 are false) # Length - length # H_l - height left # H_r - height right # dist_l - border left # dist_up - border up # Diag - diagonal # ? Find false banknotes # 1. Let&#39;s add a column with 100 filled 0 and 100 filled with 1. origin &lt;- 0 df &lt;- data.frame(df, origin) df$origin[1:100] &lt;-1 # Set origin as factor - binary data (0,1) df$origin &lt;- as.factor(df$origin) is.factor(df$origin) # 2. Visual inspection of given data head(df) typeof(df) # data type dim(df) # dimention nrow(df) ncol(df) str(df) summary(df) names(df) # names of columns colnames(df) # also names of columns # 2. HISTORGRAM par(mfrow=c(length(colnames(df))/2,2)) for (i in 1:length(colnames(df))) { hist(df[,i], main = paste(colnames(df)[i])) } # Histogram for Diagonals par(mfrow=c(1,1)) hist(df$Diag, breaks=18, probability=TRUE) # Barplot barplot(VADeaths, beside=TRUE, legend=TRUE, ylim=c(0, 100), ylab=&quot;Deaths per 1000&quot;, main=&quot;Death rates in Virginia&quot;) # Pieplot groupsize &lt;- c(18, 30, 32, 10, 10) labels &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;F&quot;) pie(groupsize, labels, col=c(&quot;red&quot;, &quot;white&quot;, &quot;grey&quot;, &quot;black&quot;, &quot;blue&quot;)) # All pairs of data scatter plot plot(df) # Length ~ Dial plot(df$Length, df$Diag) # true notes points(df$Length[df$origin==1], df$Length[df$origin==0], pch=3, col=&quot;green&quot;) # false notes points(df$Length[df$origin==1], df$Length[df$origin==0], pch=1, col=&quot;red&quot;) # If factors are given, plot makes boxplot plot(df$origin, df$Diag) title(&quot;Swiss Bank Notes&quot;) # GET DATA - TOWNS town &lt;- read.table(&quot;DATA/town_1959_2.csv&quot;, header=T, sep=&quot;\\t&quot;, dec=&quot;.&quot;) town summary(town) # Median is more stable to outliers summary(town[,3]) # lets remove 2 first towns from the data summary(town[3:1004,3]) hist(town[,3]) # log scale allows us to see outliers better hist(log(town[,3]), breaks=50) # Truncated mean is better than mean mean(town[,3], trim=0.05) # GET DATA - BASKETBALL bb &lt;- read.table(&quot;DATA/basketball.csv&quot;, header=F, sep=&quot;;&quot;, dec=&quot;.&quot;) bb # NBA Player characteristics: # percent of positives vs: # SF - light forvard # PF - heavy forvard # C - center # G - defender summary(bb[,1]) par(mfrow=c(1,1)) plot(bb[,1]~bb[,2]) par(mfrow=c(2,2)) for (i in 1:4) { hist(bb[,1], breaks=5*i, main=paste(&quot;Breaks&quot;, 5*i), ylab=&quot;&quot;) } for (i in unique(bb[,2])) { hist(bb[bb[,2]==i ,1], breaks=6, xlim=c(min(bb[,1])-5, max(bb[,1]+5)), col=&quot;white&quot;, main=i, ylab=&quot;&quot;) } # Conclusion: for several groups of data boxplots may be more informative than histograms "],["statistical-distributions.html", "Chapter 5 Statistical distributions 5.1 Normal Distribution 5.2 Bernoulli Distribution 5.3 Binomial Distribution 5.4 Geometric Distribution 5.5 Uniform Distributions 5.6 Poisson Distribution 5.7 Exponential Distribution", " Chapter 5 Statistical distributions 5.1 Normal Distribution x &lt;- seq(-6, 6, length=100) y1 &lt;- dnorm(x, sd=1) y2 &lt;- dnorm(x, sd=0.5) y3 &lt;- dnorm(x, sd=2) plot(x, y1, xlim=c(-6,6), ylim=c(0,0.8), type=&quot;l&quot;, lwd=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Normal distribution&quot;) lines(x, y2, col=&quot;red&quot;) lines(x, y3, col=&quot;green&quot;) legend(&quot;topright&quot;, legend=c(&quot;sd = 1&quot;, &quot;sd = 0.5&quot;, &quot;sd = 2&quot;), col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;), lty=c(1,1,1), lwd=2) # Area under the curve # For normal distribution auc = 1 (probability for all) library(DescTools) ## Registered S3 method overwritten by &#39;DescTools&#39;: ## method from ## reorder.factor gplots ## ## Attaching package: &#39;DescTools&#39; ## The following object is masked from &#39;package:gplots&#39;: ## ## reorder.factor AUC(x, y1) ## [1] 1 AUC(x,y2) ## [1] 1 AUC(x,y3) # to broad distribution, some samples out of the area ## [1] 0.9972921 5.2 Bernoulli Distribution 5.3 Binomial Distribution 5.4 Geometric Distribution 5.5 Uniform Distributions 5.6 Poisson Distribution 5.7 Exponential Distribution Sources Seven Must-Know Statistical Distributions and Their Simulations for Data Science "],["hypothesis-testing.html", "Chapter 6 Hypothesis testing 6.1 Hypothesis testing theory 6.2 Hypothesis test (Practice)", " Chapter 6 Hypothesis testing 6.1 Hypothesis testing theory Null hypothesis (H0): 1. H0: m = μ 2. H0: m \\(\\leq\\) μ 3. H0: m \\(\\geq\\) μ Alternative hypotheses (Ha): 1. Ha:m ≠ μ (different) 2. Ha:m &gt; μ (greater) 3. Ha:m &lt; μ (less) Note: Hypothesis 1. are called two-tailed tests and hypotheses 2. &amp; 3. are called one-tailed tests. The p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true. Note: p-value is not the probability that the null hypothesis is true. Note: Absence of evidence ⧧ evidence of absence. Cutoffs for hypothesis testing *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected. not rejected (‘negative’) rejected (‘positive’) H0 true True negative (specificity) False Positive (Type I error) H0 false False Negative (Type II error) True positive (sensitivity) Type II errors are usually more dangerous. 6.2 Hypothesis test (Practice) Check distribution (is normal?): shapiro.test() Distribution uniformity (are both normal?) Dependence of variable (are x and y dependent?) Difference of distribution parameters (are means different?) ###. 1 Is distribution normal? town &lt;- read.table(&quot;~/DataAnalysis/DATA/town_1959_2.csv&quot;, header=T, sep=&quot;\\t&quot;, dec=&quot;.&quot;) town # histogram hist(town[,3]) # log scale hist(log(town[,3]), breaks=50) # ? Is log scaled number of sitizens per town is normally distributed? # test for normal distribution of dataset data &lt;- log(town[,3]) shapiro.test(data) # H0: data is normally distributed # W = 0.97467 - # p-value = 3.15e-12 -&gt; p &lt; alpha (0.01) - H0 is incorrect # Our distripution is not normal # Different tests for normality in package &quot;nortest&quot; install.packages(&quot;nortest&quot;) library(nortest) ad.test(data) # Anderson-Darling test lillie.test(data) # Lilliefors (Kolmogorov-Smirnov) test # Emperical rule: if n &lt; 2000 -&gt; shapiro.test, else -&gt; lillie.test # In theory it is possible to use method for normally distributed data, # even if they are not normal, but almost normal: # 1. No outlayers # 2. Symmetry of histogram # 3. Bell-shaped form of histogram # Method to make normal from not-normal distribution # 1. Remove outlayers # 2. Transform data to get symmetry: log, Boxcox # 3. Bimodality: split samples into groups # Boxcox transformation install.packages(&quot;AID&quot;) library(AID) bctr = boxcoxnc(data) bctr$results ### 2. Compair centers of two distributions # Emperical rule: if notmal -&gt; mean, else -&gt; median # if median -&gt; Mann–Whitney-Wilcoxon (MWW) test # if mean -&gt; one of Student&#39;s tests # Student t-test # t.test(x,y, alternative=&quot;two.sided&quot;, paired=FALSE, var.equal=FALSE) # How to compair if dispersions are the same? # Fligner-Killeen test # Brown-Forsythe test #fligner.test(x~g, data=data.table) # Example x &lt;- read.table (&quot;~/DataAnalysis/R_data_analysis/DATA/Albuquerque Home Prices_data.txt&quot;, header=T) names(x) summary(x) # Change -9999 to NA x$AGE[x$AGE==-9999] &lt;- NA x$TAX[x$TAX==-9999] &lt;- NA fligner.test(PRICE~COR, data=x) # Dispersion is the same for both distribution # p &gt; alpha (0.01) t.test(x$PRICE[x$COR==1], x$PRICE[x$COR==0], alternative=&quot;less&quot;, paired=FALSE, var.equal=TRUE) # p-value = 0.1977 (H0 is true) # Mood&#39;s median test names(x) x1 &lt;- x[x$NE==1,1] x2 &lt;- x[x$NE==0,1] m &lt;- median(c(x1,x2)) f11 &lt;- sum(x1&gt;m) f12 &lt;- sum(x2&gt;m) f21 &lt;- sum(x1&lt;=m) f22 &lt;- sum(x2&lt;=m) table &lt;- matrix(c(f11,f12, f21, f22), nrow=2,ncol=2) chisq.test(table) # p=0.47 &gt; alpha=0.05: H0 is true # medians are the same ## Mann–Whitney-Wilcoxon (MWW) test # Check if medians are the same # U1 = n1*n2+{n1*(n1+1)/2 - T1} # U1 = n1*n2+{n2*(n2+1)/2 - T2} # U=min(U1,U2) # Ti - сумма рангов в объединенной выборке наблюдений из выборки i # n1, n2 - размеры выборок wilcox.test(x,y, alternative=&quot;two.sided&quot;, paired=FALSE, exact=TRUE, correct=FALSE) ###. 3. Dependency of variables # H0: x, y independent # Correlation analysis # Correlation (pearson) mesure linear dependency! # Correlation k depends on outlayers (must be removed) plot(x$SQFT, x$TAX) cor(x$SQFT, x$TAX, use = &quot;complete.obs&quot;) cor(x$SQFT, x$TAX, use = &quot;complete.obs&quot;, method = &quot;spearman&quot;) cor(x, use = &quot;complete.obs&quot;, method = &quot;spearman&quot;) cor(x, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) cor.test(x$SQFT, x$TAX, use = &quot;complete.obs&quot;) "],["t-procedures.html", "Chapter 7 t-Procedures 7.1 t-test and normal distribution 7.2 One-sample t-test 7.3 Practical example: t-test in R 7.4 Two samples t-test 7.5 Compare Student’s t and normal distributions 7.6 Non-parametric tests 7.7 Mann-Whitney U Rank Sum Test 7.8 Wilcoxon test", " Chapter 7 t-Procedures 7.1 t-test and normal distribution t-distribution assumes that the observations are independent and that they follow a normal distribution. If the data are dependent, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors? It is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed. Independence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement. FIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use Wilcoxon test or permutation tests. 7.2 One-sample t-test One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ). t-statistics: \\(t = \\frac{m - \\mu}{s/\\sqrt{n}}\\), where m is the sample mean n is the sample size s is the sample standard deviation with n−1 degrees of freedom μ is the theoretical value Q: And what should I do with this t-statistics? Q: What is the difference between t-test and ANOVA? Q: What is the smallest sample size which can be tested by t-test? Q: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests. 7.3 Practical example: t-test in R We want to test if N is different from given mean μ=0: N = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25) t.test(N, mu = 0, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.9931 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf 0.964019 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.01383 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.1552496 1.0487504 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.006916 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 0.239981 Inf ## sample estimates: ## mean of x ## 0.602 FIXME: why it accepts all alternatives at the same time (less and greater?) 7.4 Two samples t-test Do two different samples have the same mean? H0: 1. H0: m1 - m2 = 0 2. H0: m1 - m2 \\(\\leq\\) 0 3. H0: m1 - m2 \\(\\geq\\) 0 Ha: 1. Ha: m1 - m2 ≠ 0 (different) 2. Ha: m1 - m2 &gt; 0 (greater) 3. Ha: m1 - m2 &lt; 0 (less) The paired sample t-test has four main assumptions: The dependent variable must be continuous (interval/ratio). The observations are independent of one another. The dependent variable should be approximately normally distributed. The dependent variable should not contain any outliers. Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with Likert-type scales. t-statistics: \\(t=\\frac{y - x}{SE}\\), where y and x are the samples means. SE is the standard error for the difference. If H0 is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample). To apply t-test samples must be tested if they have equal variance: equal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic). 7.5 Compare Student’s t and normal distributions x &lt;- seq(-4, 4, length=100) hx &lt;- dnorm(x) degf &lt;- c(1, 3, 8, 30) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;, &quot;gold&quot;, &quot;black&quot;) labels &lt;- c(&quot;df=1&quot;, &quot;df=3&quot;, &quot;df=8&quot;, &quot;df=30&quot;, &quot;normal&quot;) plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Comparison of t Distributions&quot;) for (i in 1:4){ lines(x, dt(x,degf[i]), lwd=2, col=colors[i]) } legend(&quot;topright&quot;, inset=.05, title=&quot;Distributions&quot;, labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors) To generate data with known mean and sd: rnorm2 &lt;- function(n,mean,sd) { mean+sd*scale(rnorm(n)) } r &lt;- rnorm2(100,4,1) ### t-test a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179) b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180) # test homogeneity of variances using Fisher’s F-test var.test(a,b) ## ## F test to compare two variances ## ## data: a and b ## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.5223017 8.4657950 ## sample estimates: ## ratio of variances ## 2.102784 # variance is homogene (can use var.equal=T in t.test) # t-test t.test(a,b, var.equal=TRUE, # variance is homogene (tested by var.test(a,b)) paired=FALSE) # samples are independent ## ## Two Sample t-test ## ## data: a and b ## t = -0.94737, df = 18, p-value = 0.356 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.93994 4.13994 ## sample estimates: ## mean of x mean of y ## 174.8 178.2 7.6 Non-parametric tests 7.7 Mann-Whitney U Rank Sum Test The dependent variable is ordinal or continuous. The data consist of a randomly selected sample of independent observations from two independent groups. The dependent variables for the two independent groups share a similar shape. 7.8 Wilcoxon test The Wilcoxon is a non-parametric test which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions. "],["tests-for-categorical-variables.html", "Chapter 8 Tests for categorical variables 8.1 Chi-squared tests", " Chapter 8 Tests for categorical variables Categorical variable can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. 8.1 Chi-squared tests The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher’s exact test can be used. data = rbind(c(83,35), c(92,43)) data ## [,1] [,2] ## [1,] 83 35 ## [2,] 92 43 chisq.test(data, correct=F) ## ## Pearson&#39;s Chi-squared test ## ## data: data ## X-squared = 0.14172, df = 1, p-value = 0.7066 chisq.test(testor,correct=F) ## Fisher’s Exact test R Example: Group TumourShrinkage-No TumourShrinkage-Yes Total 1 Treatment 8 3 11 2 Placebo 9 4 13 3 Total 17 7 24 The null hypothesis is that there is no association between treatment and tumour shrinkage. The alternative hypothesis is that there is some association between treatment group and tumour shrinkage. data = rbind(c(8,3), c(9,4)) data ## [,1] [,2] ## [1,] 8 3 ## [2,] 9 4 fisher.test(data) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: data ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.1456912 10.6433317 ## sample estimates: ## odds ratio ## 1.176844 The output Fisher’s exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is no evidence of an association between treatment group and tumour shrinkage. "],["multiple-testing.html", "Chapter 9 Multiple testing 9.1 The Bonferroni correction", " Chapter 9 Multiple testing When performing a large number of tests, the type I error is inflated: for α=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x … (n-times) &lt;&lt;&lt; 0.095 The larger the number of tests performed, the higher the probability of a false rejection! Many data analysis approaches in genomics rely on itemby-item (i.e. multiple) testing: Microarray or RNA-Seq expression profiles of “normal” vs “perturbed” samples: gene-by-gene ChIP-chip: locus-by-locus RNAi and chemical compound screens Genome-wide association studies: marker-by-marker QTL analysis: marker-by-marker and trait-by-trait False positive rate (FPR) - the proportion of false positives among all resulst. False discovery rate (FDR) - the proportion of false positives among all significant results. Example: 20,000 genes, 100 hits, 10 of them wrong. FPR: 0.05% FDR: 10% 9.1 The Bonferroni correction The Bonferroni correction sets the significance cut-off at α/n.  "],["sources.html", "Chapter 10 Sources 10.1 t-test", " Chapter 10 Sources One-Sample T-test in R 10.1 t-test The data set shows energy expend in two groups of women: stature library(ISwR) data(energy) attach(energy) head(energy) ## expend stature ## 1 9.21 obese ## 2 7.53 lean ## 3 7.48 lean ## 4 8.08 lean ## 5 8.09 lean ## 6 10.15 lean tapply(expend, stature, mean) ## lean obese ## 8.066154 10.297778 H0: there is no difference in averages between lean and obese. t.test(expend ~ stature) ## ## Welch Two Sample t-test ## ## data: expend by stature ## t = -3.8555, df = 15.919, p-value = 0.001411 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.459167 -1.004081 ## sample estimates: ## mean in group lean mean in group obese ## 8.066154 10.297778 Alternative hypothesis is true - means are different. Mean difference is in between -3.5 and 1.0 with a probability 95%. The risk of error is 0.15% 10.1.1 Two-tailed test Compair two sets of variables. data(intake) # from package ISwR attach(intake) head(intake) ## pre post ## 1 5260 3910 ## 2 5470 4220 ## 3 5640 3885 ## 4 6180 5160 ## 5 6390 5645 ## 6 6515 4680 mean(post - pre) ## [1] -1320.455 Is difference of means significant? t.test(pre, post, paired=TRUE) ## ## Paired t-test ## ## data: pre and post ## t = 11.941, df = 10, p-value = 3.059e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1074.072 1566.838 ## sample estimates: ## mean of the differences ## 1320.455 The difference is significant with a probability 95%. The difference is in between 1074.1 and 1566.8 kJ/day "],["analysis-of-variance-anova.html", "Chapter 11 Analysis of Variance (ANOVA) 11.1 One-way ANOVA 11.2 Sources", " Chapter 11 Analysis of Variance (ANOVA) 11.1 One-way ANOVA variance = SS/df, where SS - sum of squares and df - degree of freedom \\(SS = \\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}\\), where \\(\\mu\\) is the sample mean n is the sample size \\(var(x) = \\frac{1}{n}{\\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}}\\) SST = SSE + SSC = W + B, where SST - Total Sum of Squares SSE - Error Sum of Squares - within (W) SSC - Sum of Squares Columns (treatmens) - between (B) C - columns (treatments) N - total number of observations Mean squared of columns - MSC = SSC/df_columns, where df_columns = C-1 Mean squared of error - MSE = SSE/df_error, where df_error = N-C Sum of squares (total) - SST, where df_total = N-1 F-statistics - F = MSC/MSE Let’s calculate degree of freedom for our example: df_columns = 3-1 = 2, MSC = SSC/2 df_error = 21-3 = 18, MSE = SSE/18 df_total = 21-1 = 20 # 3 groups of students with scores (1-100): a = c(82,93,61,74,69,70,53) b = c(71,62,85,94,78,66,71) c = c(64,73,87,91,56,78,87) sq = function(x) { sum((x - mean(x))^2) } sq(a) ## [1] 1039.429 sq(b) ## [1] 751.4286 sq(c) ## [1] 1021.714 Using R packages: # data # Number of calories consumed by month: may &lt;- c(2166, 1568, 2233, 1882, 2019) sep &lt;- c(2279, 2075, 2131, 2009, 1793) dec &lt;- c(2226, 2154, 2583, 2010, 2190) d &lt;- stack(list(may=may, sep=sep, dec=dec)) d ## values ind ## 1 2166 may ## 2 1568 may ## 3 2233 may ## 4 1882 may ## 5 2019 may ## 6 2279 sep ## 7 2075 sep ## 8 2131 sep ## 9 2009 sep ## 10 1793 sep ## 11 2226 dec ## 12 2154 dec ## 13 2583 dec ## 14 2010 dec ## 15 2190 dec names(d) ## [1] &quot;values&quot; &quot;ind&quot; oneway.test(values ~ ind, data=d, var.equal=TRUE) ## ## One-way analysis of means ## ## data: values and ind ## F = 1.7862, num df = 2, denom df = 12, p-value = 0.2094 # alternative using aov res &lt;- aov(values ~ ind, data = d) res ## Call: ## aov(formula = values ~ ind, data = d) ## ## Terms: ## ind Residuals ## Sum of Squares 174664.1 586719.6 ## Deg. of Freedom 2 12 ## ## Residual standard error: 221.1183 ## Estimated effects may be unbalanced summary(res) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ind 2 174664 87332 1.786 0.209 ## Residuals 12 586720 48893 11.2 Sources Example for one-way ANOVA: youtube by Brandon Foltz "],["correlation.html", "Chapter 12 Correlation", " Chapter 12 Correlation df &lt;- mtcars # correlation miles per galon of petrol ~ horse power cor.test(x = df$mpg, y = df$hp) # get parameters of analysed correlation fit &lt;- cor.test(x = df$mpg, y = df$hp) str(fit) fit$p.value # the same correlation in &#39;formula&#39; form (see. ?cor.test) ?cor.test cor.test(~mpg+hp, df) # Plot using generic plot function of ggplot from ggplot2 package plot(x = df$mpg, y = df$hp) library(ggplot2) # add color by number of engine&#39;s cylinders ggplot(df, aes(x = mpg, y = hp, col = factor(cyl)))+geom_point(size=5) # Subset of necessary data from mtcars df.sub &lt;- df[,c(1,3:7)] # Scatterplots for all pairs of variables pairs(df.sub) # Correlations of all pairs of variables cor(df.sub) # Correlation using corr.test function from &#39;psych&#39; package # Correlation for all pairs of variables library(psych) fit.sub &lt;- corr.test(df.sub) fit.sub$r # correlations fit.sub$p # dependecies of variables "],["t-test-anova-difference.html", "Chapter 13 t-test ANOVA difference", " Chapter 13 t-test ANOVA difference The t-test and ANOVA examine whether group means differ from one another. The t-test compares two groups, while ANOVA can do more than two groups. The t-test ANOVA have three assumptions: independence assumption (the elements of one sample are not related to those of the other sample), normality assumption (samples are randomly drawn from the normally distributed populstions with unknown population means; otherwise the means are no longer best measures of central tendency, thus test will not be valid), and equal variance assumption (the population variances of the two groups are equal) ANCOVA (analysis of covariance) includes covariates, interval independent variables, in the right-hand side to control their impacts. MANOVA (multivariate analysis of variance) has more than one left-hand side variable. t-test and ANOVA usage. "],["clustering.html", "Chapter 14 Clustering 14.1 Next part 14.2 Example 14.3 NEXT PART", " Chapter 14 Clustering 5 classes of clustering methods: 1. Partitioning methods - split into k-groups (k-means, k-dedoids (PAM), CLARA) 2. Hierarchical clustering 3. Fuzzy clustering 4. Density-based clustering 5. Model-based clustering bv &lt;- read.table(&quot;./DATA/beverage.csv&quot;, header=T, sep=&quot;;&quot;) head(bv) # no needs to normalize because all data is binary (0,1) # Hierarchical clustering # dist - calculate distances # hclust - hierarchical clustering clust.bv &lt;- hclust(dist(bv[,2:9]), &quot;ward.D&quot;) clust.bv # Plot clusters plot(clust.bv) plot(clust.bv, hang = -1) rect.hclust(clust.bv, k=3, border=&quot;red&quot;) # Group data by clusters groups &lt;- cutree(clust.bv, k=3) groups # Percentage in broups by drinking different beverages colMeans(bv[groups==1, 2:9])*100 colMeans(bv[groups==2, 2:9])*100 colMeans(bv[groups==3, 2:9])*100 # Interpretation # 1. People who does not have specific preference # 2. People who prefers cola and pepsi # 3. Not clear (others) # atributes of cluster analysis names(clust.bv) # chronic of combining clust.bv$merge clust.bv[1] clust.bv$height clust.bv$order clust.bv$labels clust.bv$method clust.bv$call clust.bv$dist.method # Detect the best choice for number of cluster by elbow-plot plot(1:33, clust.bv$height, type=&quot;l&quot;) ### Task. Analyse data and find groups of people # Scores (0,10) of 10 tests for candidates to get a job. # 1. Memorizing numbers # 2. Math task # 3. Solving tasks in dialoge # 4. Algorithms # 5. Self confidence # 6. Work in group # 7. Find solution # 8. Collaboration # 9. Acceptance by others setwd(&quot;~/DataAnalysis&quot;) job &lt;- read.table(&quot;DATA/assess.dat&quot;, header=T, sep=&quot;\\t&quot;) job # Clustering clust.job &lt;- hclust(dist(job[,3:ncol(job)]), &quot;ward.D&quot;) # no needs to normalize, because all numbers have the same min, max plot(clust.job) # visual number of clusters is 4 # Group data by clusters groups &lt;- cutree(clust.job, k=4) groups colMeans(job[groups==1, 3:12])*100 ### Find clusters using k-means method setwd(&quot;~/DataAnalysis&quot;) bv &lt;- read.table(&quot;DATA/beverage.csv&quot;, header=T, sep=&quot;;&quot;) bv dim(bv) names(bv) # k-means clustering, with initial 3 clusters # nstart = x - run x times with different initial clusters summ.1 = kmeans(bv[,2:9], 3, iter.max = 100) names(summ.1) # Objects by clusters summ.1$cluster # Centers of clusters summ.1$centers # 2 digits after point options(digits=2) t(summ.1$centers) options(digits=7) # Square summs summ.1$withinss # Summ of elements of vector summ.1$tot.withinss # sum(33*(apply(bv[,2:9], 2, sd))^2) summ.1$totss summ.1$tot.betweenss # Size of clusters summ.1$size # Elbow plot to detect optimal number of clusters wss &lt;- (nrow(bv[,2:9])-1)*sum(apply(bv[,2:9],2,var)) for (i in 2:15) { wss[i] &lt;- kmeans(bv[,2:9], centers=i)$tot.withinss } plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;) # We can see that diagram is rough. This is because clusters are not allways optimal # To improve situation, we have to run many initiall start coordinates and choose the best # option (add nstart=500): wss &lt;- (nrow(bv[,2:9])-1)*sum(apply(bv[,2:9],2,var)) for (i in 2:15) { wss[i] &lt;- kmeans(bv[,2:9], centers=i, nstart=500)$tot.withinss } plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;) # Warnings means that iterations were not finished for some cases. # Let&#39;s compair results for 3 and 4 clusters summ.1 = kmeans(bv[,2:9], 3, iter.max=100) summ.2 = kmeans(bv[,2:9], 4, iter.max=100) # Compair clusters. How many elements in each cluster # We can see how elements move if we take more clusters table(summ.1$cluster, summ.2$cluster) # Multidimentional scaling # Project multidimentional data to 2d bv.dist &lt;- dist(bv[,2:9]) bv.mds &lt;- cmdscale(bv.dist) plot(bv.mds, col = summ.1$cluster, xlab=&quot;Index&quot;, ylab=&quot;&quot;) # Detect optimal number of clusters install.packages(&quot;NbClust&quot;) library(&quot;NbClust&quot;) Best &lt;- NbClust(bv[,2:9], # data distance=&quot;euclidean&quot;, # distance method min.nc=2, # min number of clusters max.nc=8, # max number of clusters method=&quot;ward.D&quot;, # ward methodes index = &quot;alllong&quot; ) # choose indices 14.1 Next part library(cluster) library(factoextra) Distances: stats::dist() factoextra::get_dist() # compute a distance matrix between the rows of a data matrix factoextra::fviz_dist() # visualize distance matrix cluster::daisy() # handle both numeric and not numeric (nominal, ordinal,...) data types d &lt;- factoextra::get_dist(USArrests, stand = TRUE, method = &#39;pearson&#39;) factoextra::fviz_dist(d, gradient = list(low=&#39;blue&#39;, mid=&#39;white&#39;, high=&#39;red&#39;)) ##### library(tidyverse) library(cluster) library(factoextra) data &lt;- USArrests %&gt;% na.omit() %&gt;% scale() data factoextra::fviz_nbclust(data, kmeans, method = &#39;gap_stat&#39;) km.res &lt;- kmeans(data, 3, nstart = 25) factoextra::fviz_cluster(km.res, data = data, ellipse.type = &#39;convex&#39;, palette = &#39;jco&#39;, repel = TRUE, ggtheme = theme_minimal()) # PAM clustering pam.res &lt;- cluster::pam(data, 4) factoextra::fviz_cluster(pam.res) # CLARA clustering clara.res &lt;- clara(df, 2, samples = 50, pamLike = TRUE) clara.res dd &lt;- cbind(df, cluster = clara.res$cluster) # Medoids clara.res$medoids # Clustering head(clara.res$clustering,10) 14.2 Example library(datasets) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # Plot Petal.Length ~ Petal.Width data plot(iris$Petal.Length ~ iris$Petal.Width) set.seed(20) # Find number of clusters using wss wss &lt;- (nrow(iris[, 3:4])-1)*sum(apply(iris[, 3:4],2,var)) for (i in 2:15) wss[i] &lt;- sum(kmeans(iris[, 3:4], i)$withinss) plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;) #More than 3 clusters give no obvious advantages # Make k-means with 3 clasters ncl &lt;- 3 irisCluster &lt;- kmeans(iris[, 3:4], ncl, nstart = 20) irisCluster ## K-means clustering with 3 clusters of sizes 48, 50, 52 ## ## Cluster means: ## Petal.Length Petal.Width ## 1 5.595833 2.037500 ## 2 1.462000 0.246000 ## 3 4.269231 1.342308 ## ## Clustering vector: ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [74] 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 ## [147] 1 1 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 16.29167 2.02200 13.05769 ## (between_SS / total_SS = 94.3 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; # Compair result of clustering with real data (3 species of iris are in analysis) table(irisCluster$cluster, iris$Species) ## ## setosa versicolor virginica ## 1 0 2 46 ## 2 50 0 0 ## 3 0 48 4 # Plot data clusters &lt;- split.data.frame(iris, irisCluster$cluster) xlim &lt;- c(min(iris$Petal.Width), max(iris$Petal.Width)) ylim &lt;- c(min(iris$Petal.Length), max(iris$Petal.Length)) col &lt;- c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;) plot(0, xlab=&#39;Petal width&#39;, ylab=&#39;Petal length&#39;, xlim=xlim, ylim=ylim) for ( i in 1:ncl ) { points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim) } 14.3 NEXT PART # K-Nearest Neighbors or KNN is a clustering algorithm # k is known number of clusters (usually sqrt(N), between 3-10, but may be different) # samples must be normalized x = (x - min(x))/(max(x)-min(x)) head(iris) summary(iris) # detailed view of the data set str(iris) # view data types, sample values, categorical values, etc plot(iris) #normalization function min_max_normalizer &lt;- function(x) { num &lt;- x - min(x) denom &lt;- max(x) - min(x) return (num/denom) } #normalizing iris data set normalized_iris &lt;- as.data.frame(lapply(iris[1:4], min_max_normalizer)) #viewing normalized data summary(normalized_iris) #checking the data constituency table(iris$Species) #set seed for randomization set.seed(1234) # setting the training-test split to 67% and 33% respectively random_samples &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33)) # training data set iris.training &lt;- iris[ random_samples ==1, 1:4] #training labels iris.trainLabels &lt;- iris[ random_samples ==1, 5] # test data set iris.test &lt;- iris[ random_samples ==2, 1:4] #testing labels iris.testLabels &lt;- iris[ random_samples ==2, 5] #setting library library(class) #executing knn for k=3 iris_model &lt;- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3) #summary of the model learnt iris_model "],["support-vector-machine.html", "Chapter 15 Support Vector Machine", " Chapter 15 Support Vector Machine Definition: Support vector machine is a representation of the training data as points in space separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. Advantages: Effective in high dimensional spaces and uses a subset of training points in the decision function so it is also memory efficient. Disadvantages: The algorithm does not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. Parameters of SVM: * Type of kernel * Gamma value * C value "],["methods-and-algorithms-of-machine-learning.html", "Chapter 16 Methods and algorithms of machine learning", " Chapter 16 Methods and algorithms of machine learning Regression Analysis * Ordinary Least Squares Regression (OLSR) * Linear Regression * Logistic Regression * Stepwise Regression * Polynomial Regression * Locally Estimated Scatterplot Smoothing (LOESS) Distance-based algorithms * k-Nearest Neighbor (kNN) * Learning Vector Quantization (LVQ) * Self-organizing Map (SOM) Regularization Algorithms * Ridge Regression * Least Absolute Shrinkage and Selection Operator (LASSO) * Elastic Net * Least-Angle Regression (LARS) Decision Tree Algorithms * Classification and Regression Tree (CART) * Iterative Dichotomiser 3 (ID3) * C4.5 and C5.0 * Chi-squared Automatic Interation Detection (CHAID) * Random Forest * Conditional Decision Trees Bayesian Algorithms * Naive Bayes * Gaussian Naive Bayes * Multinomial Naive Bayes * Bayesian Belief Network (BBN) * Bayesian Network (BN) Clustering Algorithms * k-Means * k-Medians * Partitioning Around MEdoids (PAM) * Hierarchical Clustering Association Rule Mining Algorithms * Apriori algorithm * Eclat algorithm * FP-growth algorithm * Context Based Rule Mining Artifical Neural Network Algorithms * Perceptron * Back-Propagation * Hopfield Network * Radial Basis Function Network (RBFN) Deep Learining Algorithms * Deep Boltzmann Machine (DBM) * Deep Belief Networks (DBN) * Convolutional NEural Network (CNN) * Stacked Auto-Encoders Dimensionality Reduction Algorithms * Principal Component Analysis (PCA) * Principal Compnent Regression (PCR) * Partial LEast Squares Regression (PLSR) * Multidimensional Scaling (MDS) * Linear Discriminant Analysis (LDA) * Mixtrue Discriminant Analysis (MDA) * Quadratic Discriminant Analysis (QDA) 1. PCA (linear) 2. t-SNE (non-parametric/ nonlinear) 3. Sammon mapping (nonlinear) 4. Isomap (nonlinear) 5. LLE (nonlinear) 6. CCA (nonlinear) 7. SNE (nonlinear) 8. MVU (nonlinear) 9. Laplacian Eigenmaps (nonlinear) Ensemble Algorithms * Boosting * Bagging * AdaBoost * Stacked Generalization (blending) * Gradient Boosting Machines (GBM) Text Mining * Automatic summarization * Named entity recognition (NER) * Optical character recognition (OCR) * Part-of-speech tagging * Sentiment analysis * Speech recognition * Topic Modeling "],["machine-learning-functions-reference.html", "Chapter 17 Machine Learning Functions Reference 17.1 Linear Regression", " Chapter 17 Machine Learning Functions Reference 17.1 Linear Regression lm_model &lt;- lm(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2))) summary(lm_model) lm(y ~ x1 + x2 + x3) # multiple linear regression lm(log(y) ~ x) # log transformed lm(sqrt(y) ~ x) # sqrt transformed lm( y ~ log(x)) # fields transformed llm(log(y) ~ log(x)) # everything is transformed lm(y ~ .) # use all fields for regression model lm(y ~ x + 0) # forced zero intercept lm(y ~ x*k) # interaction of two variables lm(y ~ x + k + x:k) # product of xkl but without interaction lm(y ~ (x + k + ... + l)^2) # all first order interactions lm(y ~ I(x1 + x2)) # sum of variables lm(y ~ I(x1^2)) # product of variables (not interation) lm(y ~ x + I(x^2) + I(x^3)) # polynomial regression lm(y ~ poly(x,3)) # same as previous # Forward/backward stepwise regression # improve model fit &lt;- lm(y ~ x1 + x2) bwd.fit &lt;- step(fit, direction = &#39;backward&#39;) fwd.fit &lt;- step(fit, direction = &#39;forward&#39;, scope( ~ x1 + x2)) # Test linear model plot(m) # plot residuals car::outlier.test(m) dwtest(m) # Durbin-Watson Test of the model residuals # Prediction predicted_values &lt;- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test))) # Apriori dataset &lt;- read.csv(&quot;C:\\\\Datasets\\\\mushroom.csv&quot;, header = TRUE) mushroom_rules &lt;- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9)) summary(mushroom_rules) inspect(mushroom_rules) # Logistic Regression glm_mod &lt;-glm(y ∼ x1+x2, family=binomial(link=&quot;logit&quot;), data=as.data.frame(cbind(y,x1,x2))) # K-Means Clustering kmeans_model &lt;- kmeans(x=X, centers=m) # k-Nearest Neighbor Classification knn_model &lt;- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K) # Naıve Bayes library(e1071) nB_model &lt;- naiveBayes(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2))) # Decision Trees (CART) library(rpart) cart_model &lt;- rpart(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=&quot;class&quot;) plot.rpart(cart_model) text.rpart(cart_model) # AdaBoost # boosting functions - uses decision trees as base classifiers library(rpart) library(ada) # Let X be the matrix of features, and labels be a vector of 0-1 class labels. boost_model &lt;- ada(x=X, y=labels) # Support Vector Machines (SVM) library(e1071) # Let X be the matrix of features, and labels be a vector of 0-1 class labels. Let the regularization parameter be C. Use the following commands to generate the SVM model and view details: svm_model &lt;- svm(x=X, y=as.factor(labels), kernel =&quot;radial&quot;, cost=C) summary(svm_model) "],["split-data-into-train-and-test-subsets.html", "Chapter 18 Split data into train and test subsets", " Chapter 18 Split data into train and test subsets Here you can find several simple approaches to split data into train and test subset to fit and to test parameters of your model. We want to take 0.8 of our initial data to train our model. Data: datasets::iris. First approach is to create a vector containing randomly selected row ids and to apply this vector to split data. inTrain = sample(nrow(iris), nrow(iris)*0.8) # split data train = iris[inTrain, ] test = iris[-inTrain, ] The same idea to split data as before using caret package. The advantage is that createDataPartition function allows to split data many times and use these subsets to estimate parameters of our model. library(caret) trainIndex &lt;- createDataPartition(iris$Species, p=.8, list = FALSE, # if FALSE - create a vector/matrix, if TRUE - create a list times = 1) # how many subsets # split data train &lt;- iris[trainIndex, ] test &lt;- iris[-trainIndex, ] Another approch is to create a logical vecotor containing randomly distributed true/false and apply this vector to subset data. inTrain = sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.8,0.2)) # select data train = iris[inTrain, ] test = iris[!inTrain, ] Using caTools. library(caTools) inTrain = sample.split(iris, SplitRatio = .8) train = subset(iris, inTrain == TRUE) test = subset(iris, inTrain == FALSE) Using dplyr library(dplyr) iris$id &lt;- 1:nrow(iris) train &lt;- iris %&gt;% dplyr::sample_frac(.8) test &lt;- dplyr::anti_join(iris, train, by = &#39;id&#39;) "],["linear-regression-1.html", "Chapter 19 Linear Regression 19.1 Generate Random Data Set a Linear Model 19.2 Linear regression - theory 19.3 Practical example 19.4 Example of linear regression 19.5 Standard error of train data 19.6 Practical examples for linear model regression 19.7 Practical examples for linear model regression 19.8 NEXT part", " Chapter 19 Linear Regression 19.1 Generate Random Data Set a Linear Model Suppose we want to simulate from the following linear model: y = \\(\\beta\\)0 + \\(\\beta\\)1x + \\(\\epsilon\\), where \\(\\epsilon\\) ~ N(0,22). Assume x ~ N(0,12), \\(\\beta\\)0 = 0.5, \\(\\beta\\)1 = 2. set.seed(20) x &lt;-rnorm(100) e &lt;- rnorm(100, 0, 2) y &lt;- 0.5 + 2*x + e summary(y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -6.4084 -1.5402 0.6789 0.6893 2.9303 6.5052 plot(x,y) 19.2 Linear regression - theory Assume that there is approximately a linear relationship between X and Y: \\[ Y \\approx \\beta_0 + \\beta_1X\\] where \\(\\beta\\)0 is an intercept and \\(\\beta\\)1 is a slope Parameters of the line could be calculated using least squares methods: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}} \\] \\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x} \\] 19.3 Practical example Practical example from Wikipedia Set of data: (1,6), (2,5), (3, 7), (4,10) We have to find the line corresponding to the minimal sum of errors (distances from the each point to this line): 1. For all points: \\[\\beta_1 + 1\\beta_2 = 6\\] \\[\\beta_1 + 2\\beta_2 = 5\\] \\[\\beta_1 + 3\\beta_2 = 7\\] \\[\\beta_1 + 4\\beta_2 = 10\\] the least squares S: \\[S(\\beta_1, \\beta_2) = [6 - (\\beta_1 + 1\\beta_2)]^2 + [5 - (\\beta_1 + 2\\beta_2)]^2 + [7 - (\\beta_1 + 3\\beta_2)]^2 + [10 - (\\beta_1 + 4\\beta_2)]^2 = 4\\beta_1^2 + 30\\beta_2^2 + 20\\beta_1\\beta_2 - 56\\beta_1 - 154\\beta_2 + 210\\] The minimum is: \\[\\frac{\\partial{S}}{\\partial{\\beta_1}} = 0 = 8 \\beta_1 + 60\\beta_2 - 154\\] \\[\\frac{\\partial{S}}{\\partial{\\beta_2}} = 0 = 20 \\beta_1 + 20\\beta_2 - 56\\] Result in a system of two equations in two unkowns gives: \\[\\beta_1 = 3.5\\] \\[\\beta_2 = 1.4\\] The line of best fit: y = 3.5 + 1.4x All possible regression lines goes through the intersection point \\((\\bar{x}, \\bar{y})\\) 19.4 Example of linear regression x &lt;- c(1,2,3,4) y &lt;- c(6,5,7,10) lm(y~x) ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 3.5 1.4 19.5 Standard error of train data \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2\\] ## Standard error of learn data \\[MSE = \\frac{1}{n_o}\\sum_{i=1}^{n_o}(y_i^o - \\hat{f}(x_i^o))^2\\] 19.6 Practical examples for linear model regression In this simple example we have 6 persons (3 males and 3 femails) and their score from 0 to 10. We want to build a model to see the dependence of score on gender: score ~ gender + \\(\\epsilon\\), where \\(\\epsilon\\) is an error # create data frame for the dataset df = data.frame(gender=c(rep(0,3), rep(1,3)), score=c(10,8,7, 1,3,2)) df ## gender score ## 1 0 10 ## 2 0 8 ## 3 0 7 ## 4 1 1 ## 5 1 3 ## 6 1 2 # build linear model x = lm(score ~ gender, df) summary(x) ## ## Call: ## lm(formula = score ~ gender, data = df) ## ## Residuals: ## 1 2 3 4 5 6 ## 1.667e+00 -3.333e-01 -1.333e+00 -1.000e+00 1.000e+00 1.110e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.3333 0.7454 11.180 0.000364 *** ## gender -6.3333 1.0541 -6.008 0.003863 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.291 on 4 degrees of freedom ## Multiple R-squared: 0.9002, Adjusted R-squared: 0.8753 ## F-statistic: 36.1 on 1 and 4 DF, p-value: 0.003863 19.7 Practical examples for linear model regression In this simple example we have 6 persons (3 males and 3 femails) and their score from 0 to 10. We want to build a model to see the dependence of score on gender: score ~ gender + \\(\\epsilon\\), where \\(\\epsilon\\) is an error # create data frame for the dataset df = data.frame(gender=c(rep(0,3), rep(1,3)), score=c(10,8,7, 1,3,2)) df ## gender score ## 1 0 10 ## 2 0 8 ## 3 0 7 ## 4 1 1 ## 5 1 3 ## 6 1 2 # build linear model x = lm(score ~ gender, df) summary(x) ## ## Call: ## lm(formula = score ~ gender, data = df) ## ## Residuals: ## 1 2 3 4 5 6 ## 1.667e+00 -3.333e-01 -1.333e+00 -1.000e+00 1.000e+00 1.110e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.3333 0.7454 11.180 0.000364 *** ## gender -6.3333 1.0541 -6.008 0.003863 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.291 on 4 degrees of freedom ## Multiple R-squared: 0.9002, Adjusted R-squared: 0.8753 ## F-statistic: 36.1 on 1 and 4 DF, p-value: 0.003863 19.8 NEXT part # Linear regression modeling, compair with kNN # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home library(&#39;GGally&#39;) library(&#39;lmtest&#39;) library(&#39;FNN&#39;) # константы my.seed &lt;- 12345 train.percent &lt;- 0.85 # загрузка данных fileURL &lt;- &#39;https://sites.google.com/a/kiber-guu.ru/msep/mag-econ/salary_data.csv?attredirects=0&amp;d=1&#39; # преобразуем категориальные переменные в факторы wages.ru &lt;- read.csv(fileURL, row.names = 1, sep = &#39;;&#39;, as.is = T) wages.ru$male &lt;- as.factor(wages.ru$male) wages.ru$educ &lt;- as.factor(wages.ru$educ) wages.ru$forlang &lt;- as.factor(wages.ru$forlang) # обучающая выборка set.seed(my.seed) inTrain &lt;- sample(seq_along(wages.ru$salary), nrow(wages.ru) * train.percent) df.train &lt;- wages.ru[inTrain, c(colnames(wages.ru)[-1], colnames(wages.ru)[1])] df.test &lt;- wages.ru[-inTrain, -1] # Variable description # salary – среднемесячная зарплата после вычета налогов за последние 12 месяцев (рублей); # male – пол: 1 – мужчина, 0 – женщина; # educ – уровень образования: # 1 – 0-6 классов, # 2 – незаконченное среднее (7-8 классов), # 3 - незаконченное среднее плюс что-то ещё, # 4 – законченное среднее, # 5 – законченное среднее специальное, 6 – законченное высшее образование и выше; # forlang - иност. язык: 1 – владеет, 0 – нет; # exper – официальный стаж c 1.01.2002 (лет). summary(df.train) ggp &lt;- ggpairs(df.train) print(ggp, progress = F) # цвета по фактору male ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;male&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = male)) print(ggp, progress = F) # цвета по фактору educ ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;educ&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = educ)) print(ggp, progress = F) # цвета по фактору forlang ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;forlang&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = forlang)) print(ggp, progress = F) # Linear regression model model.1 &lt;- lm(salary ~ . + exper:educ + exper:forlang + exper:male, data = df.train) summary(model.1) ## Exclude uninfluencial parameters # Exclude eper:educ as paramaeters are not important model.2 &lt;- lm(salary ~ . + exper:forlang + exper:male, data = df.train) summary(model.2) # Exclude male1:exper model.3 &lt;- lm(salary ~ . + exper:forlang, data = df.train) summary(model.3) # forlang1 is less important, has no sence model.4 &lt;- lm(salary ~ male + educ + exper, data = df.train) summary(model.4) df.train$educ &lt;- as.numeric(df.train$educ) df.test$educ &lt;- as.numeric(df.test$educ) model.6 &lt;- lm(salary ~ ., data = df.train) summary(model.6) # Model 6 is week, let&#39;s add exper:male interactions df.train$educ &lt;- as.numeric(df.train$educ) model.7 &lt;- lm(salary ~ . + exper:male, data = df.train) summary(model.7) # Obviously the best decision is not to use interactions for modeling # Test remainers # тест Бройша-Пагана bptest(model.6) # статистика Дарбина-Уотсона dwtest(model.6) # графики остатков par(mar = c(4.5, 4.5, 2, 1)) par(mfrow = c(1, 3)) plot(model.7, 1) plot(model.7, 4) plot(model.7, 5) ### Comparison with kNN-method par(mfrow = c(1, 1)) # фактические значения y на тестовой выборке y.fact &lt;- wages.ru[-inTrain, 1] y.model.lm &lt;- predict(model.6, df.test) MSE.lm &lt;- sum((y.model.lm - y.fact)^2) / length(y.model.lm) # kNN требует на вход только числовые переменные df.train.num &lt;- as.data.frame(apply(df.train, 2, as.numeric)) df.test.num &lt;- as.data.frame(apply(df.test, 2, as.numeric)) for (i in 2:50){ model.knn &lt;- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% &#39;salary&#39;)], y = df.train.num[, &#39;salary&#39;], test = df.test.num, k = i) y.model.knn &lt;- model.knn$pred if (i == 2){ MSE.knn &lt;- sum((y.model.knn - y.fact)^2) / length(y.model.knn) } else { MSE.knn &lt;- c(MSE.knn, sum((y.model.knn - y.fact)^2) / length(y.model.knn)) } } # график par(mar = c(4.5, 4.5, 1, 1)) plot(2:50, MSE.knn, type = &#39;b&#39;, col = &#39;darkgreen&#39;, xlab = &#39;значение k&#39;, ylab = &#39;MSE на тестовой выборке&#39;) lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2) legend(&#39;bottomright&#39;, lty = c(1, 2), pch = c(1, NA), col = c(&#39;darkgreen&#39;, grey(0.2)), legend = c(&#39;k ближайших соседа&#39;, &#39;регрессия (все факторы)&#39;), lwd = rep(2, 2)) "],["next-part-3.html", "Chapter 20 NEXT part 20.1 NEXT part 20.2 NEXT Part", " Chapter 20 NEXT part ##### LINEAR REGRESSION # Linear regression model is a line y=ax+b, where sum of distances between all y=axi+b and given yi (sum of squares) is minimal. # Example (from R Machine Learning by Example) # Height and weight vectors for 19 children height &lt;- c(69.1,56.4,65.3,62.8,63,57.3,59.8,62.5,62.5,59.0,51.3,64,56.4,66.5,72.2,65.0,67.0,57.6,66.6) weight &lt;- c(113,84,99,103,102,83,85,113,84,99,51,90,77,112,150,128,133,85,112) plot(height,weight) cor(height,weight) # Fit linear model model &lt;- lm(weight ~ height) # weight = slope*weight + intercept abline(model) # Regression line # Get data of model #get the intercept(b0) and the slope(b1) values model # check all attributes calculated by lm attributes(model) # getting only the intercept model$coefficients[1] #or model$coefficients[[1]] # getting only the slope model$coefficients[2] #or model$coefficients[[2]] # checking the residuals residuals(model) # predicting the weight for a given height, say 60 inches model$coefficients[[2]]*50 + model$coefficients[[1]] # detailed information about the model summary(model) ##### 2. Linear regression model for multiple parameters df &lt;- mtcars # Subset of necessary data from mtcars df.sub &lt;- df[,c(1,3:7)] # Linear regression model fit &lt;- lm(mpg ~ hp, df) fit summary(fit) # Plot regression models using ggplot # parameter geom_smooth builds regression model library(ggplot2) # auto model ggplot(df, aes(hp, mpg))+geom_point(size=2)+geom_smooth() # for linear model: method = &#39;lm&#39; ggplot(df, aes(hp, mpg))+geom_point(size=2)+geom_smooth(method = &quot;lm&quot;) # Split data into two groups by am - Transmission (0 = automatic, 1 = manual) ggplot(df, aes(hp, mpg, col=factor(am)))+ geom_point(size=2)+ geom_smooth(method = &quot;lm&quot;) # Prediction of values using linear regression model fitted_values_mpg &lt;- data.frame(mpg=df$mpg, fitted=fit$fitted.values) fitted_values_mpg View(fitted_values_mpg) # Lets predict galons of petrol for given horse powers new_hp &lt;- data.frame(hp=c(100, 150, 129, 300)) predict(fit, new_hp) new_hp$mpg &lt;- predict(fit, new_hp) new_hp # Lets make regression model for cylinders as numeric (not factor) fit &lt;- lm(mpg ~ cyl, df) fit # Regression line for cyclinders ggplot(df, aes(cyl, mpg))+ geom_point()+ geom_smooth(method=&quot;lm&quot;)+ theme(axis.text=element_text(size=25), axis.title=element_text(size=25, face=&quot;bold&quot;)) ##### 3. More complex example # Linear regression modeling, compair with kNN # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home library(&#39;GGally&#39;) library(&#39;lmtest&#39;) library(&#39;FNN&#39;) # variable my.seed &lt;- 12345 train.percent &lt;- 0.85 wages.ru &lt;- read.csv(&quot;~/Projects/data_analysis/DATA/salary.csv&quot;, row.names = 1, sep = &#39;\\t&#39;, as.is = T) # transform data into factors when possible wages.ru$male &lt;- as.factor(wages.ru$male) wages.ru$educ &lt;- as.factor(wages.ru$educ) wages.ru$forlang &lt;- as.factor(wages.ru$forlang) # test data set.seed(my.seed) inTrain &lt;- sample(seq_along(wages.ru$salary), nrow(wages.ru) * train.percent) df.train &lt;- wages.ru[inTrain, c(colnames(wages.ru)[-1], colnames(wages.ru)[1])] df.test &lt;- wages.ru[-inTrain, -1] summary(df.train) ggp &lt;- ggpairs(df.train) print(ggp, progress = F) # цвета по фактору male ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;male&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = male)) print(ggp, progress = F) # цвета по фактору educ ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;educ&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = educ)) print(ggp, progress = F) # цвета по фактору forlang ggp &lt;- ggpairs(df.train[, c(&#39;exper&#39;, &#39;forlang&#39;, &#39;salary&#39;)], mapping = ggplot2::aes(color = forlang)) print(ggp, progress = F) # Linear regression model model.1 &lt;- lm(salary ~ . + exper:educ + exper:forlang + exper:male, data = df.train) summary(model.1) ## Exclude uninfluencial parameters # Exclude eper:educ as paramaeters are not important model.2 &lt;- lm(salary ~ . + exper:forlang + exper:male, data = df.train) summary(model.2) # Exclude male1:exper model.3 &lt;- lm(salary ~ . + exper:forlang, data = df.train) summary(model.3) # forlang1 is less important, has no sence model.4 &lt;- lm(salary ~ male + educ + exper, data = df.train) summary(model.4) df.train$educ &lt;- as.numeric(df.train$educ) df.test$educ &lt;- as.numeric(df.test$educ) model.6 &lt;- lm(salary ~ ., data = df.train) summary(model.6) # Model 6 is week, let&#39;s add exper:male interactions df.train$educ &lt;- as.numeric(df.train$educ) model.7 &lt;- lm(salary ~ . + exper:male, data = df.train) summary(model.7) # Obviously the best decision is not to use interactions for modeling # Test remainers # тест Бройша-Пагана bptest(model.6) # статистика Дарбина-Уотсона dwtest(model.6) # графики остатков par(mar = c(4.5, 4.5, 2, 1)) par(mfrow = c(1, 3)) plot(model.7, 1) plot(model.7, 4) plot(model.7, 5) ### Comparison with kNN-method par(mfrow = c(1, 1)) # фактические значения y на тестовой выборке y.fact &lt;- wages.ru[-inTrain, 1] y.model.lm &lt;- predict(model.6, df.test) MSE.lm &lt;- sum((y.model.lm - y.fact)^2) / length(y.model.lm) # kNN требует на вход только числовые переменные df.train.num &lt;- as.data.frame(apply(df.train, 2, as.numeric)) df.test.num &lt;- as.data.frame(apply(df.test, 2, as.numeric)) for (i in 2:50){ model.knn &lt;- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% &#39;salary&#39;)], y = df.train.num[, &#39;salary&#39;], test = df.test.num, k = i) y.model.knn &lt;- model.knn$pred if (i == 2){ MSE.knn &lt;- sum((y.model.knn - y.fact)^2) / length(y.model.knn) } else { MSE.knn &lt;- c(MSE.knn, sum((y.model.knn - y.fact)^2) / length(y.model.knn)) } } # график par(mar = c(4.5, 4.5, 1, 1)) plot(2:50, MSE.knn, type = &#39;b&#39;, col = &#39;darkgreen&#39;, xlab = &#39;значение k&#39;, ylab = &#39;MSE на тестовой выборке&#39;) lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2) legend(&#39;bottomright&#39;, lty = c(1, 2), pch = c(1, NA), col = c(&#39;darkgreen&#39;, grey(0.2)), legend = c(&#39;k ближайших соседа&#39;, &#39;регрессия (все факторы)&#39;), lwd = rep(2, 2)) 20.1 NEXT part # Linear regression (part 1) # Linear Regression (part 2) x &lt;- read.table(&quot;~/DataAnalysis/R_data_analysis/DATA/Diamond.dat&quot;, header=T) # Check 3 models # 1. ax+b # 2. ax^2+b # 3. ax^2+bx+c res.1 &lt;- lm(x[,2]~x[,1]) summary(res.1) ves2 &lt;- x[,1]*x[,1] res.2 &lt;- lm(x[,2]~ves2) summary(res.2) res.3 &lt;- lm(x[,2]~x[,1]+ves2) summary(res.3) # weight of diamands ~ weight^2 plot(x[,1]~ves2) # Conclusion: from 3 models, the 2d is optimal ### Временные ряды ser.g.01 &lt;- read.table(&quot;~/DataAnalysis/R_data_analysis/DATA/series_g.csv&quot;, header=T, sep=&quot;;&quot;) # visual inspection ser.g.01 dim(ser.g.01) names(ser.g.01) # plot vaiable plot(ser.g.01$series_g, type=&quot;l&quot;) log.ser.g &lt;- log(ser.g.01$series_g) plot(log.ser.g, type=&quot;l&quot;) time. &lt;- 1:(144+12) month.01 &lt;- rep(c(1,0,0,0,0,0,0,0,0,0,0,0), 12+1) month.02 &lt;- rep(c(0,1,0,0,0,0,0,0,0,0,0,0), 12+1) month.03 &lt;- rep(c(0,0,1,0,0,0,0,0,0,0,0,0), 12+1) month.04 &lt;- rep(c(0,0,0,1,0,0,0,0,0,0,0,0), 12+1) month.05 &lt;- rep(c(0,0,0,0,1,0,0,0,0,0,0,0), 12+1) month.06 &lt;- rep(c(0,0,0,0,0,1,0,0,0,0,0,0), 12+1) month.07 &lt;- rep(c(0,0,0,0,0,0,1,0,0,0,0,0), 12+1) month.08 &lt;- rep(c(0,0,0,0,0,0,0,1,0,0,0,0), 12+1) month.09 &lt;- rep(c(0,0,0,0,0,0,0,0,1,0,0,0), 12+1) month.10 &lt;- rep(c(0,0,0,0,0,0,0,0,0,1,0,0), 12+1) month.11 &lt;- rep(c(0,0,0,0,0,0,0,0,0,0,1,0), 12+1) month.12 &lt;- rep(c(0,0,0,0,0,0,0,0,0,0,0,1), 12+1) log.ser.g[145:(144+12)] &lt;-NA ser.g.02 &lt;- data.frame(log.ser.g, time., month.01, month.02, month.03, month.04, month.05, month.06, month.07, month.08, month.09, month.10, month.11, month.12) ser.g.02 res.01 &lt;- lm(log.ser.g ~ time. + month.02 + month.03 + month.04 + month.05 + month.06 + month.07 + month.08 + month.09 + month.10 + month.11 + month.12, ser.g.02) summary(res.01) res.01$fitted.values plot(ser.g.02$log.ser.g, type=&quot;l&quot;) lines(res.01$fitted.values) plot(ser.g.02$log.ser.g, type=&quot;l&quot;, col=&quot;green&quot;) lines(res.01$fitted.values, col=&quot;red&quot;) x.lg = predict.lm(res.01, ser.g.02) x.lg plot(x.lg, type=&quot;l&quot;, col=&quot;red&quot;) lines(ser.g.02$log.ser.g, col=&quot;green&quot;) y &lt;- exp(x.lg) plot(y, type=&quot;l&quot;, col=&quot;red&quot;) lines(ser.g.01$series_g, col=&quot;green&quot;) 20.2 NEXT Part # провести отбор оптимального подмножества переменных; # отобрать предикторы методами пошагового включения и исключения; # как построить ридж- и лассо-регрессию; # как использовать снижение размерности: PCR и PLS; # как применять эти методы в сочетании с кросс-валидацией. library(&#39;ISLR&#39;) # набор данных Hitters library(&#39;leaps&#39;) # функция regsubset() -- отбор оптимального # подмножества переменных library(&#39;glmnet&#39;) # функция glmnet() -- лассо library(&#39;pls&#39;) # регрессия на главные компоненты -- pcr() # и частный МНК -- plsr() my.seed &lt;- 1 ?Hitters fix(Hitters) names(Hitters) dim(Hitters) sum(is.na(Hitters$Salary)) Hitters &lt;- na.omit(Hitters) dim(Hitters) sum(is.na(Hitters$Salary)) ## Отбор оптимального подмножества # подгоняем модели с сочетаниями предикторов до 8 включительно regfit.full &lt;- regsubsets(Salary ~ ., Hitters) summary(regfit.full) # подгоняем модели с сочетаниями предикторов до 19 (максимум в данных) regfit.full &lt;- regsubsets(Salary ~ ., Hitters, nvmax = 19) reg.summary &lt;- summary(regfit.full) reg.summary # структура отчёта по модели (ищем характеристики качества) names(reg.summary) # R^2 и скорректированный R^2 round(reg.summary$rsq, 3) # на графике plot(1:19, reg.summary$rsq, type = &#39;b&#39;, xlab = &#39;Количество предикторов&#39;, ylab = &#39;R-квадрат&#39;) # сода же добавим скорректированный R-квадрат points(1:19, reg.summary$adjr2, col = &#39;red&#39;) # модель с максимальным скорректированным R-квадратом which.max(reg.summary$adjr2) ### 11 points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = &#39;red&#39;, cex = 2, pch = 20) legend(&#39;bottomright&#39;, legend = c(&#39;R^2&#39;, &#39;R^2_adg&#39;), col = c(&#39;black&#39;, &#39;red&#39;), lty = c(1, NA), pch = c(1, 1)) # C_p reg.summary$cp # число предикторов у оптимального значения критерия which.min(reg.summary$cp) ### 10 # график plot(reg.summary$cp, xlab = &#39;Число предикторов&#39;, ylab = &#39;C_p&#39;, type = &#39;b&#39;) points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = &#39;red&#39;, cex = 2, pch = 20) # BIC reg.summary$bic # число предикторов у оптимального значения критерия which.min(reg.summary$bic) ### 6 # график plot(reg.summary$bic, xlab = &#39;Число предикторов&#39;, ylab = &#39;BIC&#39;, type = &#39;b&#39;) points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = &#39;red&#39;, cex = 2, pch = 20) # метод plot для визуализации результатов ?plot.regsubsets plot(regfit.full, scale = &#39;r2&#39;) plot(regfit.full, scale = &#39;adjr2&#39;) plot(regfit.full, scale = &#39;Cp&#39;) plot(regfit.full, scale = &#39;bic&#39;) # коэффициенты модели с наименьшим BIC round(coef(regfit.full, 6), 3) ## Отбор путём пошагового включения и исключения переменных # Пошаговое включение regfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = &#39;forward&#39;) summary(regfit.fwd) regfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = &#39;backward&#39;) summary(regfit.bwd) round(coef(regfit.full, 7), 3) round(coef(regfit.fwd, 7), 3) round(coef(regfit.bwd, 7), 3) ### Нахождение оптимальной модели при помощи методов проверочной выборки и перекрёстной проверки set.seed(my.seed) train &lt;- sample(c(T, F), nrow(Hitters), rep = T) test &lt;- !train # обучаем модели regfit.best &lt;- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19) # матрица объясняющих переменных модели для тестовой выборки test.mat &lt;- model.matrix(Salary ~ ., data = Hitters[test, ]) # вектор ошибок val.errors &lt;- rep(NA, 19) # цикл по количеству предикторов for (i in 1:19){ coefi &lt;- coef(regfit.best, id = i) pred &lt;- test.mat[, names(coefi)] %*% coefi # записываем значение MSE на тестовой выборке в вектор val.errors[i] &lt;- mean((Hitters$Salary[test] - pred)^2) } round(val.errors, 0) # находим число предикторов у оптимальной модели which.min(val.errors) ### 10 # коэффициенты оптимальной модели round(coef(regfit.best, 10), 3) # функция для прогноза для функции regsubset() predict.regsubsets &lt;- function(object, newdata, id, ...){ form &lt;- as.formula(object$call[[2]]) mat &lt;- model.matrix(form, newdata) coefi &lt;- coef(object, id = id) xvars &lt;- names(coefi) mat[, xvars] %*% coefi } # набор с оптимальным количеством переменных на полном наборе данных regfit.best &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19) round(coef(regfit.best, 10), 3) # k-кратная кросс-валидация # отбираем 10 блоков наблюдений k &lt;- 10 set.seed(my.seed) folds &lt;- sample(1:k, nrow(Hitters), replace = T) # заготовка под матрицу с ошибками cv.errors &lt;- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19))) # заполняем матрицу в цикле по блокам данных for (j in 1:k){ best.fit &lt;- regsubsets(Salary ~ ., data = Hitters[folds != j, ], nvmax = 19) # теперь цикл по количеству объясняющих переменных for (i in 1:19){ # модельные значения Salary pred &lt;- predict(best.fit, Hitters[folds == j, ], id = i) # вписываем ошибку в матрицу cv.errors[j, i] &lt;- mean((Hitters$Salary[folds == j] - pred)^2) } } # усредняем матрицу по каждому столбцу (т.е. по блокам наблюдений), # чтобы получить оценку MSE для каждой модели с фиксированным # количеством объясняющих переменных mean.cv.errors &lt;- apply(cv.errors, 2, mean) round(mean.cv.errors, 0) # на графике plot(mean.cv.errors, type = &#39;b&#39;) points(which.min(mean.cv.errors), mean.cv.errors[which.min(mean.cv.errors)], col = &#39;red&#39;, pch = 20, cex = 2) # перестраиваем модель с 11 объясняющими переменными на всём наборе данных reg.best &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19) round(coef(reg.best, 11), 3) # из-за синтаксиса glmnet() формируем явно матрицу объясняющих... x &lt;- model.matrix(Salary ~ ., Hitters)[, -1] # и вектор значений зависимой переменной y &lt;- Hitters$Salary ### Гребневая регрессия # вектор значений гиперпараметра лямбда grid &lt;- 10^seq(10, -2, length = 100) # подгоняем серию моделей ридж-регрессии ridge.mod &lt;- glmnet(x, y, alpha = 0, lambda = grid) # размерность матрицы коэффициентов моделей dim(coef(ridge.mod)) ## [1] 20 100 # значение лямбда под номером 50 round(ridge.mod$lambda[50], 0) ## [1] 11498 # коэффициенты соответствующей модели round(coef(ridge.mod)[, 50], 3) # норма эль-два round(sqrt(sum(coef(ridge.mod)[-1, 50]^2)), 2) # всё то же для лямбды под номером 60 # значение лямбда под номером 50 round(ridge.mod$lambda[60], 0) # коэффициенты соответствующей модели round(coef(ridge.mod)[, 60], 3) # норма эль-два round(sqrt(sum(coef(ridge.mod)[-1, 60]^2)), 1) # мы можем получить значения коэффициентов для новой лямбды round(predict(ridge.mod, s = 50, type = &#39;coefficients&#39;)[1:20, ], 3) ## Метод проверочной выборки set.seed(my.seed) train &lt;- sample(1:nrow(x), nrow(x)/2) test &lt;- -train y.test &lt;- y[test] # подгоняем ридж-модели с большей точностью (thresh ниже значения по умолчанию) ridge.mod &lt;- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12) plot(ridge.mod) # прогнозы для модели с лямбда = 4 ridge.pred &lt;- predict(ridge.mod, s = 4, newx = x[test, ]) round(mean((ridge.pred - y.test)^2), 0) # сравним с MSE для нулевой модели (прогноз = среднее) round(mean((mean(y[train]) - y.test)^2), 0) # насколько модель с лямбда = 4 отличается от обычной ПЛР ridge.pred &lt;- predict(ridge.mod, s = 0, newx = x[test, ], exact = T, x = x[train, ], y = y[train]) round(mean((ridge.pred - y.test)^2), 0) # predict с лямбдой (s) = 0 даёт модель ПЛР lm(y ~ x, subset = train) round(predict(ridge.mod, s = 0, exact = T, type = &#39;coefficients&#39;, x = x[train, ], y = y[train])[1:20, ], 3) ## Подбор оптимального значения лямбда с помощью перекрёстной проверки # k-кратная кросс-валидация set.seed(my.seed) # оценка ошибки cv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 0) plot(cv.out) # значение лямбда, обеспечивающее минимальную ошибку перекрёстной проверки bestlam &lt;- cv.out$lambda.min round(bestlam, 0) ## [1] 212 # MSE на тестовой для этого значения лямбды ridge.pred &lt;- predict(ridge.mod, s = bestlam, newx = x[test, ]) round(mean((ridge.pred - y.test)^2), 0) ## [1] 96016 # наконец, подгоняем модель для оптимальной лямбды, # найденной по перекрёстной проверке out &lt;- glmnet(x, y, alpha = 0) round(predict(out, type = &#39;coefficients&#39;, s = bestlam)[1:20, ], 3) ## Лассо lasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = grid) plot(lasso.mod) # Подбор оптимального значения лямбда с помощью перекрёстной проверки set.seed(my.seed) cv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1) plot(cv.out) bestlam &lt;- cv.out$lambda.min lasso.pred &lt;- predict(lasso.mod, s = bestlam, newx = x[test, ]) round(mean((lasso.pred - y.test)^2), 0) # коэффициенты лучшей модели out &lt;- glmnet(x, y, alpha = 1, lambda = grid) lasso.coef &lt;- predict(out, type = &#39;coefficients&#39;, s = bestlam)[1:20, ] round(lasso.coef, 3) round(lasso.coef[lasso.coef != 0], 3) ### Лабораторная работа 3: регрессия при помощи методов PCR и PLS ### 6.7.1 Регрессия на главные компоненты # кросс-валидация set.seed(2) # непонятно почему они сменили зерно; похоже, опечатка pcr.fit &lt;- pcr(Salary ~ ., data = Hitters, scale = T, validation = &#39;CV&#39;) summary(pcr.fit) # график ошибок validationplot(pcr.fit, val.type = &#39;MSEP&#39;) # Подбор оптиального M: кросс-валидация на обучающей выборке set.seed(my.seed) pcr.fit &lt;- pcr(Salary ~ ., data = Hitters, subset = train, scale = T, validation = &#39;CV&#39;) validationplot(pcr.fit, val.type = &#39;MSEP&#39;) # MSE на тестовой выборке pcr.pred &lt;- predict(pcr.fit, x[test, ], ncomp = 7) round(mean((pcr.pred - y.test)^2), 0) # подгоняем модель на всей выборке для M = 7 # (оптимально по методу перекрёстной проверки) pcr.fit &lt;- pcr(y ~ x, scale = T, ncomp = 7) summary(pcr.fit) # Регрессия по методу частных наименьших квадратов set.seed(my.seed) pls.fit &lt;- plsr(Salary ~ ., data = Hitters, subset = train, scale = T, validation = &#39;CV&#39;) summary(pls.fit) # теперь подгоняем модель для найденного оптимального M = 2 # и оцениваем MSE на тестовой pls.pred &lt;- predict(pls.fit, x[test, ], ncomp = 2) round(mean((pls.pred - y.test)^2), 0) ## [1] 101417 # подгоняем модель на всей выборке pls.fit &lt;- plsr(Salary ~ ., data = Hitters, scale = T, ncomp = 2) summary(pls.fit) "],["nonlinear-regression.html", "Chapter 21 Nonlinear regression", " Chapter 21 Nonlinear regression Nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. Some nonlinear data sets can be transformed to a linear model. Sone can not be transformed. For such modeling methods of Numerical analysis should be applied such as Newton’s method, Gauss-Newton method and Levenberg–Marquardt method. Математическое моделирование Практика 7 Нелинейные модели В практических примерах ниже показано как: оценивать полиномиальную регрессию; аппроксимировать нелинейные модели ступенчатыми функциями; строить сплайны; работать с локальной регрессией; строить обобщённые линейные модели (GAM). Модели: полиномиальная регрессия, полиномиальная логистическая регрессия, ступенчатая модель, обобщённая линейная модель. Данные: Wage {ISLR} Подробные комментарии к коду лабораторных см. в [1], глава 7. library(&#39;ISLR&#39;) # набор данных Auto library(&#39;splines&#39;) # сплайны library(&#39;gam&#39;) # обобщённые аддитивные модели ## Warning: package &#39;gam&#39; was built under R version 3.3.3 ## Loading required package: foreach ## Warning: package &#39;foreach&#39; was built under R version 3.3.3 ## Loaded gam 1.14 library(&#39;akima&#39;) # график двумерной плоскости ## Warning: package &#39;akima&#39; was built under R version 3.3.3 library(&#39;ggplot2&#39;) # красивые графики ## Warning: package &#39;ggplot2&#39; was built under R version 3.3.3 my.seed &lt;- 1 Работаем с набором данных по зарплатам 3000 работников-мужчин среднеатлантического региона Wage. Присоединяем его к пространству имён функцией attach(), и дальше обращаемся напрямую к столбцам таблицы. attach(Wage) Работаем со столбцами: * wage – заработная плата работника до уплаты налогов; * age – возраст работника в годах. Полиномиальная регрессия Зависимость зарплаты от возраста Судя по графику ниже, ззаимосвязь заработной платы и возраста нелинейна. Наблюдается также группа наблюдений с высоким значением wage, граница проходит примерно на уровне 250. gp &lt;- ggplot(data = Wage, aes(x = age, y = wage)) gp &lt;- gp + geom_point() + geom_abline(slope = 0, intercept = 250, col = &#39;red&#39;) gp Подгоняем полином четвёртой степени для зависимости заработной платы от возраста. fit &lt;- lm(wage ~ poly(age, 4), data = Wage) round(coef(summary(fit)), 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.70 0.73 153.28 0.00 ## poly(age, 4)1 447.07 39.91 11.20 0.00 ## poly(age, 4)2 -478.32 39.91 -11.98 0.00 ## poly(age, 4)3 125.52 39.91 3.14 0.00 ## poly(age, 4)4 -77.91 39.91 -1.95 0.05 Функция poly(age, 4) создаёт таблицу с базисом ортогональных полиномов: линейные комбинации значений переменной age в степенях от 1 до 4. round(head(poly(age, 4)), 3) ## 1 2 3 4 ## [1,] -0.039 0.056 -0.072 0.087 ## [2,] -0.029 0.026 -0.015 -0.003 ## [3,] 0.004 -0.015 0.000 0.014 ## [4,] 0.001 -0.015 0.005 0.013 ## [5,] 0.012 -0.010 -0.011 0.010 ## [6,] 0.018 -0.002 -0.017 -0.001 # можно получить сами значения age в заданных степенях round(head(poly(age, 4, raw = T)), 3) ## 1 2 3 4 ## [1,] 18 324 5832 104976 ## [2,] 24 576 13824 331776 ## [3,] 45 2025 91125 4100625 ## [4,] 43 1849 79507 3418801 ## [5,] 50 2500 125000 6250000 ## [6,] 54 2916 157464 8503056 # на прогноз не повлияет, но оценки параметров изменяются fit.2 &lt;- lm(wage ~ poly(age, 4, raw = T), data = Wage) round(coef(summary(fit.2)), 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -184.15 60.04 -3.07 0.00 ## poly(age, 4, raw = T)1 21.25 5.89 3.61 0.00 ## poly(age, 4, raw = T)2 -0.56 0.21 -2.74 0.01 ## poly(age, 4, raw = T)3 0.01 0.00 2.22 0.03 ## poly(age, 4, raw = T)4 0.00 0.00 -1.95 0.05 # границы изменения переменной age agelims &lt;- range(age) # значения age, для которых делаем прогноз (от min до max с шагом 1) age.grid &lt;- seq(from = agelims[1], to = agelims[2]) # рассчитать прогнозы и их стандартные ошибки preds &lt;- predict(fit, newdata = list(age = age.grid), se = T) # границы доверительного интервала для заработной платы se.bands &lt;- cbind(lower.bound = preds$fit - 2*preds$se.fit, upper.bound = preds$fit + 2*preds$se.fit) # смотрим результат round(head(se.bands), 2) ## lower.bound upper.bound ## 1 41.33 62.53 ## 2 49.76 67.24 ## 3 57.39 71.76 ## 4 64.27 76.09 ## 5 70.44 80.27 ## 6 75.94 84.28 Рисуем левую панель графика со слайда 4 презентации (рис. 7.1 книги). Функция matlines() рисует грфик столбцов одной матрицы против столбцов другой. # наблюдения plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) # заголовок title(&#39;Полином четвёртой степени&#39;) # модель lines(age.grid, preds$fit, lwd = 2, col = &#39;blue&#39;) # доверительные интервалы прогноза matlines(x = age.grid, y = se.bands, lwd = 1, col = &#39;blue&#39;, lty = 3) Убедимся, что прогнозы по моделям с различными вызовами poly() совпадают. # прогнозы по второму вызову модели preds2 &lt;- predict(fit.2, newdata = list(age = age.grid), se = T) # максимальное расхождение между прогнозами по двум вариантам вызова модели max(abs(preds$fit - preds2$fit)) ## [1] 7.389644e-13 Теперь подбираем степень полинома, сравнивая модели со степенями от 1 до 5 с помощью дисперсионного анализа (ANOVA). fit.1 &lt;- lm(wage ~ age, data = Wage) fit.2 &lt;- lm(wage ~ poly(age, 2), data = Wage) fit.3 &lt;- lm(wage ~ poly(age, 3), data = Wage) fit.4 &lt;- lm(wage ~ poly(age, 4), data = Wage) fit.5 &lt;- lm(wage ~ poly(age, 5), data = Wage) round(anova(fit.1, fit.2, fit.3, fit.4, fit.5), 2) Res.Df &lt;dbl&gt; RSS &lt;dbl&gt; Df &lt;dbl&gt; Sum of Sq &lt;dbl&gt; F &lt;dbl&gt; Pr(&gt;F) &lt;dbl&gt; 2998 5022216 NA NA NA NA 2997 4793430 1 228786.01 143.59 0.00 2996 4777674 1 15755.69 9.89 0.00 2995 4771604 1 6070.15 3.81 0.05 2994 4770322 1 1282.56 0.80 0.37 5 rows Рассматриваются пять моделей, в которых степени полинома от age идут по возрастанию. В крайнем правом столбце таблице приводятся p-значения для проверки нулевой гипотезы: текущая модель не даёт статистически значимого сокращения RSS по сравнению с предыдущей моделью. Можно сделать вывод, что степени 3 достаточно, дальнейшее увеличение степени не даёт значимого улучшения качества модели. Зависимость вероятности получать зарплату &gt; 250 от возраста Теперь вернёмся к группе наблюдений с высоким wage. Рассмотрим зависимость вероятности того, что величина зарплаты больше 250, от возраста. Подгоняем логистическую регрессию и делаем прогнозы, для этого используем функцию для оценки обобщённой линейной модели glm() и указываем тип модели binomial: fit &lt;- glm(I(wage &gt; 250) ~ poly(age, 4), data = Wage, family = &#39;binomial&#39;) # прогнозы preds &lt;- predict(fit, newdata = list(age = age.grid), se = T) # пересчитываем доверительные интервалы и прогнозы в исходные ЕИ pfit &lt;- exp(preds$fit) / (1 + exp(preds$fit)) se.bands.logit &lt;- cbind(lower.bound = preds$fit - 2*preds$se.fit, upper.bound = preds$fit + 2*preds$se.fit) se.bands &lt;- exp(se.bands.logit)/(1 + exp(se.bands.logit)) # результат - доверительный интервал для вероятности события # &quot;Заработная плата выше 250&quot;. round(head(se.bands), 3) ## lower.bound upper.bound ## 1 0 0.002 ## 2 0 0.003 ## 3 0 0.004 ## 4 0 0.005 ## 5 0 0.006 ## 6 0 0.007 Достраиваем график с 4 слайда презентации (рис. 7.1 книги). Рисуем правую панель. # сетка для графика (изображаем вероятности, поэтому интервал изменения y мал) plot(age, I(wage &gt; 250), xlim = agelims, type = &#39;n&#39;, ylim = c(0, 0.2), ylab = &#39;P(Wage &gt; 250 | Age)&#39;) # фактические наблюдения показываем засечками points(jitter(age), I((wage &gt; 250) / 5), cex = 0.5, pch = &#39;|&#39;, col = &#39;darkgrey&#39;) # модель lines(age.grid, pfit, lwd = 2, col = &#39;blue&#39;) # доверительные интервалы matlines(age.grid, se.bands, lwd = 1, col = &#39;blue&#39;, lty = 3) # заголовок title(&#39;Полином четвёртой степени&#39;) Ступенчатые функции Для начала определим несколько интервалов, на каждом из которых будем моделировать зависимость wage от age своим средним уровнем. # нарезаем предиктор age на 4 равных интервала table(cut(age, 4)) ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 # подгоняем линейную модель на интервалах fit &lt;- lm(wage ~ cut(age, 4), data = Wage) round(coef(summary(fit)), 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 94.16 1.48 63.79 0.00 ## cut(age, 4)(33.5,49] 24.05 1.83 13.15 0.00 ## cut(age, 4)(49,64.5] 23.66 2.07 11.44 0.00 ## cut(age, 4)(64.5,80.1] 7.64 4.99 1.53 0.13 # прогноз -- это средние по `wage` на каждом интервале preds.cut &lt;- predict(fit, newdata = list(age = age.grid), se = T) # интервальный прогноз se.bands.cut &lt;- cbind(lower.bound = preds.cut$fit - 2*preds.cut$se.fit, upper.bound = preds.cut$fit + 2*preds.cut$se.fit) Воспроизведём график со слайда 7 презентации (рис. 7.2 книги). # наблюдения plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) # модель lines(age.grid, preds.cut$fit, lwd = 2, col = &#39;darkgreen&#39;) # доверительные интервалы прогноза matlines(x = age.grid, y = se.bands.cut, lwd = 1, col = &#39;darkgreen&#39;, lty = 3) # заголовок title(&#39;Ступенчатая функция&#39;) Правая часть графика, для вероятности того, что зарплата выше 250. fit &lt;- glm(I(wage &gt; 250) ~ cut(age, 4), data = Wage, family = &#39;binomial&#39;) # прогнозы preds &lt;- predict(fit, newdata = list(age = age.grid), se = T) # пересчитываем доверительные интервалы и прогнозы в исходные ЕИ pfit &lt;- exp(preds$fit) / (1 + exp(preds$fit)) se.bands.logit &lt;- cbind(lower.bound = preds$fit - 2*preds$se.fit, upper.bound = preds$fit + 2*preds$se.fit) se.bands &lt;- exp(se.bands.logit)/(1 + exp(se.bands.logit)) # результат - доверительный интервал для вероятности события # &quot;Заработная плата выше 250&quot;. round(head(se.bands), 3) ## lower.bound upper.bound ## 1 0.003 0.016 ## 2 0.003 0.016 ## 3 0.003 0.016 ## 4 0.003 0.016 ## 5 0.003 0.016 ## 6 0.003 0.016 # сетка для графика (изображаем вероятности, поэтому интервал изменения y мал) plot(age, I(wage &gt; 250), xlim = agelims, type = &#39;n&#39;, ylim = c(0, 0.2), ylab = &#39;P(Wage &gt; 250 | Age)&#39;) # фактические наблюдения показываем засечками points(jitter(age), I((wage &gt; 250) / 5), cex = 0.5, pch = &#39;|&#39;, col = &#39;darkgrey&#39;) # модель lines(age.grid, pfit, lwd = 2, col = &#39;darkgreen&#39;) # доверительные интервалы matlines(age.grid, se.bands, lwd = 1, col = &#39;darkgreen&#39;, lty = 3) # заголовок title(&#39;Ступенчатая функция&#39;) Сплайны Построим кубический сплайн с тремя узлами. # кубический сплайн с тремя узлами fit &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage) # прогноз preds.spl &lt;- predict(fit, newdata = list(age = age.grid), se = T) Теперь построим натуральный по трём узлам. Три узла это 6 степеней свободы. Если функции bs(), которая создаёт матрицу с базисом для полиномиального сплайна, передать только степени свободы, она распределит узлы равномерно. В данном случае это квартили распределения age. # 3 узла -- 6 степеней свободы (столбцы матрицы) dim(bs(age, knots = c(25, 40, 60))) ## [1] 3000 6 # если не указываем узлы явно... dim(bs(age, df = 6)) ## [1] 3000 6 # они привязываются к квартилям attr(bs(age, df = 6), &#39;knots&#39;) ## 25% 50% 75% ## 33.75 42.00 51.00 # натуральный сплайн fit2 &lt;- lm(wage ~ ns(age, df = 4), data = Wage) preds.spl2 &lt;- predict(fit2, newdata = list(age = age.grid), se = T) График сравнения кубического и натурального сплайнов. par(mfrow = c(1, 1), mar = c(4.5, 4.5, 1, 8.5), oma = c(0, 0, 0, 0), xpd = T) # наблюдения plot(age, wage, col = &#39;grey&#39;) # модель кубического сплайна lines(age.grid, preds.spl$fit, lwd = 2) # доверительный интервал lines(age.grid, preds.spl$fit + 2*preds.spl$se, lty = &#39;dashed&#39;) lines(age.grid, preds.spl$fit - 2*preds.spl$se, lty = &#39;dashed&#39;) # натуральный сплайн lines(age.grid, preds.spl2$fit, col = &#39;red&#39;, lwd = 2) # легенда legend(&quot;topright&quot;, inset = c(-0.7, 0), c(&#39;Кубический \\n с 3 узлами&#39;, &#39;Натуральный&#39;), lwd = rep(2, 2), col = c(&#39;black&#39;, &#39;red&#39;)) # заголовок title(&quot;Сплайны&quot;) Построим график со слайда 20 (рисунок 7.8 книги). par(mfrow = c(1, 1), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0)) # наблюдения plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) # заголовок title(&#39;Сглаживающий сплайн&#39;) # подгоняем модель с 16 степенями свободы fit &lt;- smooth.spline(age, wage, df = 16) # подгоняем модель с подбором лямбды с помощью перекрёстной проверки fit2 &lt;- smooth.spline(age, wage, cv = T) ## Warning in smooth.spline(age, wage, cv = T): cross-validation with non- ## unique &#39;x&#39; values seems doubtful fit2$df ## [1] 6.794596 # рисуем модель lines(fit, col = &#39;red&#39;, lwd = 2) lines(fit2, col = &#39;blue&#39;, lwd = 2) legend(&#39;topright&#39;, c(&#39;16 df&#39;, &#39;6.8 df&#39;), col = c(&#39;red&#39;, &#39;blue&#39;), lty = 1, lwd = 2, cex = 0.8) Локальная регрессия Строим график со слайда 24 (рис. 7.10). plot(age, wage, xlim = agelims, cex = 0.5, col = &#39;darkgrey&#39;) title(&#39;Локальная регрессия&#39;) # подгоняем модель c окном 0.2 fit &lt;- loess(wage ~ age, span = 0.2, data = Wage) # подгоняем модель c окном 0.5 fit2 &lt;- loess(wage ~ age, span = 0.5, data = Wage) # рисум модели lines(age.grid, predict(fit, data.frame(age = age.grid)), col = &#39;red&#39;, lwd = 2) lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = &#39;blue&#39;, lwd = 2) # легенда legend(&#39;topright&#39;, c(&#39;s = 0.2&#39;, &#39;s = 0.5&#39;), col = c(&#39;red&#39;, &#39;blue&#39;), lty = 1, lwd = 2, cex = 0.8) Обобщённые аддитивные модели (GAM) с непрерывным откликом Построим GAM на натуральных сплайнах степеней 4 (year), 5 (age) с категориальным предиктором edication. # GAM на натуральных сплайнах gam.ns &lt;- gam(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage) Также построим модель на сглаживающих сплайнах. # GAM на сглаживающих сплайнах gam.m3 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage) График со слайда 28 (рис. 7.12). par(mfrow = c(1, 3)) plot(gam.m3, se = T, col = &#39;blue&#39;) График со слайда 27 (рис. 7.11). par(mfrow = c(1, 3)) plot(gam.ns, se = T, col = &#39;red&#39;) График функции от year похож на прямую. Сделаем ANOVA, чтобы понять, какая степень для year лучше. gam.m1 &lt;- gam(wage ~ s(age, 5) + education, data = Wage) # без year gam.m2 &lt;- gam(wage ~ year + s(age, 5) + education, data = Wage) # year^1 anova(gam.m1, gam.m2, gam.m3, test = &#39;F&#39;) Resid. Df &lt;dbl&gt; Resid. Dev &lt;dbl&gt; Df &lt;dbl&gt; Deviance &lt;dbl&gt; F &lt;dbl&gt; Pr(&gt;F) &lt;dbl&gt; 2990 3711731 NA NA NA NA 2989 3693842 1.000000 17889.243 14.477130 0.0001447167 2986 3689770 2.999989 4071.134 1.098212 0.3485661430 3 rows Третья модель статистически не лучше второй. Кроме того, один из параметров этой модели незначим. # сводка по модели gam.m3 summary(gam.m3) ## ## Call: gam(formula = wage ~ s(year, 4) + s(age, 5) + education, data = Wage) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -119.43 -19.70 -3.33 14.17 213.48 ## ## (Dispersion Parameter for gaussian family taken to be 1235.69) ## ## Null Deviance: 5222086 on 2999 degrees of freedom ## Residual Deviance: 3689770 on 2986 degrees of freedom ## AIC: 29887.75 ## ## Number of Local Scoring Iterations: 2 ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## s(year, 4) 1 27162 27162 21.981 2.877e-06 *** ## s(age, 5) 1 195338 195338 158.081 &lt; 2.2e-16 *** ## education 4 1069726 267432 216.423 &lt; 2.2e-16 *** ## Residuals 2986 3689770 1236 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## s(year, 4) 3 1.086 0.3537 ## s(age, 5) 4 32.380 &lt;2e-16 *** ## education ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Работаем с моделью gam.m2. # прогноз по обучающей выборке preds &lt;- predict(gam.m2, newdata = Wage) Также можно использовать в GAM локальные регрессии. # GAM на локальных регрессиях gam.lo &lt;- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, data = Wage) par(mfrow = c(1, 3)) plot.gam(gam.lo, se = T, col = &#39;green&#39;) # модель со взаимодействием регрессоров year и age gam.lo.i &lt;- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : liv too small. (Discovered by lowesd) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : lv too small. (Discovered by lowesd) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : liv too small. (Discovered by lowesd) ## Warning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : lv too small. (Discovered by lowesd) plot(gam.lo.i) Логистическая GAM Построим логистическую GAM для всероятности того, что wage превышает 250. gam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age, df = 5) + education, family = &#39;binomial&#39;, data = Wage) par(mfrow = c(1, 3)) plot(gam.lr, se = T, col = &#39;green&#39;) # уровни образования по группам разного достатка table(education, I(wage &gt; 250)) ## ## education FALSE TRUE ## 1. &lt; HS Grad 268 0 ## 2. HS Grad 966 5 ## 3. Some College 643 7 ## 4. College Grad 663 22 ## 5. Advanced Degree 381 45 В категории с самым низким уровнем образования нет wage &gt; 250, поэтому убираем её. gam.lr.s &lt;- gam(I(wage &gt; 250) ~ year + s(age, df = 5) + education, family = &#39;binomial&#39;, data = Wage, subset = (education != &quot;1. &lt; HS Grad&quot;)) # график par(mfrow = c(1, 3)) plot(gam.lr.s, se = T, col = &#39;green&#39;) detach(Wage) # Nonlinear modeling Математическое моделирование Практика 8 Нелинейные модели В практических примерах ниже показано как: строить регрессионные деревья; строить деревья классификации; делать обрезку дерева; использовать бэггинг, бустинг, случайный лес для улучшения качества прогнозирования. Модели: деревья решений. Данные: Sales {ISLR}, Boston {ISLR} Подробные комментарии к коду лабораторных см. в [1], глава 8. library(&#39;tree&#39;) # деревья ## Warning: package &#39;tree&#39; was built under R version 3.4.4 library(&#39;ISLR&#39;) # наборы данных library(&#39;MASS&#39;) library(&#39;randomForest&#39;) # случайный лес ## Warning: package &#39;randomForest&#39; was built under R version 3.4.4 ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. library(&#39;gbm&#39;) ## Warning: package &#39;gbm&#39; was built under R version 3.4.4 ## Loading required package: survival ## Loading required package: lattice ## Loading required package: splines ## Loading required package: parallel ## Loaded gbm 2.1.3 Деревья решений Загрузим таблицу с данными по продажам детских кресел и добавим к ней переменную High – “высокие продажи” со значениями: Yes если продажи больше 8 (тыс. шт.); No в противном случае. ?Carseats ## starting httpd help server ... done attach(Carseats) # новая переменная High &lt;- ifelse(Sales &lt;= 8, &quot;No&quot;, &quot;Yes&quot;) # присоединяем к таблице данных Carseats &lt;- data.frame(Carseats, High) Строим дерево для категориального отклика High, отбросив непрерывный отклик Sales. # модель бинарного дерева tree.carseats &lt;- tree(High ~ . -Sales, Carseats) summary(tree.carseats) ## ## Classification tree: ## tree(formula = High ~ . - Sales, data = Carseats) ## Variables actually used in tree construction: ## [1] &quot;ShelveLoc&quot; &quot;Price&quot; &quot;Income&quot; &quot;CompPrice&quot; &quot;Population&quot; ## [6] &quot;Advertising&quot; &quot;Age&quot; &quot;US&quot; ## Number of terminal nodes: 27 ## Residual mean deviance: 0.4575 = 170.7 / 373 ## Misclassification error rate: 0.09 = 36 / 400 # график результата plot(tree.carseats) # ветви text(tree.carseats, pretty=0) # подписи tree.carseats # посмотреть всё дерево в консоли ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 400 541.500 No ( 0.59000 0.41000 ) ## 2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 ) ## 4) Price &lt; 92.5 46 56.530 Yes ( 0.30435 0.69565 ) ## 8) Income &lt; 57 10 12.220 No ( 0.70000 0.30000 ) ## 16) CompPrice &lt; 110.5 5 0.000 No ( 1.00000 0.00000 ) * ## 17) CompPrice &gt; 110.5 5 6.730 Yes ( 0.40000 0.60000 ) * ## 9) Income &gt; 57 36 35.470 Yes ( 0.19444 0.80556 ) ## 18) Population &lt; 207.5 16 21.170 Yes ( 0.37500 0.62500 ) * ## 19) Population &gt; 207.5 20 7.941 Yes ( 0.05000 0.95000 ) * ## 5) Price &gt; 92.5 269 299.800 No ( 0.75465 0.24535 ) ## 10) Advertising &lt; 13.5 224 213.200 No ( 0.81696 0.18304 ) ## 20) CompPrice &lt; 124.5 96 44.890 No ( 0.93750 0.06250 ) ## 40) Price &lt; 106.5 38 33.150 No ( 0.84211 0.15789 ) ## 80) Population &lt; 177 12 16.300 No ( 0.58333 0.41667 ) ## 160) Income &lt; 60.5 6 0.000 No ( 1.00000 0.00000 ) * ## 161) Income &gt; 60.5 6 5.407 Yes ( 0.16667 0.83333 ) * ## 81) Population &gt; 177 26 8.477 No ( 0.96154 0.03846 ) * ## 41) Price &gt; 106.5 58 0.000 No ( 1.00000 0.00000 ) * ## 21) CompPrice &gt; 124.5 128 150.200 No ( 0.72656 0.27344 ) ## 42) Price &lt; 122.5 51 70.680 Yes ( 0.49020 0.50980 ) ## 84) ShelveLoc: Bad 11 6.702 No ( 0.90909 0.09091 ) * ## 85) ShelveLoc: Medium 40 52.930 Yes ( 0.37500 0.62500 ) ## 170) Price &lt; 109.5 16 7.481 Yes ( 0.06250 0.93750 ) * ## 171) Price &gt; 109.5 24 32.600 No ( 0.58333 0.41667 ) ## 342) Age &lt; 49.5 13 16.050 Yes ( 0.30769 0.69231 ) * ## 343) Age &gt; 49.5 11 6.702 No ( 0.90909 0.09091 ) * ## 43) Price &gt; 122.5 77 55.540 No ( 0.88312 0.11688 ) ## 86) CompPrice &lt; 147.5 58 17.400 No ( 0.96552 0.03448 ) * ## 87) CompPrice &gt; 147.5 19 25.010 No ( 0.63158 0.36842 ) ## 174) Price &lt; 147 12 16.300 Yes ( 0.41667 0.58333 ) ## 348) CompPrice &lt; 152.5 7 5.742 Yes ( 0.14286 0.85714 ) * ## 349) CompPrice &gt; 152.5 5 5.004 No ( 0.80000 0.20000 ) * ## 175) Price &gt; 147 7 0.000 No ( 1.00000 0.00000 ) * ## 11) Advertising &gt; 13.5 45 61.830 Yes ( 0.44444 0.55556 ) ## 22) Age &lt; 54.5 25 25.020 Yes ( 0.20000 0.80000 ) ## 44) CompPrice &lt; 130.5 14 18.250 Yes ( 0.35714 0.64286 ) ## 88) Income &lt; 100 9 12.370 No ( 0.55556 0.44444 ) * ## 89) Income &gt; 100 5 0.000 Yes ( 0.00000 1.00000 ) * ## 45) CompPrice &gt; 130.5 11 0.000 Yes ( 0.00000 1.00000 ) * ## 23) Age &gt; 54.5 20 22.490 No ( 0.75000 0.25000 ) ## 46) CompPrice &lt; 122.5 10 0.000 No ( 1.00000 0.00000 ) * ## 47) CompPrice &gt; 122.5 10 13.860 No ( 0.50000 0.50000 ) ## 94) Price &lt; 125 5 0.000 Yes ( 0.00000 1.00000 ) * ## 95) Price &gt; 125 5 0.000 No ( 1.00000 0.00000 ) * ## 3) ShelveLoc: Good 85 90.330 Yes ( 0.22353 0.77647 ) ## 6) Price &lt; 135 68 49.260 Yes ( 0.11765 0.88235 ) ## 12) US: No 17 22.070 Yes ( 0.35294 0.64706 ) ## 24) Price &lt; 109 8 0.000 Yes ( 0.00000 1.00000 ) * ## 25) Price &gt; 109 9 11.460 No ( 0.66667 0.33333 ) * ## 13) US: Yes 51 16.880 Yes ( 0.03922 0.96078 ) * ## 7) Price &gt; 135 17 22.070 No ( 0.64706 0.35294 ) ## 14) Income &lt; 46 6 0.000 No ( 1.00000 0.00000 ) * ## 15) Income &gt; 46 11 15.160 Yes ( 0.45455 0.54545 ) * Теперь построим дерево на обучающей выборке и оценим ошибку на тестовой. # ядро генератора случайных чисел set.seed(2) # обучающая выборка train &lt;- sample(1:nrow(Carseats), 200) # тестовая выборка Carseats.test &lt;- Carseats[-train,] High.test &lt;- High[-train] # строим дерево на обучающей выборке tree.carseats &lt;- tree(High ~ . -Sales, Carseats, subset = train) # делаем прогноз tree.pred &lt;- predict(tree.carseats, Carseats.test, type = &quot;class&quot;) # матрица неточностей tbl &lt;- table(tree.pred, High.test) tbl ## High.test ## tree.pred No Yes ## No 86 27 ## Yes 30 57 # оценка точности acc.test &lt;- sum(diag(tbl))/sum(tbl) acc.test ## [1] 0.715 Обобщённая характеристика точности: доля верных прогнозов: 0.72. Теперь обрезаем дерево, используя в качестве критерия частоту ошибок классификации. Функция cv.tree() проводит кросс-валидацию для выбора лучшего дерева, аргумент prune.misclass означает, что мы минимизируем ошибку классификации. set.seed(3) cv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass) # имена элементов полученного объекта names(cv.carseats) ## [1] &quot;size&quot; &quot;dev&quot; &quot;k&quot; &quot;method&quot; # сам объект cv.carseats ## $size ## [1] 19 17 14 13 9 7 3 2 1 ## ## $dev ## [1] 55 55 53 52 50 56 69 65 80 ## ## $k ## [1] -Inf 0.0000000 0.6666667 1.0000000 1.7500000 2.0000000 ## [7] 4.2500000 5.0000000 23.0000000 ## ## $method ## [1] &quot;misclass&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;prune&quot; &quot;tree.sequence&quot; # графики изменения параметров метода по ходу обрезки дерева ################### # 1. ошибка с кросс-валидацией в зависимости от числа узлов par(mfrow = c(1, 2)) plot(cv.carseats$size, cv.carseats$dev, type = &quot;b&quot;, ylab = &#39;Частота ошибок с кросс-вал. (dev)&#39;, xlab = &#39;Число узлов (size)&#39;) # размер дерева с минимальной ошибкой opt.size &lt;- cv.carseats$size[cv.carseats$dev == min(cv.carseats$dev)] abline(v = opt.size, col = &#39;red&#39;, &#39;lwd&#39; = 2) # соотв. вертикальная прямая mtext(opt.size, at = opt.size, side = 1, col = &#39;red&#39;, line = 1) # 2. ошибка с кросс-валидацией в зависимости от штрафа на сложность plot(cv.carseats$k, cv.carseats$dev, type = &quot;b&quot;, ylab = &#39;Частота ошибок с кросс-вал. (dev)&#39;, xlab = &#39;Штраф за сложность (k)&#39;) Как видно на графике слева, минимум частоты ошибок достигается при числе узлов 9. Оценим точность дерева с 9 узлами. # дерево с 9 узлами prune.carseats &lt;- prune.misclass(tree.carseats, best = 9) # визуализация plot(prune.carseats) text(prune.carseats, pretty = 0) # прогноз на тестовую выборку tree.pred &lt;- predict(prune.carseats, Carseats.test, type = &quot;class&quot;) # матрица неточностей tbl &lt;- table(tree.pred, High.test) tbl ## High.test ## tree.pred No Yes ## No 94 24 ## Yes 22 60 # оценка точности acc.test &lt;- sum(diag(tbl))/sum(tbl) acc.test ## [1] 0.77 Точность этой модели чуть выше точности исходного дерева и составляет 0.77. Увеличив количество узлов, получим более глубокое дерево, но менее точное. # дерево с 13 узлами prune.carseats &lt;- prune.misclass(tree.carseats, best = 15) # визуализация plot(prune.carseats) text(prune.carseats, pretty = 0) # прогноз на тестовую выборку tree.pred &lt;- predict(prune.carseats, Carseats.test, type = &quot;class&quot;) # матрица неточностей tbl &lt;- table(tree.pred, High.test) tbl ## High.test ## tree.pred No Yes ## No 86 22 ## Yes 30 62 # оценка точности acc.test &lt;- sum(diag(tbl))/sum(tbl) acc.test ## [1] 0.74 # сбрасываем графические параметры par(mfrow = c(1, 1)) Регрессионные деревья Воспользуемся набором данных Boston. ?Boston # обучающая выборка set.seed(1) train &lt;- sample(1:nrow(Boston), nrow(Boston)/2) # обучающая выборка -- 50% Построим дерево регрессии для зависимой переменной medv: медианная стоимости домов, в которых живут собственники (тыс. долл.). # обучаем модель tree.boston &lt;- tree(medv ~ ., Boston, subset = train) summary(tree.boston) ## ## Regression tree: ## tree(formula = medv ~ ., data = Boston, subset = train) ## Variables actually used in tree construction: ## [1] &quot;lstat&quot; &quot;rm&quot; &quot;dis&quot; ## Number of terminal nodes: 8 ## Residual mean deviance: 12.65 = 3099 / 245 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -14.10000 -2.04200 -0.05357 0.00000 1.96000 12.60000 # визуализация plot(tree.boston) text(tree.boston, pretty = 0) Снова сделаем обрезку дерева в целях улучшения качества прогноза. cv.boston &lt;- cv.tree(tree.boston) # размер дерева с минимальной ошибкой plot(cv.boston$size, cv.boston$dev, type = &#39;b&#39;) opt.size &lt;- cv.boston$size[cv.boston$dev == min(cv.boston$dev)] abline(v = opt.size, col = &#39;red&#39;, &#39;lwd&#39; = 2) # соотв. вертикальная прямая mtext(opt.size, at = opt.size, side = 1, col = &#39;red&#39;, line = 1) В данном случаем минимум ошибки соответствует самому сложному дереву, с 8 узлами. Покажем, как при желании можно обрезать дерево до 7 узлов (ошибка ненамного выше, чем минимальная). # дерево с 7 узлами prune.boston = prune.tree(tree.boston, best = 7) # визуализация plot(prune.boston) text(prune.boston, pretty = 0) Прогноз сделаем по необрезанному дереву, т.к. там ошибка, оцененная по методу перекрёстной проверки, минимальна. # прогноз по лучшей модели (8 узлов) yhat &lt;- predict(tree.boston, newdata = Boston[-train, ]) boston.test &lt;- Boston[-train, &quot;medv&quot;] # график &quot;прогноз -- реализация&quot; plot(yhat, boston.test) # линия идеального прогноза abline(0, 1) # MSE на тестовой выборке mse.test &lt;- mean((yhat - boston.test)^2) MSE на тестовой выборке равна 25.05 (тыс.долл.). Бэггинг и метод случайного леса Рассмотрим более сложные методы улучшения качества дерева. Бэггинг – частный случай случайного леса с m=p, поэтому и то, и другое можно построить функцией randomForest(). Для начала используем бэггинг, причём возьмём все 13 предикторов на каждом шаге (аргумент mtry). # бэггинг с 13 предикторами set.seed(1) bag.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE) bag.boston ## ## Call: ## randomForest(formula = medv ~ ., data = Boston, mtry = 13, importance = TRUE, subset = train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 11.15723 ## % Var explained: 86.49 # прогноз yhat.bag = predict(bag.boston, newdata = Boston[-train, ]) # график &quot;прогноз -- реализация&quot; plot(yhat.bag, boston.test) # линия идеального прогноза abline(0, 1) # MSE на тестовой mse.test &lt;- mean((yhat.bag - boston.test)^2) mse.test ## [1] 13.50808 Ошибка на тестовой выборке равна 13.51. Можно изменить число деревьев с помощью аргумента ntree. bag.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, ntree = 25) # прогноз yhat.bag &lt;- predict(bag.boston, newdata = Boston[-train, ]) # MSE на тестовой mse.test &lt;- mean((yhat.bag - boston.test)^2) mse.test ## [1] 13.94835 Но, как видно, это только ухудшает прогноз. Теперь попробуем вырастить случайный лес. Берём 6 предикторов на каждом шаге. # обучаем модель set.seed(1) rf.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE) # прогноз yhat.rf &lt;- predict(rf.boston, newdata = Boston[-train, ]) # MSE на тестовой выборке mse.test &lt;- mean((yhat.rf - boston.test)^2) # важность предикторов importance(rf.boston) # оценки ## %IncMSE IncNodePurity ## crim 12.132320 986.50338 ## zn 1.955579 57.96945 ## indus 9.069302 882.78261 ## chas 2.210835 45.22941 ## nox 11.104823 1044.33776 ## rm 31.784033 6359.31971 ## age 10.962684 516.82969 ## dis 15.015236 1224.11605 ## rad 4.118011 95.94586 ## tax 8.587932 502.96719 ## ptratio 12.503896 830.77523 ## black 6.702609 341.30361 ## lstat 30.695224 7505.73936 varImpPlot(rf.boston) # графики Ошибка по модели случайного леса равна 11.66, что ниже, чем для бэггинга. Бустинг Построим 5000 регрессионных деревьев с глубиной 4. set.seed(1) boost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4) # график и таблица относительной важности переменных summary(boost.boston) # графики частной зависимости для двух наиболее важных предикторов par(mfrow = c(1, 2)) plot(boost.boston, i = &quot;rm&quot;) plot(boost.boston, i = &quot;lstat&quot;) # прогноз yhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000) # MSE на тестовой mse.test &lt;- mean((yhat.boost - boston.test)^2) mse.test ## [1] 11.84434 Настройку бустинга можно делать с помощью гиперпараметра λ (аргумент shrinkage). Установим его равным 0.2. # меняем значение гиперпараметра (lambda) на 0.2 -- аргумент shrinkage boost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F) # прогноз yhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000) # MSE а тестовой mse.test &lt;- mean((yhat.boost - boston.test)^2) mse.test ## [1] 11.51109 Таким образом, изменив гиперпараметр, мы ещё немного снизили ошибку прогноза. "],["spline-model.html", "Chapter 22 Spline model 22.1 Generate dataset from a given function 22.2 Split data for train and test 22.3 Diagram of the given function and generated datasets 22.4 Build a model using splines 22.5 Diagram of MSE for train and test data 22.6 Build optimal model and plot for the model 22.7 Bibliograpy", " Chapter 22 Spline model In this example we will generate data from a given function and then build a model using splines and estimate quality of the model. 22.1 Generate dataset from a given function # parameters to generate a dataset n.all &lt;- 100 # number of observations train.percent &lt;- 0.85 # portion of the data for training res.sd &lt;- 1 # standard deviation of noise x.min &lt;- 5 # min limit of the data x.max &lt;- 105 # max limit of the data # generate x set.seed(1) # to get reproducible results by randomizer x &lt;- runif(x.min, x.max, n = n.all) # noise from normal destibution set.seed(1) res &lt;- rnorm(mean = 0, sd = res.sd, n = n.all) # generate y using a given function y.func &lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3} # add noise y &lt;- y.func(x) + res 22.2 Split data for train and test # split dataset for training and test set.seed(1) # generate vector of chosen x for train data inTrain &lt;- sample(seq_along(x), size = train.percent*n.all) # train data set x.train &lt;- x[inTrain] y.train &lt;- y[inTrain] # test data set x.test &lt;- x[-inTrain] y.test &lt;- y[-inTrain] 22.3 Diagram of the given function and generated datasets # lines of generated data for plot x.line &lt;- seq(x.min, x.max, length = n.all) y.line &lt;- y.func(x.line) # PLOT # generate plot by train data par(mar = c(4, 4, 1, 1)) # reduce margins (optional) plot(x.train, y.train, main = &#39;Generated data and original function&#39;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points of test data points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # add the given function lines(x.line, y.line, lwd = 2, lty = 2) # add legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;), pch = c(16, 16, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;), lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2) 22.4 Build a model using splines We will compair sevaral models with degree of freedoms (df) from 2 to 40, where 2 correspond to a linear model. max.df &lt;- 40 # max degree of freedom (df) # tbl &lt;- data.frame(df = 2:max.df) # data frame for writing errors tbl$MSE.train &lt;- 0 # column 1: errors of train data tbl$MSE.test &lt;- 0 # сcolumn 2: errors of test data # generate models using for cycle for (i in 2:max.df) { mod &lt;- smooth.spline(x = x.train, y = y.train, df = i) # predicted values for train and test data using built model y.model.train &lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test &lt;- predict(mod, data.frame(x = x.test))$y[, 1] # MSE errors for train and test data MSE &lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) # write errors to the previously created data frame tbl[tbl$df == i, c(&#39;MSE.train&#39;, &#39;MSE.test&#39;)] &lt;- MSE } # view first rows of the table head(tbl, 4) ## df MSE.train MSE.test ## 1 2 3.6484333 3.3336892 ## 2 3 1.5185881 1.1532857 ## 3 4 0.8999800 0.8874002 ## 4 5 0.7477105 0.9483290 22.5 Diagram of MSE for train and test data # plot MSE from our table plot(x = tbl$df, y = tbl$MSE.test, main = &quot;Changes of MSE from degrees of freedom&quot;, type = &#39;l&#39;, col = &#39;red&#39;, lwd = 2, xlab = &#39;spline degree of freedom&#39;, ylab = &#39;MSE&#39;, ylim = c(min(tbl$MSE.train, tbl$MSE.test), max(tbl$MSE.train, tbl$MSE.test)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points(x = tbl$df, y = tbl$MSE.test, pch = 21, col = &#39;red&#39;, bg = &#39;red&#39;) lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) # minimal MSE abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) # add legend legend(&#39;topright&#39;, legend = c(&#39;train&#39;, &#39;test&#39;), pch = c(NA, 16), col = c(grey(0.2), &#39;red&#39;), lty = c(1, 1), lwd = c(2, 2), cex = 1.2) # df of minimal MSE for test data min.MSE.test &lt;- min(tbl$MSE.test) df.min.MSE.test &lt;- tbl[tbl$MSE.test == min.MSE.test, &#39;df&#39;] # optimal df for precise model and maximal simplicity df.my.MSE.test &lt;- 6 my.MSE.test &lt;- tbl[tbl$df == df.my.MSE.test, &#39;MSE.test&#39;] # show the optimal solution abline(v = df.my.MSE.test, lty = 2, lwd = 2) points(x = df.my.MSE.test, y = my.MSE.test, pch = 15, col = &#39;blue&#39;) mtext(df.my.MSE.test, side = 1, line = -1, at = df.my.MSE.test, col = &#39;blue&#39;, cex = 1.2) 22.6 Build optimal model and plot for the model mod.MSE.test &lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) # predict data for 250 x&#39;s to get smoothed curve x.model.plot &lt;- seq(x.min, x.max, length = 250) y.model.plot &lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] # plot train data par(mar = c(4, 4, 1, 1)) plot(x.train, y.train, main = &quot;Initial data and the best fit model&quot;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add test data points(x.test, y.test, col = &#39;red&#39;, bg = &#39;red&#39;, pch = 21) # function lines(x.line, y.line,lwd = 2, lty = 2) # add model lines(x.model.plot, y.model.plot, lwd = 2, col = &#39;blue&#39;) # legend legend(&#39;topleft&#39;, legend = c(&#39;train&#39;, &#39;test&#39;, &#39;f(X)&#39;, &#39;model&#39;), pch = c(16, 16, NA, NA), col = c(grey(0.2), &#39;red&#39;, &#39;black&#39;, &#39;blue&#39;), lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2) 22.7 Bibliograpy An Introduction to Statistical Learning by Gareth James "],["logistic-regression.html", "Chapter 23 Logistic Regression 23.1 Next part 23.2 NEXT Part 23.3 NExt part", " Chapter 23 Logistic Regression glm() In statistics, the logistic model (or logit model) is a statistical model with input (independent variable) a continuous variable and output (dependent variable) a binary variable (discret choice, y/n or 1/0). Example: Student pass the exam. A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam? hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5) pass &lt;- c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1) plot(pass~hours) glm(hours~pass) ## ## Call: glm(formula = hours ~ pass) ## ## Coefficients: ## (Intercept) pass ## 1.775 2.025 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 43.16 ## Residual Deviance: 22.66 AIC: 65.25 p &lt;- seq(0,1, by=0.05) data &lt;- data.frame(probability=p, odds=p/(1-p)) head(data) ## probability odds ## 1 0.00 0.00000000 ## 2 0.05 0.05263158 ## 3 0.10 0.11111111 ## 4 0.15 0.17647059 ## 5 0.20 0.25000000 ## 6 0.25 0.33333333 plot(data$odds~data$probability, type=&#39;o&#39;, pch=19, xlab=&#39;Probability&#39;, ylab=&#39;Odds&#39;) plot(log(data$odds)~data$odds, type=&#39;o&#39;, pch=19, xlab=&#39;Odds&#39;, ylab=&#39;log(odds)&#39;) plot(data$probability~log(data$odds), type=&#39;o&#39;, pch=19, xlab=&#39;log(odds)&#39;, ylab=&#39;Probability&#39;) 23.1 Next part library(data.table) df &lt;- fread(&#39;https://raw.githubusercontent.com/suvarzz/data/master/data_classification.csv&#39;, header=T, sep=&quot;,&quot;) head(df) plot(df[pass==1][,!3], col=&#39;red&#39;) points(df[pass==0][,!3], col=&#39;blue&#39;) model.logit &lt;- glm(pass ~ studied + slept, data = df, family = &#39;binomial&#39;) summary(model.logit) p.lda &lt;- predict(model.logit, df, type = &#39;response&#39;) df$predicted &lt;- ifelse(p.lda &gt; 0.5, 1, 0) head(df) a=-coef(model.logit)[1]/coef(model.logit)[2], b=-coef(model.logit)[1]/coef(model.logit)[3]) b0 = coef(model.logit)[1] b1 = mymodel$coefficients[[2]] b2 = mymodel$coefficients[[3]] z = b0 + (b1 * 1) + (b2 * 4) p = 1 / (1 + exp(-z)) if p=0.5 =&gt; z = 0 =&gt; b0 + b1*x + b2*y =&gt; segments(0,10.87,9.26,0) slept=(3.77-0.474*studied)/0.338 (0, 3.77/0.474) = (9.2, 0) (3.77/0.474,0) = (0, 10.87) segments(9.2,0, 0,10.87, lwd=2) -- filter() operates on rows, whereas select() operates on columns library(caret) library(neuralnet) Linear Discriminant Analysis ( LDA ) and Principal Component Analysis ( PCA ) The basic difference between these two is that LDA uses information of classes to find new features in order to maximize its separability while PCA uses the variance of each feature to do the same. In this context, LDA can be consider a supervised algorithm and PCA an unsupervised algorithm. https://github.com/BIMSBbioinfo/janggu or via 23.2 NEXT Part # For binary dependent variables build models: # 1. LR - Logistic regression # 2. LDA - Linear discriminant analysis # 3. QDA - quadrat discriminant analysis # Detect border of decision, use ROC-curves # Source: Course &#39;Math modeling&#39; practical work, State University of Management, Moscow # link: https://sites.google.com/a/kiber-guu.ru/r-practice/home library(&#39;ISLR&#39;) library(&#39;GGally&#39;) library(&#39;MASS&#39;) my.seed &lt;- 12345 train.percent &lt;- 0.85 options(&quot;ggmatrix.progress.bar&quot; = FALSE) # Data set from ISLR: is credits returned Y/N dependent on: student, average balance and income. ?Default head(Default) str(Default) ### Primary analysis # Scatter plots for primary analysis ggp &lt;- ggpairs(Default) print(ggp, progress = FALSE) # доли наблюдений в столбце default table(Default$default) / sum(table(Default$default)) ### Train data subset set.seed(my.seed) inTrain &lt;- sample(seq_along(Default$default), nrow(Default)*train.percent) df &lt;- Default[inTrain, ] # фактические значения на обучающей выборке Факт &lt;- df$default ### 1. Logistic regression modeling model.logit &lt;- glm(default ~ balance, data = df, family = &#39;binomial&#39;) summary(model.logit) # прогноз: вероятности принадлежности классу &#39;Yes&#39; (дефолт) p.logit &lt;- predict(model.logit, df, type = &#39;response&#39;) Прогноз &lt;- factor(ifelse(p.logit &gt; 0.5, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # матрица неточностей conf.m &lt;- table(Факт, Прогноз) conf.m # чувствительность conf.m[2, 2] / sum(conf.m[2, ]) # специфичность conf.m[1, 1] / sum(conf.m[1, ]) # верность sum(diag(conf.m)) / sum(conf.m) ###. 2. LDA model model.lda &lt;- lda(default ~ balance, data = Default[inTrain, ]) model.lda # прогноз: вероятности принадлежности классу &#39;Yes&#39; (дефолт) p.lda &lt;- predict(model.lda, df, type = &#39;response&#39;) Прогноз &lt;- factor(ifelse(p.lda$posterior[, &#39;Yes&#39;] &gt; 0.5, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # матрица неточностей conf.m &lt;- table(Факт, Прогноз) conf.m conf.m[2, 2] / sum(conf.m[2, ]) # sensitivity conf.m[1, 1] / sum(conf.m[1, ]) # specitivity sum(diag(conf.m)) / sum(conf.m) # accurasy ### 3. QDA modeling model.qda &lt;- qda(default ~ balance, data = Default[inTrain, ]) model.qda # прогноз: вероятности принадлежности классу &#39;Yes&#39; (дефолт) p.qda &lt;- predict(model.qda, df, type = &#39;response&#39;) Прогноз &lt;- factor(ifelse(p.qda$posterior[, &#39;Yes&#39;] &gt; 0.5, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # матрица неточностей conf.m &lt;- table(Факт, Прогноз) conf.m conf.m[2, 2] / sum(conf.m[2, ]) # чувствительность conf.m[1, 1] / sum(conf.m[1, ]) # специфичность sum(diag(conf.m)) / sum(conf.m) # верность ### 4. ROC-curve and detection of decision border # считаем 1-SPC и TPR для всех вариантов границы отсечения x &lt;- NULL # для (1 - SPC) y &lt;- NULL # для TPR # заготовка под матрицу неточностей tbl &lt;- as.data.frame(matrix(rep(0, 4), 2, 2)) rownames(tbl) &lt;- c(&#39;fact.No&#39;, &#39;fact.Yes&#39;) colnames(tbl) &lt;- c(&#39;predict.No&#39;, &#39;predict.Yes&#39;) # вектор вероятностей для перебора p.vector &lt;- seq(0, 1, length = 501) # цикл по вероятностям отсечения for (p in p.vector){ # прогноз Прогноз &lt;- factor(ifelse(p.lda$posterior[, &#39;Yes&#39;] &gt; p, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) # фрейм со сравнением факта и прогноза df.compare &lt;- data.frame(Факт = Факт, Прогноз = Прогноз) # заполняем матрицу неточностей tbl[1, 1] &lt;- nrow(df.compare[df.compare$Факт == &#39;No&#39; &amp; df.compare$Прогноз == &#39;No&#39;, ]) tbl[2, 2] &lt;- nrow(df.compare[df.compare$Факт == &#39;Yes&#39; &amp; df.compare$Прогноз == &#39;Yes&#39;, ]) tbl[1, 2] &lt;- nrow(df.compare[df.compare$Факт == &#39;No&#39; &amp; df.compare$Прогноз == &#39;Yes&#39;, ]) tbl[2, 1] &lt;- nrow(df.compare[df.compare$Факт == &#39;Yes&#39; &amp; df.compare$Прогноз == &#39;No&#39;, ]) # считаем характеристики TPR &lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1]) y &lt;- c(y, TPR) SPC &lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2]) x &lt;- c(x, 1 - SPC) } # строим ROC-кривую par(mar = c(5, 5, 1, 1)) # кривая plot(x, y, type = &#39;l&#39;, col = &#39;blue&#39;, lwd = 3, xlab = &#39;(1 - SPC)&#39;, ylab = &#39;TPR&#39;, xlim = c(0, 1), ylim = c(0, 1)) # прямая случайного классификатора abline(a = 0, b = 1, lty = 3, lwd = 2) # точка для вероятности 0.5 points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16) text(x[p.vector == 0.5], y[p.vector == 0.5], &#39;p = 0.5&#39;, pos = 4) # точка для вероятности 0.2 points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16) text(x[p.vector == 0.2], y[p.vector == 0.2], &#39;p = 0.2&#39;, pos = 4) Прогноз &lt;- factor(ifelse(p.lda$posterior[, &#39;Yes&#39;] &gt; 0.2, 2, 1), levels = c(1, 2), labels = c(&#39;No&#39;, &#39;Yes&#39;)) conf.m &lt;- table(Факт, Прогноз) conf.m # чувствительность conf.m[2, 2] / sum(conf.m[2, ]) # специфичность conf.m[1, 1] / sum(conf.m[1, ]) # верность sum(diag(conf.m)) / sum(conf.m) 23.3 NExt part # Example of logistic regression # Source: 17 - Анализ данных в R. Логистическая регрессия by Anatoliy Karpov # Read data set train.csv: # Statistics of students in a school # gender - male/femail # read, write, math - points for subjects # hon - if honorary degree Y/N # FIX combine train and test into one csv file. Split train and test inside this script setwd(&quot;~/RData&quot;) df &lt;- read.csv(&quot;train.csv&quot;, sep=&quot;;&quot;) # Visual inspection of the dataset head(df) str(df) View(df) # N-not the best mark, Y-the best mark library(ggplot2) ggplot(df, aes(read,math,col=gender))+geom_point()+facet_grid(.~hon)+ theme(axis.text=element_text(size=25), axis.title=element_text(size=25, face=&#39;bold&#39;)) # Apply logistic regression # How hon depends on different variables: read, math, gender fit &lt;- glm(hon ~ read + math + gender, df, family = &quot;binomial&quot;) summary(fit) # Meanings of coefficients: # read-estimate: 0.06677 - if female, math is fixed, if read change to 1, then ln(odds) will be changed to 0.06677 # Get data from fit exp(fit$coefficients) # Predict model - ln(odds) head(predict(object=fit)) # Predict model - return probability to get the best mark for every person head(predict(object = fit, type = &quot;response&quot;)) # Add probabilities to get the best mark for every person in df df$prob &lt;- predict(object = fit, type = &quot;response&quot;) df # Part 2 # ROC-curve of predicted model library(ROCR) # Predicted values and real values pred_fit &lt;- prediction(df$prob, df$hon) # Calculate tpr - true positive rate and fpr - false positive rate perf_fit &lt;- performance(pred_fit, &quot;tpr&quot;, &quot;fpr&quot;) # plot ROC-curve plot(perf_fit, colorize=T, print.cutoffs.at = seq(0,1, by=0.1)) # Area under the curve: 0.87 auc &lt;- performance(pred_fit, measure = &quot;auc&quot;) str(auc) # How to detect the border and make a decision if student will get honorary degree # Specificity - how good we can predict negative results perf3 &lt;- performance(pred_fit, x.measure = &quot;cutoff&quot;, measure = &quot;spec&quot;) # Sencitivity - how good we can predict positive results perf4 &lt;- performance(pred_fit, x.measure = &quot;cutoff&quot;, measure = &quot;sens&quot;) # Общая интенсивность классификатора perf5 &lt;- performance(pred_fit, x.measure = &quot;cutoff&quot;, measure = &quot;acc&quot;) plot(perf3, col = &#39;red&#39;, lwd = 2) plot(add=T, perf4, col = &quot;green&quot;, lwd = 2) plot(add=T, perf5, lwd = 2) legend(x = 0.6, y = 0.3, c(&quot;spec&quot;, &quot;sens&quot;, &quot;accur&quot;), lty=1, col=c(&quot;red&quot;, &quot;green&quot;, &quot;black&quot;), bty=&#39;n&#39;, cex=1, lwd=2) abline(v=0.255, lwd=2) # The border is the intersection of all three curves # Add column with prediced values Y/N df$pred_resp &lt;- factor(ifelse(df$prob &gt; 0.255, 1, 0), labels=c(&quot;N&quot;, &quot;Y&quot;)) # 1 if prediction is correct, 0 if not correct df$correct &lt;- ifelse(df$pred_resp == df$hon, 1, 0) df # blue - correct classified, red - incorrect classified # it is more difficult to predict positive result ggplot(df, aes(prob, fill = factor(correct)))+ geom_dotplot()+ theme(axis.text=element_text(size=25), axis.title=element_text(size=25, face=&quot;bold&quot;)) # Percent of positive predictions mean(df$correct) # Part 3 - Prediction using test data test_df &lt;- read.csv(&quot;test.csv&quot;, sep=&quot;;&quot;) test_df # Predict honorary members test_df$prob_predict &lt;- predict(fit, newdata=test_df, type=&quot;response&quot;) test_df$pred_resp &lt;- factor(ifelse(test_df$prob_predict &gt; 0.255, 1, 0), labels=c(&quot;N&quot;, &quot;Y&quot;)) test_df "],["multiple-linear-regression.html", "Chapter 24 Multiple linear regression", " Chapter 24 Multiple linear regression Source: Анализ данных в R. Множественная линейная регрессия # Dataset swiss ?swiss swiss &lt;- data.frame(swiss) str(swiss) # Histogram of fertility hist(swiss$Fertility, col=&#39;red&#39;) # Numeric predictors for Fertility prediction fit &lt;- lm(Fertility ~ Examination + Catholic, data = swiss) summary(fit) # the principal predictor is an &#39;examination&#39; with negative correlation. # Interaction of variables &#39;examination&#39; and &#39;catholics&#39; &#39;*&#39; fit2 &lt;- lm(Fertility ~ Examination*Catholic, data = swiss) summary(fit2) confint(fit2) # Categorical predictors # Histogram obviously have two parts -&gt; we can split data for two factors hist(swiss$Catholic, col = &#39;red&#39;) # Lets split &#39;Catholics&#39; for two groups: with many &#39;lots&#39; and few &#39;few&#39; swiss$religious &lt;- ifelse(swiss$Catholic &gt; 60, &#39;Lots&#39;, &#39;Few&#39;) swiss$religious &lt;- as.factor(swiss$religious) fit3 &lt;- lm(Fertility ~ Examination + religious, data = swiss) summary(fit3) # Interaction of variables fit4 &lt;- lm(Fertility ~ religious*Examination, data = swiss) summary(fit4) # plots ggplot(swiss, aes(x = Examination, y = Fertility)) + geom_point() ggplot(swiss, aes(x = Examination, y = Fertility)) + geom_point() + geom_smooth() ggplot(swiss, aes(x = Examination, y = Fertility)) + geom_point() + geom_smooth(method = &#39;lm&#39;) ggplot(swiss, aes(x = Examination, y = Fertility, col = religious)) + geom_point() ggplot(swiss, aes(x = Examination, y = Fertility, col = religious)) + geom_point() + geom_smooth() ggplot(swiss, aes(x = Examination, y = Fertility, col = religious)) + geom_point() + geom_smooth(method = &#39;lm&#39;) # fit5 &lt;- lm(Fertility ~ religious*Infant.Mortality*Examination, data = swiss) summary(fit5) # model comparison rm(swiss) swiss &lt;- data.frame(swiss) fit_full &lt;- lm(Fertility ~ ., data = swiss) summary(fit_full) fit_reduced1 &lt;- lm(Fertility ~ Infant.Mortality + Examination + Catholic + Education, data = swiss) summary(fit_reduced1) anova(fit_full, fit_reduced1) fit_reduced2 &lt;- lm(Fertility ~ Infant.Mortality + Education + Catholic + Agriculture, data = swiss) summary(fit_reduced2) anova(fit_full, fit_reduced2) # model selection optimal_fit &lt;- step(fit_full, direction = &#39;backward&#39;) summary(optimal_fit) "],["simple-markov-process.html", "Chapter 25 Simple Markov process", " Chapter 25 Simple Markov process Here, we will consider a simple example of Markov process with implementation in R. The following example is taken from Bodo Winter website. A Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states. Let’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.2), and there’s a probability of a bored student becoming alert (say 0.25). Let’s say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point \\(t\\). Given the transition probabilities above, what’s the number of alert and bored students at the next point in time, \\(t+1\\)? Multiply 20 by 0.2 (=4) and these will be the alert students that turn bored. And then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert. So, at \\(t+1\\), there’s going to be 20-4+20 alert students. And there’s going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert. A handy way of representing this Markov process is by defining a transition probability matrix: A B A\\(_{t+1}\\) 0.8 0.25 B\\(_{t+1}\\) 0.2 0.75 What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point \\(t+1\\). And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way: \\[ \\begin{bmatrix} 0.8 &amp; 0.25 \\\\ 0.2 &amp; 0.75 \\end{bmatrix}\\times\\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix} = \\begin{bmatrix} 0.8\\times0.2 + 0.25\\times0.8 \\\\ 0.2\\times0.2 + 0.75\\times0.8 \\end{bmatrix} = \\begin{bmatrix} 0.36 \\\\ 0.64 \\end{bmatrix} \\] The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students. Now, you might ask yourself: What happens if this process continues? What happens at \\(t+2\\), \\(t+3\\) etc.? Will it be the case that at one point there are no bored students any more? Let’s simulate this in R and find out! Let’s call this tpm for transition probability matrix: tpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE) colnames(tpm) = c(&#39;A&#39;,&#39;B&#39;) rownames(tpm) = c(&#39;At+1&#39;, &#39;Bt+1&#39;) tpm ## A B ## At+1 0.8 0.25 ## Bt+1 0.2 0.75 Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at \\(t+1\\). And 0.25 students who were in state B at time point t will be in state A at \\(t+1\\). The second row has a similar interpretation for alert and bored students becoming bored at \\(t+1\\). Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we’ll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions – and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2). Let’s start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called sm (short for student matrix): sm = as.matrix(c(0.1, 0.9)) rownames(sm)= c(&#39;A&#39;, &#39;B&#39;) sm ## [,1] ## A 0.1 ## B 0.9 Now let’s repeat by looping: for(i in 1:10){ sm = tpm %*% sm } Here, we’re looping 10 times and on each iteration, we multiply the matrix tpm with the student matrix sm. We take this result and store it in sm. This means that at the next iteration, our fixed transition probability matrix will be multiplied by a different student matrix, allowing for the proportions to slowly change over time. R operator ’%*%’ is used for matrix multiplication Outcome of our ten loop iterations: sm ## [,1] ## At+1 0.5544017 ## Bt+1 0.4455983 So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations. Let’s reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations. for(i in 1:1000){ sm = tpm %*% sm } sm ## [,1] ## At+1 0.5555556 ## Bt+1 0.4444444 A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called Markov convergence. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It’s not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we’re dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What’s so cool about this is that it is, at first sight, fairly counterintuitive. At least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that’s not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium – albeit sometimes a little faster or slower. You can play around with different values for the sm object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert – the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn’t matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity. 25.0.1 Sources Bodo Winter website "],["naive-bayes.html", "Chapter 26 Naive Bayes", " Chapter 26 Naive Bayes \\(P(c|x) = \\frac{P(x|c)(P(c))}{P(x)}\\), where \\(P(c|x)\\) - posteriour probability \\(P(x|c)\\) - Likelihood \\(P(c)\\) - Class Prior Probbility \\(P(x)\\) - Predictor Prior Probability "]]
