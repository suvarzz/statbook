# Tree-based models

Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.  

CART is both a generic term to describe tree algorithms and also a specific name for Breiman’s original algorithm for constructing classification and regression trees.  

* **Decision Tree**: A tree-shaped graph or model of decisions used to determine a course of action or show a statistical probability.  
* **Classification Tree**: A decision tree that performs classification (predicts a categorical response).  
* **Regression Tree**: A decision tree that performs regression (predicts a numeric response).  
* **Split Point**: A split point occurs at each node of the tree where a decision is made (e.g. x > 7 vs. x ≤ 7).  
* **Terminal Node**: A terminal node is a node which has no descendants (child nodes). Also called a “leaf node.”  

**Properties of Trees**  
* Can handle huge datasets.  
* Can handle mixed predictors implicitly – numeric and categorical.  
* Easily ignore redundant variables.  
* Handle missing data elegantly through surrogate splits.  
* Small trees are easy to interpret.  
* Large trees are hard to interpret.  
* Prediction performance is often poor (high variance).  

**Tree Algorithms**  
There are a handful of different tree algorithms in addition to Breiman’s original CART algorithm. Namely, ID3, C4.5 and C5.0, all created by Ross Quinlan. C5.0 is an improvement over C4.5, however, the C4.5 algorithm is still quite popular since the multi-threaded version of C5.0 is proprietary (although the single threaded is released as GPL).  

**CART vs C4.5**  
Here are some of the differences between CART and C4.5:

* Tests in CART are always binary, but C4.5 allows two or more outcomes.  
* CART uses the Gini diversity index to rank tests, whereas C4.5 uses information-based criteria.  
* CART prunes trees using a cost-complexity model whose parameters are estimated by cross-validation; C4.5 uses a single-pass algorithm derived from binomial confidence limits.  
* With respect to missing data, CART looks for surrogate tests that approximate the outcomes when the tested attribute has an unknown value, but C4.5 apportions the case probabilistically among the outcomes.  

Decision trees are formed by a collection of rules based on variables in the modeling data set:  

1. Rules based on variables’ values are selected to get the best split to differentiate observations based on the dependent variable.  
2. Once a rule is selected and splits a node into two, the same process is applied to each “child” node (i.e. it is a recursive procedure).  
3. Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)  

Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.  

## Classification Tree example
Let's use the data frame kyphosis to predict a type of deformation (kyphosis) after surgery, from age in months (Age), number of vertebrae involved (Number), and the highest vertebrae operated on (Start).

```{r}
# Classification Tree with rpart
library(rpart)

# grow tree
fit <- rpart(Kyphosis ~ Age + Number + Start,
   method="class", data=kyphosis)

printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits

# plot tree
plot(fit, uniform=TRUE,
   main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

# create attractive postscript plot of tree
post(fit, title = "Classification Tree for Kyphosis")

# prune the tree
pfit<- prune(fit, cp=fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])

# plot the pruned tree
#FIXME: pfit is not a tree just a root error
#plot(pfit, uniform=TRUE,
#   main="Pruned Classification Tree for Kyphosis")
#text(pfit, use.n=TRUE, all=TRUE, cex=.8)
#post(pfit, file = "c:/ptree.ps",
#   title = "Pruned Classification Tree for Kyphosis")
```

## Regression Tree example
```{r}
# Regression Tree Example
library(rpart)

# grow tree
fit <- rpart(Mileage~Price + Country + Reliability + Type,
   method="anova", data=cu.summary)

printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits

# create additional plots
par(mfrow=c(1,2)) # two plots on one page
rsq.rpart(fit) # visualize cross-validation results  

# plot tree
plot(fit, uniform=TRUE,
   main="Regression Tree for Mileage ")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

# create attractive postcript plot of tree
post(fit, file = "tree2.ps",
   title = "Regression Tree for Mileage ")

# prune the tree
pfit<- prune(fit, cp=0.01160389) # from cptable   

# plot the pruned tree
plot(pfit, uniform=TRUE,
   main="Pruned Regression Tree for Mileage")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
post(pfit, file = "ptree2.ps",
   title = "Pruned Regression Tree for Mileage")
```

**Sources**  
[Tree-Based Models at Quick-R by datacamp](https://www.statmethods.net/advstats/cart.html)  
[UseR! Machine Learnign Turorial](https://koalaverse.github.io/machine-learning-in-R/decision-trees.html)
